{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CoOp Training on Google Colab\n",
        "\n",
        "This notebook implements **Context Optimization (CoOp)** for Vision-Language Models on Google Colab.\n",
        "\n",
        "## Project Overview\n",
        "- **Base Framework**: [Dassl.pytorch](https://github.com/KaiyangZhou/Dassl.pytorch)\n",
        "- **Model**: [CoOp](https://github.com/KaiyangZhou/CoOp) - Prompt Learning for CLIP\n",
        "- **Dataset**: Oxford-IIIT Pets (37 classes)\n",
        "- **Experiments**: 1-shot, 4-shot, and 16-shot learning\n",
        "\n",
        "## Requirements\n",
        "- GPU: Tesla T4 or better\n",
        "- Runtime: Python 3.10+\n",
        "- CUDA: 11.0+\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7e2DQZnvuBpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9x13YItMk9",
        "outputId": "97205bba-67c7-4ef1-b945-eb4ff992de37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Dassl.pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone Dassl.pytorch repository (domain adaptation & semi-supervised learning framework)\n",
        "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to Dassl.pytorch\n",
        "%cd Dassl.pytorch/"
      ],
      "metadata": {
        "id": "FWB9jdEAtgQO",
        "outputId": "d0f200b1-43ea-4923-af12-f84d16ede462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dassl.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify PyTorch and CUDA availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")"
      ],
      "metadata": {
        "id": "6aCDX2TUthZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9ad100-9a7f-4eac-dfcc-07d83cd2a877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dassl dependencies from requirements.txt\n",
        "# Note: Colab will prompt you to restart runtime after this step\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-F_gPKCPtlSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0561684-fec2-48a2-d7fb-18567fead43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
            "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
            "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting yacs (from -r requirements.txt (line 4))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
            "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
            "  Downloading tb_nightly-2.21.0a20251023-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Collecting ftfy (from -r requirements.txt (line 11))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2024.11.6)\n",
            "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
            "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
            "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.0.2)\n",
            "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (11.3.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.23.0+cu126)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from yacs->-r requirements.txt (line 4)) (6.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 5)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 5)) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (75.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->-r requirements.txt (line 11)) (0.2.14)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tb-nightly->-r requirements.txt (line 6)) (4.15.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.5.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2025.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
            "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading tb_nightly-2.21.0a20251023-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: yapf, mccabe, yacs, pyflakes, pycodestyle, littleutils, isort, ftfy, entrypoints, tb-nightly, outdated, flake8, ogb, wilds\n",
            "  Attempting uninstall: entrypoints\n",
            "    Found existing installation: entrypoints 0.4\n",
            "    Uninstalling entrypoints-0.4:\n",
            "      Successfully uninstalled entrypoints-0.4\n",
            "Successfully installed entrypoints-0.3 flake8-3.7.9 ftfy-6.3.1 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.21.0a20251023 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "entrypoints"
                ]
              },
              "id": "c0021f07a97a4106a64ff875d5d8d180"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ STOP: Restart Runtime Required\n",
        "\n",
        "After running the cell above, Colab will show a **\"RESTART RUNTIME\"** button.\n",
        "\n",
        "### Steps to Continue:\n",
        "1. Click the **\"RESTART RUNTIME\"** button (appears above)\n",
        "2. After restart, **re-run cells 1, 2, and 3** to restore the environment\n",
        "3. Then continue from **Cell 5** below\n",
        "\n",
        "**Note**: Do NOT re-run Cell 4 after restart.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FwHWY0ZS1_u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dassl in editable mode for development\n",
        "# Run this AFTER restarting runtime and re-running cells 1-3\n",
        "!python setup.py develop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5asjDma_8YjH",
        "outputId": "ab98d9bf-a21b-43e7-de7c-895d5efff738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running develop\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating dassl.egg-info\n",
            "writing dassl.egg-info/PKG-INFO\n",
            "writing dependency_links to dassl.egg-info/dependency_links.txt\n",
            "writing requirements to dassl.egg-info/requires.txt\n",
            "writing top-level names to dassl.egg-info/top_level.txt\n",
            "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "reading manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.12/dist-packages/dassl.egg-link (link to .)\n",
            "Adding dassl 0.6.3 to easy-install.pth file\n",
            "\n",
            "Installed /content/Dassl.pytorch\n",
            "Processing dependencies for dassl==0.6.3\n",
            "Searching for tabulate==0.9.0\n",
            "Best match: tabulate 0.9.0\n",
            "Adding tabulate 0.9.0 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for wilds==1.2.2\n",
            "Best match: wilds 1.2.2\n",
            "Adding wilds 1.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for regex==2024.11.6\n",
            "Best match: regex 2024.11.6\n",
            "Adding regex 2024.11.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for ftfy==6.3.1\n",
            "Best match: ftfy 6.3.1\n",
            "Adding ftfy 6.3.1 to easy-install.pth file\n",
            "Installing ftfy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tqdm==4.67.1\n",
            "Best match: tqdm 4.67.1\n",
            "Adding tqdm 4.67.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scikit-learn==1.6.1\n",
            "Best match: scikit-learn 1.6.1\n",
            "Adding scikit-learn 1.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for scipy==1.16.3\n",
            "Best match: scipy 1.16.3\n",
            "Adding scipy 1.16.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tb-nightly==2.21.0a20251023\n",
            "Best match: tb-nightly 2.21.0a20251023\n",
            "Adding tb-nightly 2.21.0a20251023 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for gdown==5.2.0\n",
            "Best match: gdown 5.2.0\n",
            "Adding gdown 5.2.0 to easy-install.pth file\n",
            "Installing gdown script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for isort==4.3.21\n",
            "Best match: isort 4.3.21\n",
            "Adding isort 4.3.21 to easy-install.pth file\n",
            "Installing isort script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for yapf==0.29.0\n",
            "Best match: yapf 0.29.0\n",
            "Adding yapf 0.29.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for flake8==3.7.9\n",
            "Best match: flake8 3.7.9\n",
            "Adding flake8 3.7.9 to easy-install.pth file\n",
            "Installing flake8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torchvision==0.23.0+cu126\n",
            "Best match: torchvision 0.23.0+cu126\n",
            "Adding torchvision 0.23.0+cu126 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for torch==2.8.0+cu126\n",
            "Best match: torch 2.8.0+cu126\n",
            "Adding torch 2.8.0+cu126 to easy-install.pth file\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pytz==2025.2\n",
            "Best match: pytz 2025.2\n",
            "Adding pytz 2025.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pillow==11.3.0\n",
            "Best match: pillow 11.3.0\n",
            "Adding pillow 11.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pandas==2.2.2\n",
            "Best match: pandas 2.2.2\n",
            "Adding pandas 2.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for outdated==0.2.2\n",
            "Best match: outdated 0.2.2\n",
            "Adding outdated 0.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for ogb==1.3.6\n",
            "Best match: ogb 1.3.6\n",
            "Adding ogb 1.3.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for numpy==2.0.2\n",
            "Best match: numpy 2.0.2\n",
            "Adding numpy 2.0.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing numpy-config script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for wcwidth==0.2.14\n",
            "Best match: wcwidth 0.2.14\n",
            "Adding wcwidth 0.2.14 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for threadpoolctl==3.6.0\n",
            "Best match: threadpoolctl 3.6.0\n",
            "Adding threadpoolctl 3.6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for joblib==1.5.2\n",
            "Best match: joblib 1.5.2\n",
            "Adding joblib 1.5.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for werkzeug==3.1.3\n",
            "Best match: werkzeug 3.1.3\n",
            "Adding werkzeug 3.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for setuptools==75.2.0\n",
            "Best match: setuptools 75.2.0\n",
            "Adding setuptools 75.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for protobuf==5.29.5\n",
            "Best match: protobuf 5.29.5\n",
            "Adding protobuf 5.29.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for packaging==25.0\n",
            "Best match: packaging 25.0\n",
            "Adding packaging 25.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for markdown==3.10\n",
            "Best match: markdown 3.10\n",
            "Adding markdown 3.10 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for grpcio==1.76.0\n",
            "Best match: grpcio 1.76.0\n",
            "Adding grpcio 1.76.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for requests==2.32.4\n",
            "Best match: requests 2.32.4\n",
            "Adding requests 2.32.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for filelock==3.20.0\n",
            "Best match: filelock 3.20.0\n",
            "Adding filelock 3.20.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for beautifulsoup4==4.13.5\n",
            "Best match: beautifulsoup4 4.13.5\n",
            "Adding beautifulsoup4 4.13.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pyyaml==6.0.3\n",
            "Best match: pyyaml 6.0.3\n",
            "Adding pyyaml 6.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for mccabe==0.6.1\n",
            "Best match: mccabe 0.6.1\n",
            "Adding mccabe 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pycodestyle==2.5.0\n",
            "Best match: pycodestyle 2.5.0\n",
            "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
            "Installing pycodestyle script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for pyflakes==2.1.1\n",
            "Best match: pyflakes 2.1.1\n",
            "Adding pyflakes 2.1.1 to easy-install.pth file\n",
            "Installing pyflakes script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for entrypoints==0.3\n",
            "Best match: entrypoints 0.3\n",
            "Adding entrypoints 0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for triton==3.4.0\n",
            "Best match: triton 3.4.0\n",
            "Adding triton 3.4.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufile-cu12==1.11.1.6\n",
            "Best match: nvidia-cufile-cu12 1.11.1.6\n",
            "Adding nvidia-cufile-cu12 1.11.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvjitlink-cu12==12.6.85\n",
            "Best match: nvidia-nvjitlink-cu12 12.6.85\n",
            "Adding nvidia-nvjitlink-cu12 12.6.85 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nvtx-cu12==12.6.77\n",
            "Best match: nvidia-nvtx-cu12 12.6.77\n",
            "Adding nvidia-nvtx-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-nccl-cu12==2.27.3\n",
            "Best match: nvidia-nccl-cu12 2.27.3\n",
            "Adding nvidia-nccl-cu12 2.27.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparselt-cu12==0.7.1\n",
            "Best match: nvidia-cusparselt-cu12 0.7.1\n",
            "Adding nvidia-cusparselt-cu12 0.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusparse-cu12==12.5.4.2\n",
            "Best match: nvidia-cusparse-cu12 12.5.4.2\n",
            "Adding nvidia-cusparse-cu12 12.5.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cusolver-cu12==11.7.1.2\n",
            "Best match: nvidia-cusolver-cu12 11.7.1.2\n",
            "Adding nvidia-cusolver-cu12 11.7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-curand-cu12==10.3.7.77\n",
            "Best match: nvidia-curand-cu12 10.3.7.77\n",
            "Adding nvidia-curand-cu12 10.3.7.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cufft-cu12==11.3.0.4\n",
            "Best match: nvidia-cufft-cu12 11.3.0.4\n",
            "Adding nvidia-cufft-cu12 11.3.0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cublas-cu12==12.6.4.1\n",
            "Best match: nvidia-cublas-cu12 12.6.4.1\n",
            "Adding nvidia-cublas-cu12 12.6.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cudnn-cu12==9.10.2.21\n",
            "Best match: nvidia-cudnn-cu12 9.10.2.21\n",
            "Adding nvidia-cudnn-cu12 9.10.2.21 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-cupti-cu12==12.6.80\n",
            "Best match: nvidia-cuda-cupti-cu12 12.6.80\n",
            "Adding nvidia-cuda-cupti-cu12 12.6.80 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-runtime-cu12==12.6.77\n",
            "Best match: nvidia-cuda-runtime-cu12 12.6.77\n",
            "Adding nvidia-cuda-runtime-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.6.77 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for fsspec==2025.3.0\n",
            "Best match: fsspec 2025.3.0\n",
            "Adding fsspec 2025.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for jinja2==3.1.6\n",
            "Best match: jinja2 3.1.6\n",
            "Adding jinja2 3.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for networkx==3.5\n",
            "Best match: networkx 3.5\n",
            "Adding networkx 3.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for sympy==1.13.3\n",
            "Best match: sympy 1.13.3\n",
            "Adding sympy 1.13.3 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for typing-extensions==4.15.0\n",
            "Best match: typing-extensions 4.15.0\n",
            "Adding typing-extensions 4.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for tzdata==2025.2\n",
            "Best match: tzdata 2025.2\n",
            "Adding tzdata 2025.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for python-dateutil==2.9.0.post0\n",
            "Best match: python-dateutil 2.9.0.post0\n",
            "Adding python-dateutil 2.9.0.post0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for littleutils==0.2.4\n",
            "Best match: littleutils 0.2.4\n",
            "Adding littleutils 0.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for urllib3==2.5.0\n",
            "Best match: urllib3 2.5.0\n",
            "Adding urllib3 2.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for six==1.17.0\n",
            "Best match: six 1.17.0\n",
            "Adding six 1.17.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for markupsafe==3.0.3\n",
            "Best match: markupsafe 3.0.3\n",
            "Adding markupsafe 3.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for PySocks==1.7.1\n",
            "Best match: PySocks 1.7.1\n",
            "Adding PySocks 1.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for certifi==2025.10.5\n",
            "Best match: certifi 2025.10.5\n",
            "Adding certifi 2025.10.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for idna==3.11\n",
            "Best match: idna 3.11\n",
            "Adding idna 3.11 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for charset-normalizer==3.4.4\n",
            "Best match: charset-normalizer 3.4.4\n",
            "Adding charset-normalizer 3.4.4 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for soupsieve==2.8\n",
            "Best match: soupsieve 2.8\n",
            "Adding soupsieve 2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/dist-packages\n",
            "Finished processing dependencies for dassl==0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Dassl installation after runtime restart\n",
        "import dassl\n",
        "\n",
        "print(f\"Dassl version: {dassl.__version__}\")\n",
        "print(\"Dassl installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bIOi69H9EGU",
        "outputId": "e2c8ca78-e9d8-43a5-a83b-68f89be078f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dassl version: 0.6.3\n",
            "Dassl installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate back to /content directory\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RF-hWQl9eyu",
        "outputId": "72647c26-f069-4ffd-9492-f9781bd29033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone CoOp (Context Optimization) repository\n",
        "!git clone https://github.com/KaiyangZhou/CoOp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNt99w9z9jNb",
        "outputId": "94db21ba-4724-41b7-fb16-34889c93dd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoOp'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 455 (delta 217), reused 199 (delta 199), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (455/455), 1.40 MiB | 4.67 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to CoOp\n",
        "%cd CoOp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfddXTM89mKR",
        "outputId": "9323dd65-6f8c-48c3-f8b9-4c5d0c63c3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenAI CLIP and required packages (ftfy, regex, tqdm)\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqOsoJt-9osT",
        "outputId": "1410910e-276a-4467-9dd7-de3534a579f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-y0b4kjkb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-y0b4kjkb\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=9b034ebcd26be722e7b6bd9fbbf54734278c919435d7c82276ea9589a13033e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yt_rqaeq/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify CLIP installation and load a test model\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"RN50\", device=device)\n",
        "\n",
        "print(\"✓ CLIP loaded successfully!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ResNet-50\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeUXX_qH9rxW",
        "outputId": "fbec1260-423d-4658-f9d2-2130ebc101fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 256M/256M [00:05<00:00, 45.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ CLIP loaded successfully!\n",
            "Device: cuda\n",
            "Model: ResNet-50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Setup: Oxford-IIIT Pets\n",
        "\n",
        "We'll download and prepare the **Oxford-IIIT Pets** dataset:\n",
        "- **Classes**: 37 pet breeds (dogs and cats)\n",
        "- **Images**: ~7,400 images\n",
        "- **Size**: ~800 MB\n",
        "\n",
        "### Dataset Components:\n",
        "1. Images (pets photos)\n",
        "2. Annotations (segmentation masks)\n",
        "3. Split file (train/val/test division)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GeFTCJag4cE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create main data directory for all datasets\n",
        "!mkdir -p data/"
      ],
      "metadata": {
        "id": "URNuBvA390Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to data directory\n",
        "%cd /content/CoOp/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZERqdcgb-GFP",
        "outputId": "4e9e42c9-ff29-4abb-a1aa-f3dee8af7518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for Oxford-IIIT Pets dataset\n",
        "!mkdir -p oxford_pets\n",
        "%cd oxford_pets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA5jzOEPCMmF",
        "outputId": "47f5b963-dd93-4674-f40f-8851f6c1bdb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp/data/oxford_pets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Oxford Pets images (~800 MB, may take 2-3 minutes)\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azd9SVKICOkI",
        "outputId": "fdd537d9-086c-4078-e2c8-17dcd13f739a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-15 19:21:54--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/pets/images.tar.gz [following]\n",
            "--2025-11-15 19:21:55--  https://thor.robots.ox.ac.uk/pets/images.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/octet-stream]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  22.9MB/s    in 32s     \n",
            "\n",
            "2025-11-15 19:22:27 (23.5 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract images from compressed archive\n",
        "!tar -xzf images.tar.gz\n",
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWXTIv0oCRhK",
        "outputId": "3b73d710-a72b-4073-e578-ee09e133a988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 756M\n",
            "drwxr-xr-x 2 1001 1001 304K Jun 18  2012 images\n",
            "-rw-r--r-- 1 root root 756M Jul 29  2022 images.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download annotations (segmentation masks and labels)\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIvIjZ84Ccbg",
        "outputId": "77da49c1-2829-41af-82ad-3d6a5ef308be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-15 19:22:48--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/pets/annotations.tar.gz [following]\n",
            "--2025-11-15 19:22:48--  https://thor.robots.ox.ac.uk/pets/annotations.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/octet-stream]\n",
            "Saving to: ‘annotations.tar.gz’\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  11.9MB/s    in 1.5s    \n",
            "\n",
            "2025-11-15 19:22:51 (11.9 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract annotations\n",
        "!tar -xzf annotations.tar.gz\n",
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1lKuhweCh08",
        "outputId": "4b1897f3-b7be-44a3-c035-08f403085074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 774M\n",
            "drwxr-xr-x 4  501 staff 4.0K Jun 30  2012 annotations\n",
            "-rw-r--r-- 1 root root   19M Jul 29  2022 annotations.tar.gz\n",
            "drwxr-xr-x 2 1001  1001 304K Jun 18  2012 images\n",
            "-rw-r--r-- 1 root root  756M Jul 29  2022 images.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown if not already installed\n",
        "!pip install gdown -q\n",
        "\n",
        "# Download train/val/test split file from Google Drive\n",
        "!gdown 1501r8Ber4nNKvmlFVQZ8SeUHTcdTTEqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwRzdLGIClVs",
        "outputId": "af4c38bf-afa7-443c-8e3f-83cd8c2b1aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1501r8Ber4nNKvmlFVQZ8SeUHTcdTTEqs\n",
            "To: /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "\r  0% 0.00/724k [00:00<?, ?B/s]\r100% 724k/724k [00:00<00:00, 7.73MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify dataset structure and files\n",
        "!echo \"=== Main directory ===\"\n",
        "!ls -lh\n",
        "\n",
        "!echo \"\\n=== Images directory (first 10) ===\"\n",
        "!ls images/ | head -10\n",
        "\n",
        "!echo \"\\n=== Annotations directory ===\"\n",
        "!ls annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVKFcEGpCrRP",
        "outputId": "8a4f2ab7-7a7c-46b9-a7c5-fd86d5c5f768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Main directory ===\n",
            "total 775M\n",
            "drwxr-xr-x 4  501 staff 4.0K Jun 30  2012 annotations\n",
            "-rw-r--r-- 1 root root   19M Jul 29  2022 annotations.tar.gz\n",
            "drwxr-xr-x 2 1001  1001 304K Jun 18  2012 images\n",
            "-rw-r--r-- 1 root root  756M Jul 29  2022 images.tar.gz\n",
            "-rw-r--r-- 1 root root  708K Sep  5  2021 split_zhou_OxfordPets.json\n",
            "\\n=== Images directory (first 10) ===\n",
            "Abyssinian_100.jpg\n",
            "Abyssinian_100.mat\n",
            "Abyssinian_101.jpg\n",
            "Abyssinian_101.mat\n",
            "Abyssinian_102.jpg\n",
            "Abyssinian_102.mat\n",
            "Abyssinian_103.jpg\n",
            "Abyssinian_104.jpg\n",
            "Abyssinian_105.jpg\n",
            "Abyssinian_106.jpg\n",
            "\\n=== Annotations directory ===\n",
            "list.txt  README  test.txt  trainval.txt  trimaps  xmls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove compressed files to free up disk space (~775 MB)\n",
        "!rm images.tar.gz annotations.tar.gz\n",
        "!echo \"✓ Compressed files removed successfully\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaQqYh27CwuE",
        "outputId": "1cd6479d-3492-4061-d525-df07fc4c7846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Compressed files removed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bug Fix: PyTorch 2.x Compatibility\n",
        "\n",
        "CoOp and Dassl were developed for older PyTorch versions. We need to fix compatibility issues with PyTorch 2.x:\n",
        "\n",
        "### Issue 1: LRScheduler `verbose` Parameter\n",
        "- **Problem**: PyTorch 2.0+ removed the `verbose` parameter from `LRScheduler.__init__()`\n",
        "- **Solution**: Remove `verbose` parameter from Dassl's lr_scheduler.py\n",
        "\n",
        "This fix is required for training to work properly.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jBcRRNYM5jaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to Dassl's optimizer module directory\n",
        "%cd /content/Dassl.pytorch/dassl/optim/"
      ],
      "metadata": {
        "id": "4Y4M5VBj5198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581f7585-8424-443f-e45e-cde1e220555a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dassl.pytorch/dassl/optim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the problematic lines in lr_scheduler.py\n",
        "!sed -n '15,50p' lr_scheduler.py"
      ],
      "metadata": {
        "id": "5NhW0_XglRpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213cfeb8-b32a-49ec-a53d-b2282b0f5857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        successor,\n",
            "        warmup_epoch,\n",
            "        last_epoch=-1,\n",
            "        verbose=False\n",
            "    ):\n",
            "        self.successor = successor\n",
            "        self.warmup_epoch = warmup_epoch\n",
            "        super().__init__(optimizer, last_epoch, verbose)\n",
            "\n",
            "    def get_lr(self):\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def step(self, epoch=None):\n",
            "        if self.last_epoch >= self.warmup_epoch:\n",
            "            self.successor.step(epoch)\n",
            "            self._last_lr = self.successor.get_last_lr()\n",
            "        else:\n",
            "            super().step(epoch)\n",
            "\n",
            "\n",
            "class ConstantWarmupScheduler(_BaseWarmupScheduler):\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        optimizer,\n",
            "        successor,\n",
            "        warmup_epoch,\n",
            "        cons_lr,\n",
            "        last_epoch=-1,\n",
            "        verbose=False\n",
            "    ):\n",
            "        self.cons_lr = cons_lr\n",
            "        super().__init__(\n",
            "            optimizer, successor, warmup_epoch, last_epoch, verbose\n",
            "        )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix: Remove 'verbose' parameter from LRScheduler.__init__() calls\n",
        "# This fixes compatibility with PyTorch 2.0+\n",
        "!sed -i 's/super().__init__(optimizer, last_epoch, verbose)/super().__init__(optimizer, last_epoch)/g' lr_scheduler.py\n",
        "\n",
        "!echo \"✓ LRScheduler compatibility fix applied\""
      ],
      "metadata": {
        "id": "2dp9xOQxlV3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b5fecf-d92f-4dda-be9f-1fcb74d6c33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ LRScheduler compatibility fix applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the fix was applied correctly\n",
        "!grep -n \"super().__init__\" lr_scheduler.py | head -10"
      ],
      "metadata": {
        "id": "H7BjfgNGlieN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7fc25c-7949-46b2-ae3c-b18e3d3c6017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22:        super().__init__(optimizer, last_epoch)\n",
            "47:        super().__init__(\n",
            "69:        super().__init__(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate back to CoOp root directory\n",
        "%cd /content/CoOp\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6iPgtA7CzdM",
        "outputId": "aeaaaff4-be7c-4ac1-d2d5-0fc1149529cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CoOp\n",
            "/content/CoOp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset configuration (classes, transforms, paths)\n",
        "!echo \"=== Oxford Pets Dataset Configuration ===\"\n",
        "!cat configs/datasets/oxford_pets.yaml\n",
        "\n",
        "!echo \"\\n=== Trainer Configuration ===\"\n",
        "!cat configs/trainers/CoOp/vit_b16_ep50.yaml | head -30"
      ],
      "metadata": {
        "id": "VBoLudZA7Cqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4d2c97-6a14-4d24-83a2-4e0c039ff47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Oxford Pets Dataset Configuration ===\n",
            "DATASET:\n",
            "  NAME: \"OxfordPets\"\\n=== Trainer Configuration ===\n",
            "DATALOADER:\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "  NUM_WORKERS: 8\n",
            "\n",
            "INPUT:\n",
            "  SIZE: (224, 224)\n",
            "  INTERPOLATION: \"bicubic\"\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  TRANSFORMS: [\"random_resized_crop\", \"random_flip\", \"normalize\"]\n",
            "\n",
            "OPTIM:\n",
            "  NAME: \"sgd\"\n",
            "  LR: 0.002\n",
            "  MAX_EPOCH: 50\n",
            "  LR_SCHEDULER: \"cosine\"\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_TYPE: \"constant\"\n",
            "  WARMUP_CONS_LR: 1e-5\n",
            "\n",
            "TRAIN:\n",
            "  PRINT_FREQ: 5\n",
            "\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: \"ViT-B/16\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Experiments\n",
        "\n",
        "We'll train CoOp with three different configurations:\n",
        "1. **1-shot**: 1 training sample per class (quick test, ~5 min)\n",
        "2. **4-shot**: 4 training samples per class (~15 min)\n",
        "3. **16-shot**: 16 training samples per class (~30 min)\n",
        "\n",
        "### Training Configuration:\n",
        "- **Backbone**: ViT-B/16 (Vision Transformer)\n",
        "- **Epochs**: 50\n",
        "- **Context Tokens**: 16 learnable tokens\n",
        "- **Optimizer**: SGD with cosine learning rate schedule\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Qgq6lDTs6ObV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training: 1-shot learning (quick test, ~5 minutes)\n",
        "# Purpose: Verify that training pipeline works correctly\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_1shot \\\n",
        "DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXVuolioC4fw",
        "outputId": "3f2faf57-28f5-4de9-d224-13c81c2d0947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:23:42.551184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763234622.571974    1768 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763234622.578296    1768 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763234622.593772    1768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234622.593798    1768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234622.593802    1768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234622.593805    1768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:23:42.598468: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/oxford_pets_1shot\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_1shot\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 1-shot dataset\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/CoOp/data/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "100%|███████████████████████████████████████| 351M/351M [00:06<00:00, 57.8MiB/s]\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_1shot/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 2.548 (2.548) data 0.525 (0.525) loss 2.7266 (2.7266) acc 37.5000 (37.5000) lr 2.0000e-03 eta 0:02:04\n",
            "epoch [2/50] batch [1/1] time 0.568 (0.568) data 0.416 (0.416) loss 2.7539 (2.7539) acc 31.2500 (31.2500) lr 1.9980e-03 eta 0:00:27\n",
            "epoch [3/50] batch [1/1] time 0.537 (0.537) data 0.389 (0.389) loss 1.9775 (1.9775) acc 43.7500 (43.7500) lr 1.9921e-03 eta 0:00:25\n",
            "epoch [4/50] batch [1/1] time 0.538 (0.538) data 0.392 (0.392) loss 1.3994 (1.3994) acc 68.7500 (68.7500) lr 1.9823e-03 eta 0:00:24\n",
            "epoch [5/50] batch [1/1] time 0.567 (0.567) data 0.424 (0.424) loss 1.5107 (1.5107) acc 65.6250 (65.6250) lr 1.9686e-03 eta 0:00:25\n",
            "epoch [6/50] batch [1/1] time 0.537 (0.537) data 0.392 (0.392) loss 0.8447 (0.8447) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:00:23\n",
            "epoch [7/50] batch [1/1] time 0.532 (0.532) data 0.387 (0.387) loss 0.7612 (0.7612) acc 78.1250 (78.1250) lr 1.9298e-03 eta 0:00:22\n",
            "epoch [8/50] batch [1/1] time 0.541 (0.541) data 0.397 (0.397) loss 0.7256 (0.7256) acc 71.8750 (71.8750) lr 1.9048e-03 eta 0:00:22\n",
            "epoch [9/50] batch [1/1] time 0.530 (0.530) data 0.387 (0.387) loss 0.5791 (0.5791) acc 87.5000 (87.5000) lr 1.8763e-03 eta 0:00:21\n",
            "epoch [10/50] batch [1/1] time 0.723 (0.723) data 0.579 (0.579) loss 0.7354 (0.7354) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:00:28\n",
            "epoch [11/50] batch [1/1] time 0.736 (0.736) data 0.591 (0.591) loss 0.7397 (0.7397) acc 75.0000 (75.0000) lr 1.8090e-03 eta 0:00:28\n",
            "epoch [12/50] batch [1/1] time 0.734 (0.734) data 0.590 (0.590) loss 0.6533 (0.6533) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:00:27\n",
            "epoch [13/50] batch [1/1] time 0.556 (0.556) data 0.411 (0.411) loss 0.8037 (0.8037) acc 75.0000 (75.0000) lr 1.7290e-03 eta 0:00:20\n",
            "epoch [14/50] batch [1/1] time 0.544 (0.544) data 0.401 (0.401) loss 0.4746 (0.4746) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:00:19\n",
            "epoch [15/50] batch [1/1] time 0.553 (0.553) data 0.410 (0.410) loss 0.5806 (0.5806) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:00:19\n",
            "epoch [16/50] batch [1/1] time 0.529 (0.529) data 0.385 (0.385) loss 0.5229 (0.5229) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:00:17\n",
            "epoch [17/50] batch [1/1] time 0.548 (0.548) data 0.402 (0.402) loss 0.6729 (0.6729) acc 87.5000 (87.5000) lr 1.5358e-03 eta 0:00:18\n",
            "epoch [18/50] batch [1/1] time 0.555 (0.555) data 0.411 (0.411) loss 0.6748 (0.6748) acc 84.3750 (84.3750) lr 1.4818e-03 eta 0:00:17\n",
            "epoch [19/50] batch [1/1] time 0.542 (0.542) data 0.396 (0.396) loss 0.4194 (0.4194) acc 87.5000 (87.5000) lr 1.4258e-03 eta 0:00:16\n",
            "epoch [20/50] batch [1/1] time 0.538 (0.538) data 0.393 (0.393) loss 0.1527 (0.1527) acc 96.8750 (96.8750) lr 1.3681e-03 eta 0:00:16\n",
            "epoch [21/50] batch [1/1] time 0.554 (0.554) data 0.411 (0.411) loss 0.2737 (0.2737) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:00:16\n",
            "epoch [22/50] batch [1/1] time 0.531 (0.531) data 0.388 (0.388) loss 0.3154 (0.3154) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:00:14\n",
            "epoch [23/50] batch [1/1] time 0.618 (0.618) data 0.474 (0.474) loss 0.4421 (0.4421) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:00:16\n",
            "epoch [24/50] batch [1/1] time 0.554 (0.554) data 0.410 (0.410) loss 0.6523 (0.6523) acc 81.2500 (81.2500) lr 1.1253e-03 eta 0:00:14\n",
            "epoch [25/50] batch [1/1] time 0.534 (0.534) data 0.391 (0.391) loss 0.4885 (0.4885) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:00:13\n",
            "epoch [26/50] batch [1/1] time 0.551 (0.551) data 0.404 (0.404) loss 0.6182 (0.6182) acc 84.3750 (84.3750) lr 1.0000e-03 eta 0:00:13\n",
            "epoch [27/50] batch [1/1] time 0.728 (0.728) data 0.581 (0.581) loss 0.4412 (0.4412) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:00:16\n",
            "epoch [28/50] batch [1/1] time 0.725 (0.725) data 0.580 (0.580) loss 0.5244 (0.5244) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:00:15\n",
            "epoch [29/50] batch [1/1] time 0.758 (0.758) data 0.613 (0.613) loss 0.3267 (0.3267) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:00:15\n",
            "epoch [30/50] batch [1/1] time 0.540 (0.540) data 0.393 (0.393) loss 0.3315 (0.3315) acc 90.6250 (90.6250) lr 7.5131e-04 eta 0:00:10\n",
            "epoch [31/50] batch [1/1] time 0.538 (0.538) data 0.393 (0.393) loss 0.4399 (0.4399) acc 87.5000 (87.5000) lr 6.9098e-04 eta 0:00:10\n",
            "epoch [32/50] batch [1/1] time 0.543 (0.543) data 0.398 (0.398) loss 0.3357 (0.3357) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:00:09\n",
            "epoch [33/50] batch [1/1] time 0.533 (0.533) data 0.388 (0.388) loss 0.5142 (0.5142) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:00:09\n",
            "epoch [34/50] batch [1/1] time 0.538 (0.538) data 0.387 (0.387) loss 0.4978 (0.4978) acc 87.5000 (87.5000) lr 5.1825e-04 eta 0:00:08\n",
            "epoch [35/50] batch [1/1] time 0.544 (0.544) data 0.397 (0.397) loss 0.3496 (0.3496) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:00:08\n",
            "epoch [36/50] batch [1/1] time 0.528 (0.528) data 0.383 (0.383) loss 0.5059 (0.5059) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:07\n",
            "epoch [37/50] batch [1/1] time 0.535 (0.535) data 0.395 (0.395) loss 0.4453 (0.4453) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:06\n",
            "epoch [38/50] batch [1/1] time 0.561 (0.561) data 0.418 (0.418) loss 0.3726 (0.3726) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:06\n",
            "epoch [39/50] batch [1/1] time 0.525 (0.525) data 0.382 (0.382) loss 0.3618 (0.3618) acc 87.5000 (87.5000) lr 2.7103e-04 eta 0:00:05\n",
            "epoch [40/50] batch [1/1] time 0.542 (0.542) data 0.397 (0.397) loss 0.3777 (0.3777) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:05\n",
            "epoch [41/50] batch [1/1] time 0.546 (0.546) data 0.400 (0.400) loss 0.3132 (0.3132) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [42/50] batch [1/1] time 0.536 (0.536) data 0.392 (0.392) loss 0.3630 (0.3630) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:00:04\n",
            "epoch [43/50] batch [1/1] time 0.564 (0.564) data 0.417 (0.417) loss 0.3843 (0.3843) acc 84.3750 (84.3750) lr 1.2369e-04 eta 0:00:03\n",
            "epoch [44/50] batch [1/1] time 0.654 (0.654) data 0.507 (0.507) loss 0.4932 (0.4932) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:00:03\n",
            "epoch [45/50] batch [1/1] time 0.740 (0.740) data 0.594 (0.594) loss 0.2725 (0.2725) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:03\n",
            "epoch [46/50] batch [1/1] time 0.746 (0.746) data 0.593 (0.593) loss 0.3486 (0.3486) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:02\n",
            "epoch [47/50] batch [1/1] time 0.545 (0.545) data 0.398 (0.398) loss 0.3401 (0.3401) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:01\n",
            "epoch [48/50] batch [1/1] time 0.563 (0.563) data 0.417 (0.417) loss 0.2510 (0.2510) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:01\n",
            "epoch [49/50] batch [1/1] time 0.535 (0.535) data 0.391 (0.391) loss 0.5098 (0.5098) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.533 (0.533) data 0.388 (0.388) loss 0.3726 (0.3726) acc 93.7500 (93.7500) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_1shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:26<00:00,  1.38it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,328\n",
            "* accuracy: 90.7%\n",
            "* error: 9.3%\n",
            "* macro_f1: 90.6%\n",
            "Elapsed: 0:01:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display training results for 1-shot experiment\n",
        "!echo \"=== Training Log (last 30 lines) ===\"\n",
        "!tail -30 output/oxford_pets_1shot/log.txt\n",
        "\n",
        "!echo \"\\n=== Final Accuracy ===\"\n",
        "!grep \"accuracy:\" output/oxford_pets_1shot/log.txt | tail -3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSm8Dp63C-ZL",
        "outputId": "9ca347ce-88e6-4bf3-a282-75f6b4095dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Log (last 30 lines) ===\n",
            "epoch [32/50] batch [1/1] time 0.543 (0.543) data 0.398 (0.398) loss 0.3357 (0.3357) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:00:09\n",
            "epoch [33/50] batch [1/1] time 0.533 (0.533) data 0.388 (0.388) loss 0.5142 (0.5142) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:00:09\n",
            "epoch [34/50] batch [1/1] time 0.538 (0.538) data 0.387 (0.387) loss 0.4978 (0.4978) acc 87.5000 (87.5000) lr 5.1825e-04 eta 0:00:08\n",
            "epoch [35/50] batch [1/1] time 0.544 (0.544) data 0.397 (0.397) loss 0.3496 (0.3496) acc 93.7500 (93.7500) lr 4.6417e-04 eta 0:00:08\n",
            "epoch [36/50] batch [1/1] time 0.528 (0.528) data 0.383 (0.383) loss 0.5059 (0.5059) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:07\n",
            "epoch [37/50] batch [1/1] time 0.535 (0.535) data 0.395 (0.395) loss 0.4453 (0.4453) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:06\n",
            "epoch [38/50] batch [1/1] time 0.561 (0.561) data 0.418 (0.418) loss 0.3726 (0.3726) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:06\n",
            "epoch [39/50] batch [1/1] time 0.525 (0.525) data 0.382 (0.382) loss 0.3618 (0.3618) acc 87.5000 (87.5000) lr 2.7103e-04 eta 0:00:05\n",
            "epoch [40/50] batch [1/1] time 0.542 (0.542) data 0.397 (0.397) loss 0.3777 (0.3777) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:05\n",
            "epoch [41/50] batch [1/1] time 0.546 (0.546) data 0.400 (0.400) loss 0.3132 (0.3132) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [42/50] batch [1/1] time 0.536 (0.536) data 0.392 (0.392) loss 0.3630 (0.3630) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:00:04\n",
            "epoch [43/50] batch [1/1] time 0.564 (0.564) data 0.417 (0.417) loss 0.3843 (0.3843) acc 84.3750 (84.3750) lr 1.2369e-04 eta 0:00:03\n",
            "epoch [44/50] batch [1/1] time 0.654 (0.654) data 0.507 (0.507) loss 0.4932 (0.4932) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:00:03\n",
            "epoch [45/50] batch [1/1] time 0.740 (0.740) data 0.594 (0.594) loss 0.2725 (0.2725) acc 87.5000 (87.5000) lr 7.0224e-05 eta 0:00:03\n",
            "epoch [46/50] batch [1/1] time 0.746 (0.746) data 0.593 (0.593) loss 0.3486 (0.3486) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:02\n",
            "epoch [47/50] batch [1/1] time 0.545 (0.545) data 0.398 (0.398) loss 0.3401 (0.3401) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:01\n",
            "epoch [48/50] batch [1/1] time 0.563 (0.563) data 0.417 (0.417) loss 0.2510 (0.2510) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:01\n",
            "epoch [49/50] batch [1/1] time 0.535 (0.535) data 0.391 (0.391) loss 0.5098 (0.5098) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.533 (0.533) data 0.388 (0.388) loss 0.3726 (0.3726) acc 93.7500 (93.7500) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_1shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,328\n",
            "* accuracy: 90.7%\n",
            "* error: 9.3%\n",
            "* macro_f1: 90.6%\n",
            "Elapsed: 0:01:06\n",
            "\\n=== Final Accuracy ===\n",
            "* accuracy: 90.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training: 4-shot learning (~15 minutes)\n",
        "# Expected accuracy: ~92-93%\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_4shot \\\n",
        "DATASET.NUM_SHOTS 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c37QPCJ5ilZG",
        "outputId": "f366c80b-c6a7-4209-e5e8-8d4f66e53375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:25:56.508059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763234756.530716    3434 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763234756.536992    3434 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763234756.553766    3434 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234756.553798    3434 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234756.553802    3434 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234756.553808    3434 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:25:56.558813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/oxford_pets_4shot\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_4shot\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 4-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/CoOp/data/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  148\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_4shot/tensorboard)\n",
            "epoch [1/50] batch [1/4] time 2.527 (2.527) data 0.730 (0.730) loss 2.8184 (2.8184) acc 40.6250 (40.6250) lr 1.0000e-05 eta 0:08:22\n",
            "epoch [1/50] batch [2/4] time 0.147 (1.337) data 0.001 (0.366) loss 2.5000 (2.6592) acc 40.6250 (40.6250) lr 1.0000e-05 eta 0:04:24\n",
            "epoch [1/50] batch [3/4] time 0.149 (0.941) data 0.000 (0.244) loss 2.8262 (2.7148) acc 43.7500 (41.6667) lr 1.0000e-05 eta 0:03:05\n",
            "epoch [1/50] batch [4/4] time 0.149 (0.743) data 0.000 (0.183) loss 2.4863 (2.6577) acc 43.7500 (42.1875) lr 2.0000e-03 eta 0:02:25\n",
            "epoch [2/50] batch [1/4] time 1.038 (1.038) data 0.891 (0.891) loss 2.6113 (2.6113) acc 46.8750 (46.8750) lr 2.0000e-03 eta 0:03:22\n",
            "epoch [2/50] batch [2/4] time 0.150 (0.594) data 0.001 (0.446) loss 1.7666 (2.1890) acc 53.1250 (50.0000) lr 2.0000e-03 eta 0:01:55\n",
            "epoch [2/50] batch [3/4] time 0.149 (0.446) data 0.000 (0.297) loss 2.1699 (2.1826) acc 37.5000 (45.8333) lr 2.0000e-03 eta 0:01:26\n",
            "epoch [2/50] batch [4/4] time 0.151 (0.372) data 0.000 (0.223) loss 1.0078 (1.8889) acc 68.7500 (51.5625) lr 1.9980e-03 eta 0:01:11\n",
            "epoch [3/50] batch [1/4] time 0.994 (0.994) data 0.840 (0.840) loss 0.8774 (0.8774) acc 71.8750 (71.8750) lr 1.9980e-03 eta 0:03:09\n",
            "epoch [3/50] batch [2/4] time 0.149 (0.571) data 0.001 (0.420) loss 0.6973 (0.7874) acc 75.0000 (73.4375) lr 1.9980e-03 eta 0:01:48\n",
            "epoch [3/50] batch [3/4] time 0.148 (0.430) data 0.000 (0.280) loss 0.6367 (0.7371) acc 75.0000 (73.9583) lr 1.9980e-03 eta 0:01:21\n",
            "epoch [3/50] batch [4/4] time 0.149 (0.360) data 0.000 (0.210) loss 0.4270 (0.6596) acc 87.5000 (77.3438) lr 1.9921e-03 eta 0:01:07\n",
            "epoch [4/50] batch [1/4] time 1.668 (1.668) data 1.517 (1.517) loss 0.5913 (0.5913) acc 81.2500 (81.2500) lr 1.9921e-03 eta 0:05:11\n",
            "epoch [4/50] batch [2/4] time 0.148 (0.908) data 0.001 (0.759) loss 0.5649 (0.5781) acc 81.2500 (81.2500) lr 1.9921e-03 eta 0:02:48\n",
            "epoch [4/50] batch [3/4] time 0.149 (0.655) data 0.000 (0.506) loss 0.4731 (0.5431) acc 81.2500 (81.2500) lr 1.9921e-03 eta 0:02:01\n",
            "epoch [4/50] batch [4/4] time 0.150 (0.529) data 0.000 (0.379) loss 0.5854 (0.5537) acc 78.1250 (80.4688) lr 1.9823e-03 eta 0:01:37\n",
            "epoch [5/50] batch [1/4] time 0.931 (0.931) data 0.770 (0.770) loss 0.6221 (0.6221) acc 78.1250 (78.1250) lr 1.9823e-03 eta 0:02:50\n",
            "epoch [5/50] batch [2/4] time 0.148 (0.540) data 0.000 (0.385) loss 0.5449 (0.5835) acc 78.1250 (78.1250) lr 1.9823e-03 eta 0:01:38\n",
            "epoch [5/50] batch [3/4] time 0.150 (0.410) data 0.000 (0.257) loss 0.6846 (0.6172) acc 81.2500 (79.1667) lr 1.9823e-03 eta 0:01:14\n",
            "epoch [5/50] batch [4/4] time 0.148 (0.344) data 0.000 (0.193) loss 1.0830 (0.7336) acc 71.8750 (77.3438) lr 1.9686e-03 eta 0:01:02\n",
            "epoch [6/50] batch [1/4] time 0.956 (0.956) data 0.806 (0.806) loss 0.6982 (0.6982) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:02:51\n",
            "epoch [6/50] batch [2/4] time 0.149 (0.552) data 0.000 (0.403) loss 0.7354 (0.7168) acc 65.6250 (75.0000) lr 1.9686e-03 eta 0:01:38\n",
            "epoch [6/50] batch [3/4] time 0.148 (0.417) data 0.000 (0.269) loss 0.4468 (0.6268) acc 84.3750 (78.1250) lr 1.9686e-03 eta 0:01:13\n",
            "epoch [6/50] batch [4/4] time 0.149 (0.350) data 0.000 (0.202) loss 0.6768 (0.6393) acc 81.2500 (78.9062) lr 1.9511e-03 eta 0:01:01\n",
            "epoch [7/50] batch [1/4] time 0.990 (0.990) data 0.840 (0.840) loss 0.6436 (0.6436) acc 71.8750 (71.8750) lr 1.9511e-03 eta 0:02:53\n",
            "epoch [7/50] batch [2/4] time 0.150 (0.570) data 0.001 (0.420) loss 0.6138 (0.6287) acc 75.0000 (73.4375) lr 1.9511e-03 eta 0:01:39\n",
            "epoch [7/50] batch [3/4] time 0.149 (0.430) data 0.000 (0.280) loss 0.6216 (0.6263) acc 75.0000 (73.9583) lr 1.9511e-03 eta 0:01:14\n",
            "epoch [7/50] batch [4/4] time 0.150 (0.360) data 0.000 (0.210) loss 0.6636 (0.6356) acc 84.3750 (76.5625) lr 1.9298e-03 eta 0:01:01\n",
            "epoch [8/50] batch [1/4] time 1.052 (1.052) data 0.862 (0.862) loss 0.4351 (0.4351) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:02:59\n",
            "epoch [8/50] batch [2/4] time 0.148 (0.600) data 0.000 (0.431) loss 0.6890 (0.5620) acc 81.2500 (84.3750) lr 1.9298e-03 eta 0:01:42\n",
            "epoch [8/50] batch [3/4] time 0.151 (0.450) data 0.000 (0.288) loss 0.5752 (0.5664) acc 84.3750 (84.3750) lr 1.9298e-03 eta 0:01:16\n",
            "epoch [8/50] batch [4/4] time 0.151 (0.376) data 0.000 (0.216) loss 0.7378 (0.6093) acc 78.1250 (82.8125) lr 1.9048e-03 eta 0:01:03\n",
            "epoch [9/50] batch [1/4] time 1.042 (1.042) data 0.895 (0.895) loss 0.3833 (0.3833) acc 93.7500 (93.7500) lr 1.9048e-03 eta 0:02:54\n",
            "epoch [9/50] batch [2/4] time 0.150 (0.596) data 0.001 (0.448) loss 0.6289 (0.5061) acc 84.3750 (89.0625) lr 1.9048e-03 eta 0:01:38\n",
            "epoch [9/50] batch [3/4] time 0.150 (0.447) data 0.000 (0.299) loss 0.5449 (0.5190) acc 87.5000 (88.5417) lr 1.9048e-03 eta 0:01:13\n",
            "epoch [9/50] batch [4/4] time 0.151 (0.373) data 0.000 (0.224) loss 0.7856 (0.5857) acc 81.2500 (86.7188) lr 1.8763e-03 eta 0:01:01\n",
            "epoch [10/50] batch [1/4] time 1.032 (1.032) data 0.883 (0.883) loss 0.9604 (0.9604) acc 65.6250 (65.6250) lr 1.8763e-03 eta 0:02:48\n",
            "epoch [10/50] batch [2/4] time 0.149 (0.591) data 0.000 (0.442) loss 0.7915 (0.8760) acc 75.0000 (70.3125) lr 1.8763e-03 eta 0:01:35\n",
            "epoch [10/50] batch [3/4] time 0.151 (0.444) data 0.000 (0.295) loss 0.3491 (0.7004) acc 90.6250 (77.0833) lr 1.8763e-03 eta 0:01:11\n",
            "epoch [10/50] batch [4/4] time 0.151 (0.371) data 0.000 (0.221) loss 0.7373 (0.7096) acc 71.8750 (75.7812) lr 1.8443e-03 eta 0:00:59\n",
            "epoch [11/50] batch [1/4] time 1.225 (1.225) data 1.074 (1.074) loss 0.7832 (0.7832) acc 75.0000 (75.0000) lr 1.8443e-03 eta 0:03:14\n",
            "epoch [11/50] batch [2/4] time 0.151 (0.688) data 0.000 (0.537) loss 0.6318 (0.7075) acc 75.0000 (75.0000) lr 1.8443e-03 eta 0:01:48\n",
            "epoch [11/50] batch [3/4] time 0.151 (0.509) data 0.000 (0.358) loss 0.5996 (0.6715) acc 78.1250 (76.0417) lr 1.8443e-03 eta 0:01:19\n",
            "epoch [11/50] batch [4/4] time 0.151 (0.419) data 0.000 (0.269) loss 0.6348 (0.6624) acc 75.0000 (75.7812) lr 1.8090e-03 eta 0:01:05\n",
            "epoch [12/50] batch [1/4] time 1.430 (1.430) data 1.273 (1.273) loss 0.6294 (0.6294) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:03:41\n",
            "epoch [12/50] batch [2/4] time 0.151 (0.790) data 0.001 (0.637) loss 0.6099 (0.6196) acc 81.2500 (82.8125) lr 1.8090e-03 eta 0:02:01\n",
            "epoch [12/50] batch [3/4] time 0.149 (0.577) data 0.000 (0.425) loss 0.7930 (0.6774) acc 71.8750 (79.1667) lr 1.8090e-03 eta 0:01:28\n",
            "epoch [12/50] batch [4/4] time 0.150 (0.470) data 0.000 (0.318) loss 0.3428 (0.5938) acc 87.5000 (81.2500) lr 1.7705e-03 eta 0:01:11\n",
            "epoch [13/50] batch [1/4] time 0.973 (0.973) data 0.816 (0.816) loss 0.6504 (0.6504) acc 75.0000 (75.0000) lr 1.7705e-03 eta 0:02:26\n",
            "epoch [13/50] batch [2/4] time 0.150 (0.561) data 0.000 (0.408) loss 0.5825 (0.6165) acc 75.0000 (75.0000) lr 1.7705e-03 eta 0:01:24\n",
            "epoch [13/50] batch [3/4] time 0.149 (0.424) data 0.000 (0.272) loss 0.4946 (0.5758) acc 81.2500 (77.0833) lr 1.7705e-03 eta 0:01:03\n",
            "epoch [13/50] batch [4/4] time 0.150 (0.356) data 0.000 (0.204) loss 0.3955 (0.5308) acc 87.5000 (79.6875) lr 1.7290e-03 eta 0:00:52\n",
            "epoch [14/50] batch [1/4] time 1.014 (1.014) data 0.867 (0.867) loss 0.6372 (0.6372) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:02:29\n",
            "epoch [14/50] batch [2/4] time 0.152 (0.583) data 0.001 (0.434) loss 0.5908 (0.6140) acc 84.3750 (84.3750) lr 1.7290e-03 eta 0:01:25\n",
            "epoch [14/50] batch [3/4] time 0.151 (0.439) data 0.000 (0.289) loss 0.3049 (0.5110) acc 93.7500 (87.5000) lr 1.7290e-03 eta 0:01:03\n",
            "epoch [14/50] batch [4/4] time 0.152 (0.367) data 0.000 (0.217) loss 0.5762 (0.5273) acc 84.3750 (86.7188) lr 1.6845e-03 eta 0:00:52\n",
            "epoch [15/50] batch [1/4] time 1.020 (1.020) data 0.872 (0.872) loss 0.4585 (0.4585) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:02:25\n",
            "epoch [15/50] batch [2/4] time 0.151 (0.586) data 0.000 (0.436) loss 0.4666 (0.4625) acc 84.3750 (85.9375) lr 1.6845e-03 eta 0:01:23\n",
            "epoch [15/50] batch [3/4] time 0.151 (0.441) data 0.000 (0.291) loss 0.7124 (0.5458) acc 78.1250 (83.3333) lr 1.6845e-03 eta 0:01:02\n",
            "epoch [15/50] batch [4/4] time 0.151 (0.368) data 0.000 (0.218) loss 0.6528 (0.5726) acc 81.2500 (82.8125) lr 1.6374e-03 eta 0:00:51\n",
            "epoch [16/50] batch [1/4] time 0.985 (0.985) data 0.835 (0.835) loss 0.6162 (0.6162) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:02:16\n",
            "epoch [16/50] batch [2/4] time 0.150 (0.567) data 0.000 (0.418) loss 0.4106 (0.5134) acc 90.6250 (85.9375) lr 1.6374e-03 eta 0:01:18\n",
            "epoch [16/50] batch [3/4] time 0.152 (0.429) data 0.000 (0.278) loss 0.7729 (0.5999) acc 81.2500 (84.3750) lr 1.6374e-03 eta 0:00:58\n",
            "epoch [16/50] batch [4/4] time 0.151 (0.359) data 0.000 (0.209) loss 0.4814 (0.5703) acc 90.6250 (85.9375) lr 1.5878e-03 eta 0:00:48\n",
            "epoch [17/50] batch [1/4] time 0.851 (0.851) data 0.681 (0.681) loss 0.4500 (0.4500) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [17/50] batch [2/4] time 0.151 (0.501) data 0.000 (0.340) loss 0.3328 (0.3914) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:01:07\n",
            "epoch [17/50] batch [3/4] time 0.151 (0.384) data 0.000 (0.227) loss 0.3508 (0.3778) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:00:51\n",
            "epoch [17/50] batch [4/4] time 0.151 (0.326) data 0.000 (0.170) loss 0.3228 (0.3641) acc 87.5000 (85.1562) lr 1.5358e-03 eta 0:00:43\n",
            "epoch [18/50] batch [1/4] time 0.953 (0.953) data 0.799 (0.799) loss 0.4844 (0.4844) acc 81.2500 (81.2500) lr 1.5358e-03 eta 0:02:04\n",
            "epoch [18/50] batch [2/4] time 0.152 (0.552) data 0.001 (0.400) loss 0.4138 (0.4491) acc 90.6250 (85.9375) lr 1.5358e-03 eta 0:01:11\n",
            "epoch [18/50] batch [3/4] time 0.152 (0.419) data 0.000 (0.267) loss 0.5239 (0.4740) acc 87.5000 (86.4583) lr 1.5358e-03 eta 0:00:54\n",
            "epoch [18/50] batch [4/4] time 0.152 (0.352) data 0.000 (0.200) loss 0.5659 (0.4970) acc 87.5000 (86.7188) lr 1.4818e-03 eta 0:00:45\n",
            "epoch [19/50] batch [1/4] time 1.666 (1.666) data 1.517 (1.517) loss 0.2886 (0.2886) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:03:31\n",
            "epoch [19/50] batch [2/4] time 0.152 (0.909) data 0.000 (0.759) loss 0.4653 (0.3770) acc 90.6250 (89.0625) lr 1.4818e-03 eta 0:01:54\n",
            "epoch [19/50] batch [3/4] time 0.152 (0.657) data 0.000 (0.506) loss 0.7886 (0.5142) acc 75.0000 (84.3750) lr 1.4818e-03 eta 0:01:22\n",
            "epoch [19/50] batch [4/4] time 0.147 (0.529) data 0.000 (0.379) loss 0.6055 (0.5370) acc 81.2500 (83.5938) lr 1.4258e-03 eta 0:01:05\n",
            "epoch [20/50] batch [1/4] time 1.177 (1.177) data 1.028 (1.028) loss 0.7729 (0.7729) acc 81.2500 (81.2500) lr 1.4258e-03 eta 0:02:24\n",
            "epoch [20/50] batch [2/4] time 0.151 (0.664) data 0.000 (0.514) loss 0.4048 (0.5889) acc 87.5000 (84.3750) lr 1.4258e-03 eta 0:01:21\n",
            "epoch [20/50] batch [3/4] time 0.152 (0.494) data 0.000 (0.343) loss 0.5435 (0.5737) acc 84.3750 (84.3750) lr 1.4258e-03 eta 0:00:59\n",
            "epoch [20/50] batch [4/4] time 0.151 (0.408) data 0.000 (0.257) loss 0.5479 (0.5673) acc 84.3750 (84.3750) lr 1.3681e-03 eta 0:00:48\n",
            "epoch [21/50] batch [1/4] time 1.041 (1.041) data 0.893 (0.893) loss 0.3667 (0.3667) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:02:03\n",
            "epoch [21/50] batch [2/4] time 0.151 (0.596) data 0.000 (0.447) loss 0.4194 (0.3931) acc 93.7500 (90.6250) lr 1.3681e-03 eta 0:01:10\n",
            "epoch [21/50] batch [3/4] time 0.151 (0.448) data 0.000 (0.298) loss 0.3318 (0.3726) acc 96.8750 (92.7083) lr 1.3681e-03 eta 0:00:52\n",
            "epoch [21/50] batch [4/4] time 0.151 (0.374) data 0.000 (0.223) loss 0.3518 (0.3674) acc 84.3750 (90.6250) lr 1.3090e-03 eta 0:00:43\n",
            "epoch [22/50] batch [1/4] time 1.028 (1.028) data 0.880 (0.880) loss 0.5186 (0.5186) acc 84.3750 (84.3750) lr 1.3090e-03 eta 0:01:58\n",
            "epoch [22/50] batch [2/4] time 0.142 (0.585) data 0.000 (0.440) loss 0.8506 (0.6846) acc 71.8750 (78.1250) lr 1.3090e-03 eta 0:01:06\n",
            "epoch [22/50] batch [3/4] time 0.151 (0.440) data 0.000 (0.294) loss 0.5747 (0.6479) acc 84.3750 (80.2083) lr 1.3090e-03 eta 0:00:49\n",
            "epoch [22/50] batch [4/4] time 0.153 (0.369) data 0.000 (0.220) loss 0.2783 (0.5555) acc 87.5000 (82.0312) lr 1.2487e-03 eta 0:00:41\n",
            "epoch [23/50] batch [1/4] time 0.955 (0.955) data 0.797 (0.797) loss 0.5283 (0.5283) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:01:46\n",
            "epoch [23/50] batch [2/4] time 0.151 (0.553) data 0.000 (0.399) loss 0.3281 (0.4282) acc 93.7500 (92.1875) lr 1.2487e-03 eta 0:01:00\n",
            "epoch [23/50] batch [3/4] time 0.152 (0.420) data 0.000 (0.266) loss 0.5181 (0.4582) acc 81.2500 (88.5417) lr 1.2487e-03 eta 0:00:45\n",
            "epoch [23/50] batch [4/4] time 0.151 (0.353) data 0.000 (0.200) loss 0.6123 (0.4967) acc 84.3750 (87.5000) lr 1.1874e-03 eta 0:00:38\n",
            "epoch [24/50] batch [1/4] time 0.971 (0.971) data 0.819 (0.819) loss 0.8062 (0.8062) acc 75.0000 (75.0000) lr 1.1874e-03 eta 0:01:43\n",
            "epoch [24/50] batch [2/4] time 0.152 (0.562) data 0.000 (0.410) loss 0.2708 (0.5385) acc 96.8750 (85.9375) lr 1.1874e-03 eta 0:00:59\n",
            "epoch [24/50] batch [3/4] time 0.152 (0.425) data 0.000 (0.273) loss 0.4175 (0.4981) acc 87.5000 (86.4583) lr 1.1874e-03 eta 0:00:44\n",
            "epoch [24/50] batch [4/4] time 0.152 (0.357) data 0.000 (0.205) loss 0.4592 (0.4884) acc 84.3750 (85.9375) lr 1.1253e-03 eta 0:00:37\n",
            "epoch [25/50] batch [1/4] time 1.006 (1.006) data 0.858 (0.858) loss 0.4192 (0.4192) acc 90.6250 (90.6250) lr 1.1253e-03 eta 0:01:43\n",
            "epoch [25/50] batch [2/4] time 0.152 (0.579) data 0.000 (0.429) loss 0.0978 (0.2585) acc 100.0000 (95.3125) lr 1.1253e-03 eta 0:00:59\n",
            "epoch [25/50] batch [3/4] time 0.151 (0.437) data 0.000 (0.286) loss 0.3206 (0.2792) acc 96.8750 (95.8333) lr 1.1253e-03 eta 0:00:44\n",
            "epoch [25/50] batch [4/4] time 0.152 (0.366) data 0.000 (0.215) loss 0.3298 (0.2918) acc 90.6250 (94.5312) lr 1.0628e-03 eta 0:00:36\n",
            "epoch [26/50] batch [1/4] time 0.934 (0.934) data 0.646 (0.646) loss 0.5161 (0.5161) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:01:32\n",
            "epoch [26/50] batch [2/4] time 0.270 (0.602) data 0.112 (0.379) loss 0.2087 (0.3624) acc 100.0000 (95.3125) lr 1.0628e-03 eta 0:00:58\n",
            "epoch [26/50] batch [3/4] time 0.161 (0.455) data 0.000 (0.253) loss 0.5640 (0.4296) acc 78.1250 (89.5833) lr 1.0628e-03 eta 0:00:44\n",
            "epoch [26/50] batch [4/4] time 0.155 (0.380) data 0.000 (0.190) loss 0.4360 (0.4312) acc 90.6250 (89.8438) lr 1.0000e-03 eta 0:00:36\n",
            "epoch [27/50] batch [1/4] time 1.510 (1.510) data 1.356 (1.356) loss 0.4563 (0.4563) acc 84.3750 (84.3750) lr 1.0000e-03 eta 0:02:23\n",
            "epoch [27/50] batch [2/4] time 0.151 (0.830) data 0.000 (0.678) loss 0.8296 (0.6429) acc 78.1250 (81.2500) lr 1.0000e-03 eta 0:01:18\n",
            "epoch [27/50] batch [3/4] time 0.152 (0.604) data 0.000 (0.452) loss 0.5405 (0.6088) acc 87.5000 (83.3333) lr 1.0000e-03 eta 0:00:56\n",
            "epoch [27/50] batch [4/4] time 0.152 (0.491) data 0.000 (0.339) loss 0.3789 (0.5513) acc 87.5000 (84.3750) lr 9.3721e-04 eta 0:00:45\n",
            "epoch [28/50] batch [1/4] time 1.029 (1.029) data 0.880 (0.880) loss 0.4429 (0.4429) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:01:33\n",
            "epoch [28/50] batch [2/4] time 0.152 (0.590) data 0.000 (0.440) loss 0.3794 (0.4111) acc 93.7500 (89.0625) lr 9.3721e-04 eta 0:00:53\n",
            "epoch [28/50] batch [3/4] time 0.151 (0.444) data 0.000 (0.293) loss 0.3699 (0.3974) acc 87.5000 (88.5417) lr 9.3721e-04 eta 0:00:39\n",
            "epoch [28/50] batch [4/4] time 0.152 (0.371) data 0.000 (0.220) loss 0.2732 (0.3663) acc 93.7500 (89.8438) lr 8.7467e-04 eta 0:00:32\n",
            "epoch [29/50] batch [1/4] time 1.002 (1.002) data 0.847 (0.847) loss 0.3875 (0.3875) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:01:27\n",
            "epoch [29/50] batch [2/4] time 0.153 (0.577) data 0.000 (0.424) loss 0.4685 (0.4280) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:00:49\n",
            "epoch [29/50] batch [3/4] time 0.152 (0.436) data 0.000 (0.283) loss 0.1349 (0.3303) acc 96.8750 (88.5417) lr 8.7467e-04 eta 0:00:37\n",
            "epoch [29/50] batch [4/4] time 0.153 (0.365) data 0.000 (0.212) loss 0.4556 (0.3616) acc 84.3750 (87.5000) lr 8.1262e-04 eta 0:00:30\n",
            "epoch [30/50] batch [1/4] time 1.028 (1.028) data 0.875 (0.875) loss 0.5684 (0.5684) acc 84.3750 (84.3750) lr 8.1262e-04 eta 0:01:25\n",
            "epoch [30/50] batch [2/4] time 0.152 (0.590) data 0.001 (0.438) loss 0.3608 (0.4646) acc 93.7500 (89.0625) lr 8.1262e-04 eta 0:00:48\n",
            "epoch [30/50] batch [3/4] time 0.153 (0.444) data 0.000 (0.292) loss 0.7817 (0.5703) acc 78.1250 (85.4167) lr 8.1262e-04 eta 0:00:35\n",
            "epoch [30/50] batch [4/4] time 0.154 (0.372) data 0.000 (0.219) loss 0.2242 (0.4838) acc 96.8750 (88.2812) lr 7.5131e-04 eta 0:00:29\n",
            "epoch [31/50] batch [1/4] time 0.964 (0.964) data 0.801 (0.801) loss 0.2228 (0.2228) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:01:16\n",
            "epoch [31/50] batch [2/4] time 0.153 (0.559) data 0.001 (0.401) loss 0.4819 (0.3524) acc 90.6250 (92.1875) lr 7.5131e-04 eta 0:00:43\n",
            "epoch [31/50] batch [3/4] time 0.153 (0.424) data 0.000 (0.267) loss 0.4133 (0.3727) acc 87.5000 (90.6250) lr 7.5131e-04 eta 0:00:32\n",
            "epoch [31/50] batch [4/4] time 0.155 (0.356) data 0.000 (0.200) loss 0.3325 (0.3626) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:00:27\n",
            "epoch [32/50] batch [1/4] time 0.938 (0.938) data 0.770 (0.770) loss 0.4094 (0.4094) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:01:10\n",
            "epoch [32/50] batch [2/4] time 0.153 (0.546) data 0.001 (0.385) loss 0.3564 (0.3829) acc 87.5000 (89.0625) lr 6.9098e-04 eta 0:00:40\n",
            "epoch [32/50] batch [3/4] time 0.155 (0.415) data 0.000 (0.257) loss 0.3123 (0.3594) acc 93.7500 (90.6250) lr 6.9098e-04 eta 0:00:30\n",
            "epoch [32/50] batch [4/4] time 0.155 (0.350) data 0.001 (0.193) loss 0.2167 (0.3237) acc 96.8750 (92.1875) lr 6.3188e-04 eta 0:00:25\n",
            "epoch [33/50] batch [1/4] time 1.050 (1.050) data 0.898 (0.898) loss 0.2310 (0.2310) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:01:14\n",
            "epoch [33/50] batch [2/4] time 0.154 (0.602) data 0.001 (0.449) loss 0.2859 (0.2584) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:00:42\n",
            "epoch [33/50] batch [3/4] time 0.153 (0.452) data 0.000 (0.300) loss 0.3428 (0.2865) acc 93.7500 (91.6667) lr 6.3188e-04 eta 0:00:31\n",
            "epoch [33/50] batch [4/4] time 0.154 (0.377) data 0.000 (0.225) loss 1.0059 (0.4664) acc 71.8750 (86.7188) lr 5.7422e-04 eta 0:00:25\n",
            "epoch [34/50] batch [1/4] time 1.471 (1.471) data 1.313 (1.313) loss 0.3579 (0.3579) acc 90.6250 (90.6250) lr 5.7422e-04 eta 0:01:38\n",
            "epoch [34/50] batch [2/4] time 0.153 (0.812) data 0.000 (0.656) loss 0.4766 (0.4172) acc 87.5000 (89.0625) lr 5.7422e-04 eta 0:00:53\n",
            "epoch [34/50] batch [3/4] time 0.153 (0.592) data 0.000 (0.438) loss 0.8687 (0.5677) acc 75.0000 (84.3750) lr 5.7422e-04 eta 0:00:38\n",
            "epoch [34/50] batch [4/4] time 0.153 (0.483) data 0.000 (0.328) loss 0.3936 (0.5242) acc 93.7500 (86.7188) lr 5.1825e-04 eta 0:00:30\n",
            "epoch [35/50] batch [1/4] time 1.375 (1.375) data 1.223 (1.223) loss 0.2198 (0.2198) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:01:26\n",
            "epoch [35/50] batch [2/4] time 0.152 (0.764) data 0.000 (0.612) loss 0.5044 (0.3621) acc 90.6250 (93.7500) lr 5.1825e-04 eta 0:00:47\n",
            "epoch [35/50] batch [3/4] time 0.154 (0.561) data 0.000 (0.408) loss 0.5342 (0.4195) acc 87.5000 (91.6667) lr 5.1825e-04 eta 0:00:34\n",
            "epoch [35/50] batch [4/4] time 0.153 (0.459) data 0.000 (0.306) loss 0.1144 (0.3432) acc 100.0000 (93.7500) lr 4.6417e-04 eta 0:00:27\n",
            "epoch [36/50] batch [1/4] time 0.823 (0.823) data 0.644 (0.644) loss 0.4546 (0.4546) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:00:48\n",
            "epoch [36/50] batch [2/4] time 0.165 (0.494) data 0.004 (0.324) loss 0.5820 (0.5183) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:00:28\n",
            "epoch [36/50] batch [3/4] time 0.152 (0.380) data 0.000 (0.216) loss 0.5811 (0.5392) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:00:21\n",
            "epoch [36/50] batch [4/4] time 0.153 (0.323) data 0.000 (0.162) loss 0.3787 (0.4991) acc 81.2500 (85.9375) lr 4.1221e-04 eta 0:00:18\n",
            "epoch [37/50] batch [1/4] time 1.006 (1.006) data 0.847 (0.847) loss 0.4658 (0.4658) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:55\n",
            "epoch [37/50] batch [2/4] time 0.154 (0.580) data 0.001 (0.424) loss 0.4473 (0.4565) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:31\n",
            "epoch [37/50] batch [3/4] time 0.153 (0.438) data 0.000 (0.283) loss 0.2922 (0.4018) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [37/50] batch [4/4] time 0.153 (0.367) data 0.000 (0.212) loss 0.3191 (0.3811) acc 93.7500 (91.4062) lr 3.6258e-04 eta 0:00:19\n",
            "epoch [38/50] batch [1/4] time 1.020 (1.020) data 0.869 (0.869) loss 0.7070 (0.7070) acc 84.3750 (84.3750) lr 3.6258e-04 eta 0:00:52\n",
            "epoch [38/50] batch [2/4] time 0.154 (0.587) data 0.000 (0.435) loss 0.3467 (0.5269) acc 90.6250 (87.5000) lr 3.6258e-04 eta 0:00:29\n",
            "epoch [38/50] batch [3/4] time 0.154 (0.443) data 0.000 (0.290) loss 0.3440 (0.4659) acc 93.7500 (89.5833) lr 3.6258e-04 eta 0:00:21\n",
            "epoch [38/50] batch [4/4] time 0.153 (0.370) data 0.000 (0.218) loss 0.4739 (0.4679) acc 87.5000 (89.0625) lr 3.1545e-04 eta 0:00:17\n",
            "epoch [39/50] batch [1/4] time 1.002 (1.002) data 0.844 (0.844) loss 0.5522 (0.5522) acc 87.5000 (87.5000) lr 3.1545e-04 eta 0:00:47\n",
            "epoch [39/50] batch [2/4] time 0.155 (0.578) data 0.000 (0.422) loss 0.4436 (0.4979) acc 87.5000 (87.5000) lr 3.1545e-04 eta 0:00:26\n",
            "epoch [39/50] batch [3/4] time 0.155 (0.437) data 0.000 (0.282) loss 0.4778 (0.4912) acc 90.6250 (88.5417) lr 3.1545e-04 eta 0:00:19\n",
            "epoch [39/50] batch [4/4] time 0.154 (0.366) data 0.000 (0.211) loss 0.4316 (0.4763) acc 84.3750 (87.5000) lr 2.7103e-04 eta 0:00:16\n",
            "epoch [40/50] batch [1/4] time 0.960 (0.960) data 0.807 (0.807) loss 0.5376 (0.5376) acc 84.3750 (84.3750) lr 2.7103e-04 eta 0:00:41\n",
            "epoch [40/50] batch [2/4] time 0.155 (0.557) data 0.000 (0.404) loss 0.4832 (0.5104) acc 84.3750 (84.3750) lr 2.7103e-04 eta 0:00:23\n",
            "epoch [40/50] batch [3/4] time 0.155 (0.423) data 0.000 (0.269) loss 0.3538 (0.4582) acc 90.6250 (86.4583) lr 2.7103e-04 eta 0:00:17\n",
            "epoch [40/50] batch [4/4] time 0.155 (0.356) data 0.000 (0.202) loss 0.1979 (0.3931) acc 93.7500 (88.2812) lr 2.2949e-04 eta 0:00:14\n",
            "epoch [41/50] batch [1/4] time 0.877 (0.877) data 0.718 (0.718) loss 0.4866 (0.4866) acc 84.3750 (84.3750) lr 2.2949e-04 eta 0:00:34\n",
            "epoch [41/50] batch [2/4] time 0.156 (0.516) data 0.000 (0.359) loss 0.5693 (0.5280) acc 81.2500 (82.8125) lr 2.2949e-04 eta 0:00:19\n",
            "epoch [41/50] batch [3/4] time 0.154 (0.396) data 0.000 (0.239) loss 0.4336 (0.4965) acc 84.3750 (83.3333) lr 2.2949e-04 eta 0:00:14\n",
            "epoch [41/50] batch [4/4] time 0.157 (0.336) data 0.000 (0.180) loss 0.3140 (0.4509) acc 90.6250 (85.1562) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [42/50] batch [1/4] time 1.533 (1.533) data 1.380 (1.380) loss 0.4609 (0.4609) acc 84.3750 (84.3750) lr 1.9098e-04 eta 0:00:53\n",
            "epoch [42/50] batch [2/4] time 0.154 (0.844) data 0.000 (0.690) loss 0.1046 (0.2827) acc 100.0000 (92.1875) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [42/50] batch [3/4] time 0.154 (0.614) data 0.000 (0.460) loss 0.5576 (0.3744) acc 84.3750 (89.5833) lr 1.9098e-04 eta 0:00:20\n",
            "epoch [42/50] batch [4/4] time 0.152 (0.498) data 0.000 (0.345) loss 0.6128 (0.4340) acc 78.1250 (86.7188) lr 1.5567e-04 eta 0:00:15\n",
            "epoch [43/50] batch [1/4] time 0.959 (0.959) data 0.826 (0.826) loss 0.3994 (0.3994) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:29\n",
            "epoch [43/50] batch [2/4] time 0.156 (0.558) data 0.000 (0.413) loss 0.3616 (0.3805) acc 93.7500 (92.1875) lr 1.5567e-04 eta 0:00:16\n",
            "epoch [43/50] batch [3/4] time 0.154 (0.423) data 0.000 (0.275) loss 0.5161 (0.4257) acc 81.2500 (88.5417) lr 1.5567e-04 eta 0:00:12\n",
            "epoch [43/50] batch [4/4] time 0.155 (0.356) data 0.000 (0.207) loss 0.3127 (0.3975) acc 90.6250 (89.0625) lr 1.2369e-04 eta 0:00:09\n",
            "epoch [44/50] batch [1/4] time 0.814 (0.814) data 0.592 (0.592) loss 0.4094 (0.4094) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:21\n",
            "epoch [44/50] batch [2/4] time 0.396 (0.605) data 0.244 (0.418) loss 0.3865 (0.3979) acc 93.7500 (92.1875) lr 1.2369e-04 eta 0:00:15\n",
            "epoch [44/50] batch [3/4] time 0.155 (0.455) data 0.000 (0.279) loss 0.5034 (0.4331) acc 81.2500 (88.5417) lr 1.2369e-04 eta 0:00:11\n",
            "epoch [44/50] batch [4/4] time 0.154 (0.380) data 0.000 (0.209) loss 0.3918 (0.4228) acc 93.7500 (89.8438) lr 9.5173e-05 eta 0:00:09\n",
            "epoch [45/50] batch [1/4] time 0.832 (0.832) data 0.672 (0.672) loss 0.3428 (0.3428) acc 84.3750 (84.3750) lr 9.5173e-05 eta 0:00:19\n",
            "epoch [45/50] batch [2/4] time 0.231 (0.532) data 0.079 (0.376) loss 0.5332 (0.4380) acc 84.3750 (84.3750) lr 9.5173e-05 eta 0:00:11\n",
            "epoch [45/50] batch [3/4] time 0.155 (0.406) data 0.000 (0.251) loss 0.5176 (0.4645) acc 81.2500 (83.3333) lr 9.5173e-05 eta 0:00:08\n",
            "epoch [45/50] batch [4/4] time 0.156 (0.343) data 0.000 (0.188) loss 0.8384 (0.5580) acc 71.8750 (80.4688) lr 7.0224e-05 eta 0:00:06\n",
            "epoch [46/50] batch [1/4] time 1.005 (1.005) data 0.854 (0.854) loss 0.3506 (0.3506) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:19\n",
            "epoch [46/50] batch [2/4] time 0.155 (0.580) data 0.000 (0.427) loss 0.3196 (0.3351) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:10\n",
            "epoch [46/50] batch [3/4] time 0.154 (0.438) data 0.000 (0.285) loss 0.1918 (0.2873) acc 96.8750 (92.7083) lr 7.0224e-05 eta 0:00:07\n",
            "epoch [46/50] batch [4/4] time 0.155 (0.368) data 0.000 (0.214) loss 0.2341 (0.2740) acc 96.8750 (93.7500) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [1/4] time 0.777 (0.777) data 0.593 (0.593) loss 0.2712 (0.2712) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:11\n",
            "epoch [47/50] batch [2/4] time 0.276 (0.527) data 0.125 (0.359) loss 0.5132 (0.3922) acc 87.5000 (90.6250) lr 4.8943e-05 eta 0:00:07\n",
            "epoch [47/50] batch [3/4] time 0.162 (0.405) data 0.000 (0.239) loss 0.3152 (0.3665) acc 96.8750 (92.7083) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [4/4] time 0.155 (0.342) data 0.000 (0.180) loss 0.5000 (0.3999) acc 90.6250 (92.1875) lr 3.1417e-05 eta 0:00:04\n",
            "epoch [48/50] batch [1/4] time 0.995 (0.995) data 0.846 (0.846) loss 0.4922 (0.4922) acc 84.3750 (84.3750) lr 3.1417e-05 eta 0:00:10\n",
            "epoch [48/50] batch [2/4] time 0.154 (0.574) data 0.000 (0.423) loss 0.4871 (0.4896) acc 81.2500 (82.8125) lr 3.1417e-05 eta 0:00:05\n",
            "epoch [48/50] batch [3/4] time 0.156 (0.435) data 0.000 (0.282) loss 0.3633 (0.4475) acc 84.3750 (83.3333) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [4/4] time 0.155 (0.365) data 0.000 (0.212) loss 0.2164 (0.3897) acc 93.7500 (85.9375) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [1/4] time 1.469 (1.469) data 1.317 (1.317) loss 0.8564 (0.8564) acc 78.1250 (78.1250) lr 1.7713e-05 eta 0:00:10\n",
            "epoch [49/50] batch [2/4] time 0.155 (0.812) data 0.000 (0.658) loss 0.2068 (0.5316) acc 96.8750 (87.5000) lr 1.7713e-05 eta 0:00:04\n",
            "epoch [49/50] batch [3/4] time 0.156 (0.593) data 0.000 (0.439) loss 0.3733 (0.4788) acc 84.3750 (86.4583) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [4/4] time 0.155 (0.484) data 0.000 (0.329) loss 0.3254 (0.4405) acc 87.5000 (86.7188) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [1/4] time 1.362 (1.362) data 1.209 (1.209) loss 0.4207 (0.4207) acc 84.3750 (84.3750) lr 7.8853e-06 eta 0:00:04\n",
            "epoch [50/50] batch [2/4] time 0.155 (0.758) data 0.000 (0.605) loss 0.2629 (0.3418) acc 93.7500 (89.0625) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [3/4] time 0.154 (0.557) data 0.000 (0.403) loss 0.5371 (0.4069) acc 84.3750 (87.5000) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [4/4] time 0.153 (0.456) data 0.000 (0.303) loss 0.1539 (0.3437) acc 100.0000 (90.6250) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_4shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:26<00:00,  1.37it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,398\n",
            "* accuracy: 92.6%\n",
            "* error: 7.4%\n",
            "* macro_f1: 92.5%\n",
            "Elapsed: 0:01:54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display training results for 4-shot experiment\n",
        "!echo \"=== Training Log (last 30 lines) ===\"\n",
        "!tail -30 output/oxford_pets_4shot/log.txt\n",
        "\n",
        "!echo \"\\n=== Final Accuracy ===\"\n",
        "!grep \"accuracy:\" output/oxford_pets_4shot/log.txt | tail -3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqh27GKqirfW",
        "outputId": "a44e8c34-36d7-4f04-8a99-04d8b6eaeb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Log (last 30 lines) ===\n",
            "epoch [46/50] batch [2/4] time 0.155 (0.580) data 0.000 (0.427) loss 0.3196 (0.3351) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:10\n",
            "epoch [46/50] batch [3/4] time 0.154 (0.438) data 0.000 (0.285) loss 0.1918 (0.2873) acc 96.8750 (92.7083) lr 7.0224e-05 eta 0:00:07\n",
            "epoch [46/50] batch [4/4] time 0.155 (0.368) data 0.000 (0.214) loss 0.2341 (0.2740) acc 96.8750 (93.7500) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [1/4] time 0.777 (0.777) data 0.593 (0.593) loss 0.2712 (0.2712) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:11\n",
            "epoch [47/50] batch [2/4] time 0.276 (0.527) data 0.125 (0.359) loss 0.5132 (0.3922) acc 87.5000 (90.6250) lr 4.8943e-05 eta 0:00:07\n",
            "epoch [47/50] batch [3/4] time 0.162 (0.405) data 0.000 (0.239) loss 0.3152 (0.3665) acc 96.8750 (92.7083) lr 4.8943e-05 eta 0:00:05\n",
            "epoch [47/50] batch [4/4] time 0.155 (0.342) data 0.000 (0.180) loss 0.5000 (0.3999) acc 90.6250 (92.1875) lr 3.1417e-05 eta 0:00:04\n",
            "epoch [48/50] batch [1/4] time 0.995 (0.995) data 0.846 (0.846) loss 0.4922 (0.4922) acc 84.3750 (84.3750) lr 3.1417e-05 eta 0:00:10\n",
            "epoch [48/50] batch [2/4] time 0.154 (0.574) data 0.000 (0.423) loss 0.4871 (0.4896) acc 81.2500 (82.8125) lr 3.1417e-05 eta 0:00:05\n",
            "epoch [48/50] batch [3/4] time 0.156 (0.435) data 0.000 (0.282) loss 0.3633 (0.4475) acc 84.3750 (83.3333) lr 3.1417e-05 eta 0:00:03\n",
            "epoch [48/50] batch [4/4] time 0.155 (0.365) data 0.000 (0.212) loss 0.2164 (0.3897) acc 93.7500 (85.9375) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [1/4] time 1.469 (1.469) data 1.317 (1.317) loss 0.8564 (0.8564) acc 78.1250 (78.1250) lr 1.7713e-05 eta 0:00:10\n",
            "epoch [49/50] batch [2/4] time 0.155 (0.812) data 0.000 (0.658) loss 0.2068 (0.5316) acc 96.8750 (87.5000) lr 1.7713e-05 eta 0:00:04\n",
            "epoch [49/50] batch [3/4] time 0.156 (0.593) data 0.000 (0.439) loss 0.3733 (0.4788) acc 84.3750 (86.4583) lr 1.7713e-05 eta 0:00:02\n",
            "epoch [49/50] batch [4/4] time 0.155 (0.484) data 0.000 (0.329) loss 0.3254 (0.4405) acc 87.5000 (86.7188) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [1/4] time 1.362 (1.362) data 1.209 (1.209) loss 0.4207 (0.4207) acc 84.3750 (84.3750) lr 7.8853e-06 eta 0:00:04\n",
            "epoch [50/50] batch [2/4] time 0.155 (0.758) data 0.000 (0.605) loss 0.2629 (0.3418) acc 93.7500 (89.0625) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [3/4] time 0.154 (0.557) data 0.000 (0.403) loss 0.5371 (0.4069) acc 84.3750 (87.5000) lr 7.8853e-06 eta 0:00:00\n",
            "epoch [50/50] batch [4/4] time 0.153 (0.456) data 0.000 (0.303) loss 0.1539 (0.3437) acc 100.0000 (90.6250) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_4shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,398\n",
            "* accuracy: 92.6%\n",
            "* error: 7.4%\n",
            "* macro_f1: 92.5%\n",
            "Elapsed: 0:01:54\n",
            "\\n=== Final Accuracy ===\n",
            "* accuracy: 92.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training: 16-shot learning (~30 minutes)\n",
        "# This is the standard benchmark configuration\n",
        "# Expected accuracy: ~93-94%\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_16shot \\\n",
        "DATASET.NUM_SHOTS 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6AKsKGAlIlt",
        "outputId": "2bd0a85c-c0f4-471e-fe76-c6da2e3c75e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:28:45.861756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763234925.881885    5568 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763234925.888364    5568 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763234925.904109    5568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234925.904137    5568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234925.904140    5568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234925.904146    5568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:28:45.908820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_pets_16shot\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_16shot\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/CoOp/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_16shot/tensorboard)\n",
            "epoch [1/50] batch [5/18] time 0.152 (1.250) data 0.000 (0.759) loss 2.0918 (2.4035) acc 53.1250 (43.1250) lr 1.0000e-05 eta 0:18:38\n",
            "epoch [1/50] batch [10/18] time 0.153 (0.702) data 0.000 (0.380) loss 2.3848 (2.4148) acc 40.6250 (43.1250) lr 1.0000e-05 eta 0:10:24\n",
            "epoch [1/50] batch [15/18] time 0.155 (0.519) data 0.000 (0.253) loss 1.1406 (2.0287) acc 65.6250 (50.4167) lr 1.0000e-05 eta 0:07:39\n",
            "epoch [2/50] batch [5/18] time 0.157 (0.636) data 0.000 (0.369) loss 0.6626 (1.0325) acc 78.1250 (70.0000) lr 2.0000e-03 eta 0:09:18\n",
            "epoch [2/50] batch [10/18] time 0.164 (0.396) data 0.000 (0.185) loss 0.8149 (0.9345) acc 71.8750 (70.9375) lr 2.0000e-03 eta 0:05:45\n",
            "epoch [2/50] batch [15/18] time 0.153 (0.316) data 0.000 (0.123) loss 0.8574 (0.8983) acc 68.7500 (71.0417) lr 2.0000e-03 eta 0:04:33\n",
            "epoch [3/50] batch [5/18] time 0.157 (0.897) data 0.000 (0.563) loss 0.6284 (0.7428) acc 84.3750 (78.7500) lr 1.9980e-03 eta 0:12:50\n",
            "epoch [3/50] batch [10/18] time 0.164 (0.527) data 0.000 (0.282) loss 0.5464 (0.7069) acc 81.2500 (78.1250) lr 1.9980e-03 eta 0:07:30\n",
            "epoch [3/50] batch [15/18] time 0.156 (0.403) data 0.000 (0.188) loss 0.8408 (0.7199) acc 75.0000 (78.7500) lr 1.9980e-03 eta 0:05:41\n",
            "epoch [4/50] batch [5/18] time 0.337 (0.657) data 0.000 (0.334) loss 0.5039 (0.6781) acc 90.6250 (83.1250) lr 1.9921e-03 eta 0:09:12\n",
            "epoch [4/50] batch [10/18] time 0.154 (0.425) data 0.000 (0.169) loss 0.4651 (0.6793) acc 90.6250 (80.9375) lr 1.9921e-03 eta 0:05:55\n",
            "epoch [4/50] batch [15/18] time 0.158 (0.337) data 0.000 (0.112) loss 0.8174 (0.6774) acc 75.0000 (80.0000) lr 1.9921e-03 eta 0:04:39\n",
            "epoch [5/50] batch [5/18] time 0.340 (0.823) data 0.000 (0.348) loss 0.3035 (0.6842) acc 90.6250 (83.1250) lr 1.9823e-03 eta 0:11:16\n",
            "epoch [5/50] batch [10/18] time 0.160 (0.499) data 0.001 (0.174) loss 0.6133 (0.6942) acc 75.0000 (81.5625) lr 1.9823e-03 eta 0:06:48\n",
            "epoch [5/50] batch [15/18] time 0.162 (0.386) data 0.000 (0.116) loss 0.4458 (0.6616) acc 90.6250 (82.0833) lr 1.9823e-03 eta 0:05:13\n",
            "epoch [6/50] batch [5/18] time 0.327 (0.557) data 0.000 (0.231) loss 0.9595 (0.7625) acc 68.7500 (75.6250) lr 1.9686e-03 eta 0:07:28\n",
            "epoch [6/50] batch [10/18] time 0.155 (0.368) data 0.000 (0.117) loss 0.6587 (0.6597) acc 81.2500 (80.0000) lr 1.9686e-03 eta 0:04:54\n",
            "epoch [6/50] batch [15/18] time 0.156 (0.298) data 0.000 (0.078) loss 0.7266 (0.6735) acc 75.0000 (78.7500) lr 1.9686e-03 eta 0:03:57\n",
            "epoch [7/50] batch [5/18] time 0.226 (0.603) data 0.001 (0.350) loss 0.4028 (0.6371) acc 90.6250 (79.3750) lr 1.9511e-03 eta 0:07:54\n",
            "epoch [7/50] batch [10/18] time 0.158 (0.380) data 0.000 (0.175) loss 0.8984 (0.6515) acc 71.8750 (79.6875) lr 1.9511e-03 eta 0:04:57\n",
            "epoch [7/50] batch [15/18] time 0.152 (0.306) data 0.000 (0.117) loss 0.7510 (0.6474) acc 78.1250 (80.4167) lr 1.9511e-03 eta 0:03:57\n",
            "epoch [8/50] batch [5/18] time 0.168 (0.685) data 0.000 (0.452) loss 0.3591 (0.6527) acc 90.6250 (79.3750) lr 1.9298e-03 eta 0:08:46\n",
            "epoch [8/50] batch [10/18] time 0.162 (0.421) data 0.000 (0.226) loss 0.6636 (0.6771) acc 87.5000 (79.3750) lr 1.9298e-03 eta 0:05:21\n",
            "epoch [8/50] batch [15/18] time 0.154 (0.332) data 0.000 (0.151) loss 0.5361 (0.6298) acc 81.2500 (80.0000) lr 1.9298e-03 eta 0:04:11\n",
            "epoch [9/50] batch [5/18] time 0.165 (0.617) data 0.000 (0.351) loss 0.4133 (0.5976) acc 87.5000 (81.2500) lr 1.9048e-03 eta 0:07:43\n",
            "epoch [9/50] batch [10/18] time 0.163 (0.387) data 0.000 (0.176) loss 0.7163 (0.6690) acc 75.0000 (78.7500) lr 1.9048e-03 eta 0:04:48\n",
            "epoch [9/50] batch [15/18] time 0.154 (0.309) data 0.000 (0.117) loss 0.8037 (0.6255) acc 75.0000 (79.7917) lr 1.9048e-03 eta 0:03:48\n",
            "epoch [10/50] batch [5/18] time 0.236 (0.863) data 0.009 (0.460) loss 0.3694 (0.6519) acc 87.5000 (79.3750) lr 1.8763e-03 eta 0:10:32\n",
            "epoch [10/50] batch [10/18] time 0.150 (0.508) data 0.000 (0.230) loss 0.6958 (0.6073) acc 81.2500 (81.5625) lr 1.8763e-03 eta 0:06:10\n",
            "epoch [10/50] batch [15/18] time 0.153 (0.390) data 0.000 (0.154) loss 0.7671 (0.6049) acc 75.0000 (81.2500) lr 1.8763e-03 eta 0:04:42\n",
            "epoch [11/50] batch [5/18] time 0.243 (0.586) data 0.000 (0.309) loss 0.6812 (0.5452) acc 78.1250 (81.8750) lr 1.8443e-03 eta 0:06:58\n",
            "epoch [11/50] batch [10/18] time 0.152 (0.369) data 0.000 (0.154) loss 0.3894 (0.5394) acc 87.5000 (81.8750) lr 1.8443e-03 eta 0:04:21\n",
            "epoch [11/50] batch [15/18] time 0.150 (0.297) data 0.000 (0.103) loss 0.5781 (0.5901) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:03:29\n",
            "epoch [12/50] batch [5/18] time 0.343 (0.725) data 0.002 (0.345) loss 0.4336 (0.5331) acc 87.5000 (84.3750) lr 1.8090e-03 eta 0:08:25\n",
            "epoch [12/50] batch [10/18] time 0.156 (0.451) data 0.001 (0.173) loss 0.5698 (0.5104) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:05:11\n",
            "epoch [12/50] batch [15/18] time 0.151 (0.351) data 0.000 (0.115) loss 0.4758 (0.5049) acc 87.5000 (84.5833) lr 1.8090e-03 eta 0:04:01\n",
            "epoch [13/50] batch [5/18] time 0.178 (0.610) data 0.000 (0.296) loss 0.6255 (0.5494) acc 75.0000 (80.0000) lr 1.7705e-03 eta 0:06:53\n",
            "epoch [13/50] batch [10/18] time 0.156 (0.382) data 0.000 (0.148) loss 0.5693 (0.5662) acc 81.2500 (81.8750) lr 1.7705e-03 eta 0:04:17\n",
            "epoch [13/50] batch [15/18] time 0.152 (0.305) data 0.000 (0.099) loss 0.2944 (0.5346) acc 96.8750 (82.9167) lr 1.7705e-03 eta 0:03:24\n",
            "epoch [14/50] batch [5/18] time 0.178 (0.613) data 0.005 (0.364) loss 0.5332 (0.5977) acc 84.3750 (80.6250) lr 1.7290e-03 eta 0:06:45\n",
            "epoch [14/50] batch [10/18] time 0.164 (0.384) data 0.000 (0.182) loss 0.3604 (0.5712) acc 87.5000 (80.9375) lr 1.7290e-03 eta 0:04:12\n",
            "epoch [14/50] batch [15/18] time 0.151 (0.307) data 0.000 (0.121) loss 0.6616 (0.5466) acc 78.1250 (82.2917) lr 1.7290e-03 eta 0:03:19\n",
            "epoch [15/50] batch [5/18] time 0.158 (0.681) data 0.000 (0.447) loss 0.3469 (0.6348) acc 87.5000 (81.2500) lr 1.6845e-03 eta 0:07:17\n",
            "epoch [15/50] batch [10/18] time 0.162 (0.417) data 0.000 (0.224) loss 0.5518 (0.5735) acc 78.1250 (82.8125) lr 1.6845e-03 eta 0:04:26\n",
            "epoch [15/50] batch [15/18] time 0.152 (0.329) data 0.000 (0.149) loss 0.5161 (0.5686) acc 90.6250 (83.5417) lr 1.6845e-03 eta 0:03:28\n",
            "epoch [16/50] batch [5/18] time 0.187 (0.648) data 0.002 (0.349) loss 1.0420 (0.7205) acc 75.0000 (78.7500) lr 1.6374e-03 eta 0:06:44\n",
            "epoch [16/50] batch [10/18] time 0.154 (0.404) data 0.000 (0.175) loss 0.5488 (0.5749) acc 81.2500 (82.5000) lr 1.6374e-03 eta 0:04:10\n",
            "epoch [16/50] batch [15/18] time 0.152 (0.320) data 0.000 (0.117) loss 0.4365 (0.5372) acc 90.6250 (84.1667) lr 1.6374e-03 eta 0:03:17\n",
            "epoch [17/50] batch [5/18] time 0.260 (0.840) data 0.000 (0.508) loss 0.2067 (0.4924) acc 96.8750 (87.5000) lr 1.5878e-03 eta 0:08:30\n",
            "epoch [17/50] batch [10/18] time 0.152 (0.499) data 0.000 (0.254) loss 0.8594 (0.5176) acc 75.0000 (85.6250) lr 1.5878e-03 eta 0:05:00\n",
            "epoch [17/50] batch [15/18] time 0.152 (0.384) data 0.000 (0.169) loss 0.6069 (0.5181) acc 81.2500 (85.0000) lr 1.5878e-03 eta 0:03:49\n",
            "epoch [18/50] batch [5/18] time 0.162 (0.624) data 0.000 (0.358) loss 0.7217 (0.6154) acc 75.0000 (80.0000) lr 1.5358e-03 eta 0:06:07\n",
            "epoch [18/50] batch [10/18] time 0.156 (0.391) data 0.000 (0.179) loss 0.3313 (0.5831) acc 84.3750 (81.5625) lr 1.5358e-03 eta 0:03:48\n",
            "epoch [18/50] batch [15/18] time 0.154 (0.311) data 0.000 (0.119) loss 0.7358 (0.5875) acc 81.2500 (82.2917) lr 1.5358e-03 eta 0:03:00\n",
            "epoch [19/50] batch [5/18] time 0.460 (0.794) data 0.000 (0.353) loss 0.4441 (0.3879) acc 87.5000 (89.3750) lr 1.4818e-03 eta 0:07:33\n",
            "epoch [19/50] batch [10/18] time 0.153 (0.492) data 0.000 (0.177) loss 0.6357 (0.5222) acc 75.0000 (83.7500) lr 1.4818e-03 eta 0:04:38\n",
            "epoch [19/50] batch [15/18] time 0.154 (0.380) data 0.000 (0.118) loss 0.8506 (0.5251) acc 68.7500 (83.5417) lr 1.4818e-03 eta 0:03:33\n",
            "epoch [20/50] batch [5/18] time 0.204 (0.607) data 0.001 (0.338) loss 0.1740 (0.3922) acc 100.0000 (90.6250) lr 1.4258e-03 eta 0:05:35\n",
            "epoch [20/50] batch [10/18] time 0.167 (0.382) data 0.000 (0.169) loss 0.7412 (0.5013) acc 75.0000 (85.6250) lr 1.4258e-03 eta 0:03:29\n",
            "epoch [20/50] batch [15/18] time 0.153 (0.307) data 0.000 (0.113) loss 0.4702 (0.5183) acc 90.6250 (83.9583) lr 1.4258e-03 eta 0:02:46\n",
            "epoch [21/50] batch [5/18] time 0.205 (0.602) data 0.002 (0.336) loss 0.3198 (0.4273) acc 93.7500 (87.5000) lr 1.3681e-03 eta 0:05:22\n",
            "epoch [21/50] batch [10/18] time 0.154 (0.383) data 0.000 (0.168) loss 0.8511 (0.5320) acc 78.1250 (84.6875) lr 1.3681e-03 eta 0:03:23\n",
            "epoch [21/50] batch [15/18] time 0.156 (0.308) data 0.000 (0.112) loss 0.3608 (0.5487) acc 87.5000 (83.3333) lr 1.3681e-03 eta 0:02:41\n",
            "epoch [22/50] batch [5/18] time 0.155 (0.654) data 0.000 (0.397) loss 0.3210 (0.4039) acc 90.6250 (85.6250) lr 1.3090e-03 eta 0:05:38\n",
            "epoch [22/50] batch [10/18] time 0.164 (0.406) data 0.000 (0.199) loss 0.3235 (0.4515) acc 84.3750 (83.4375) lr 1.3090e-03 eta 0:03:27\n",
            "epoch [22/50] batch [15/18] time 0.155 (0.322) data 0.000 (0.133) loss 0.5771 (0.5035) acc 81.2500 (82.0833) lr 1.3090e-03 eta 0:02:43\n",
            "epoch [23/50] batch [5/18] time 0.259 (0.565) data 0.008 (0.278) loss 0.7280 (0.4925) acc 75.0000 (87.5000) lr 1.2487e-03 eta 0:04:42\n",
            "epoch [23/50] batch [10/18] time 0.153 (0.365) data 0.000 (0.139) loss 0.3867 (0.4983) acc 84.3750 (85.9375) lr 1.2487e-03 eta 0:03:00\n",
            "epoch [23/50] batch [15/18] time 0.155 (0.295) data 0.000 (0.093) loss 0.4849 (0.5113) acc 84.3750 (84.7917) lr 1.2487e-03 eta 0:02:24\n",
            "epoch [24/50] batch [5/18] time 0.204 (0.870) data 0.000 (0.494) loss 0.6772 (0.4949) acc 71.8750 (83.1250) lr 1.1874e-03 eta 0:06:58\n",
            "epoch [24/50] batch [10/18] time 0.153 (0.512) data 0.000 (0.247) loss 0.4529 (0.4804) acc 90.6250 (84.0625) lr 1.1874e-03 eta 0:04:03\n",
            "epoch [24/50] batch [15/18] time 0.154 (0.393) data 0.000 (0.165) loss 0.5278 (0.4560) acc 78.1250 (84.5833) lr 1.1874e-03 eta 0:03:04\n",
            "epoch [25/50] batch [5/18] time 0.165 (0.628) data 0.000 (0.369) loss 0.3574 (0.4760) acc 90.6250 (86.8750) lr 1.1253e-03 eta 0:04:50\n",
            "epoch [25/50] batch [10/18] time 0.157 (0.391) data 0.000 (0.185) loss 0.3352 (0.5533) acc 93.7500 (84.3750) lr 1.1253e-03 eta 0:02:58\n",
            "epoch [25/50] batch [15/18] time 0.152 (0.311) data 0.000 (0.123) loss 0.5713 (0.6021) acc 84.3750 (82.9167) lr 1.1253e-03 eta 0:02:21\n",
            "epoch [26/50] batch [5/18] time 0.229 (0.786) data 0.000 (0.374) loss 0.5674 (0.5858) acc 71.8750 (81.2500) lr 1.0628e-03 eta 0:05:49\n",
            "epoch [26/50] batch [10/18] time 0.154 (0.478) data 0.000 (0.187) loss 0.5786 (0.6145) acc 78.1250 (78.7500) lr 1.0628e-03 eta 0:03:30\n",
            "epoch [26/50] batch [15/18] time 0.151 (0.370) data 0.000 (0.125) loss 0.5835 (0.6030) acc 87.5000 (80.4167) lr 1.0628e-03 eta 0:02:41\n",
            "epoch [27/50] batch [5/18] time 0.153 (0.698) data 0.000 (0.432) loss 0.3643 (0.4458) acc 93.7500 (86.8750) lr 1.0000e-03 eta 0:04:58\n",
            "epoch [27/50] batch [10/18] time 0.162 (0.427) data 0.000 (0.216) loss 0.2986 (0.4758) acc 93.7500 (86.8750) lr 1.0000e-03 eta 0:03:00\n",
            "epoch [27/50] batch [15/18] time 0.151 (0.336) data 0.000 (0.144) loss 0.4055 (0.4607) acc 81.2500 (86.2500) lr 1.0000e-03 eta 0:02:19\n",
            "epoch [28/50] batch [5/18] time 0.243 (0.611) data 0.000 (0.332) loss 0.3945 (0.5000) acc 84.3750 (83.7500) lr 9.3721e-04 eta 0:04:10\n",
            "epoch [28/50] batch [10/18] time 0.156 (0.394) data 0.000 (0.167) loss 0.5103 (0.4847) acc 87.5000 (85.6250) lr 9.3721e-04 eta 0:02:39\n",
            "epoch [28/50] batch [15/18] time 0.154 (0.315) data 0.000 (0.111) loss 0.4189 (0.4897) acc 81.2500 (85.6250) lr 9.3721e-04 eta 0:02:05\n",
            "epoch [29/50] batch [5/18] time 0.159 (0.628) data 0.002 (0.377) loss 0.4490 (0.5611) acc 84.3750 (83.1250) lr 8.7467e-04 eta 0:04:05\n",
            "epoch [29/50] batch [10/18] time 0.156 (0.394) data 0.000 (0.190) loss 0.6641 (0.5500) acc 78.1250 (81.2500) lr 8.7467e-04 eta 0:02:32\n",
            "epoch [29/50] batch [15/18] time 0.152 (0.314) data 0.000 (0.126) loss 0.5361 (0.5592) acc 84.3750 (81.4583) lr 8.7467e-04 eta 0:01:59\n",
            "epoch [30/50] batch [5/18] time 0.161 (0.611) data 0.001 (0.317) loss 0.7935 (0.5793) acc 78.1250 (81.2500) lr 8.1262e-04 eta 0:03:48\n",
            "epoch [30/50] batch [10/18] time 0.168 (0.385) data 0.000 (0.159) loss 0.5703 (0.5271) acc 84.3750 (83.1250) lr 8.1262e-04 eta 0:02:21\n",
            "epoch [30/50] batch [15/18] time 0.153 (0.307) data 0.000 (0.106) loss 0.5210 (0.5134) acc 87.5000 (83.7500) lr 8.1262e-04 eta 0:01:51\n",
            "epoch [31/50] batch [5/18] time 0.244 (0.793) data 0.000 (0.499) loss 0.3213 (0.4463) acc 87.5000 (86.2500) lr 7.5131e-04 eta 0:04:41\n",
            "epoch [31/50] batch [10/18] time 0.152 (0.476) data 0.000 (0.249) loss 0.3142 (0.4836) acc 90.6250 (86.2500) lr 7.5131e-04 eta 0:02:46\n",
            "epoch [31/50] batch [15/18] time 0.152 (0.369) data 0.000 (0.166) loss 0.3252 (0.4556) acc 93.7500 (87.2917) lr 7.5131e-04 eta 0:02:07\n",
            "epoch [32/50] batch [5/18] time 0.231 (0.605) data 0.008 (0.295) loss 0.6450 (0.4861) acc 75.0000 (85.0000) lr 6.9098e-04 eta 0:03:23\n",
            "epoch [32/50] batch [10/18] time 0.157 (0.380) data 0.000 (0.148) loss 0.7856 (0.5399) acc 78.1250 (82.5000) lr 6.9098e-04 eta 0:02:06\n",
            "epoch [32/50] batch [15/18] time 0.153 (0.305) data 0.000 (0.099) loss 0.7798 (0.5148) acc 68.7500 (82.9167) lr 6.9098e-04 eta 0:01:39\n",
            "epoch [33/50] batch [5/18] time 0.275 (0.836) data 0.000 (0.406) loss 0.7686 (0.4207) acc 81.2500 (88.1250) lr 6.3188e-04 eta 0:04:26\n",
            "epoch [33/50] batch [10/18] time 0.152 (0.503) data 0.000 (0.203) loss 0.4097 (0.4809) acc 87.5000 (86.5625) lr 6.3188e-04 eta 0:02:38\n",
            "epoch [33/50] batch [15/18] time 0.152 (0.387) data 0.000 (0.135) loss 0.5479 (0.4796) acc 87.5000 (86.0417) lr 6.3188e-04 eta 0:01:59\n",
            "epoch [34/50] batch [5/18] time 0.219 (0.603) data 0.006 (0.301) loss 0.7031 (0.5640) acc 71.8750 (81.2500) lr 5.7422e-04 eta 0:03:01\n",
            "epoch [34/50] batch [10/18] time 0.156 (0.380) data 0.000 (0.151) loss 0.3152 (0.4770) acc 90.6250 (85.0000) lr 5.7422e-04 eta 0:01:52\n",
            "epoch [34/50] batch [15/18] time 0.153 (0.305) data 0.000 (0.101) loss 0.6987 (0.4998) acc 78.1250 (84.3750) lr 5.7422e-04 eta 0:01:28\n",
            "epoch [35/50] batch [5/18] time 0.232 (0.590) data 0.000 (0.286) loss 0.4045 (0.5269) acc 84.3750 (84.3750) lr 5.1825e-04 eta 0:02:46\n",
            "epoch [35/50] batch [10/18] time 0.153 (0.377) data 0.000 (0.143) loss 0.3870 (0.5069) acc 87.5000 (85.0000) lr 5.1825e-04 eta 0:01:44\n",
            "epoch [35/50] batch [15/18] time 0.153 (0.304) data 0.000 (0.096) loss 0.6665 (0.5256) acc 84.3750 (85.2083) lr 5.1825e-04 eta 0:01:22\n",
            "epoch [36/50] batch [5/18] time 0.156 (0.641) data 0.000 (0.397) loss 0.7729 (0.6167) acc 75.0000 (81.2500) lr 4.6417e-04 eta 0:02:49\n",
            "epoch [36/50] batch [10/18] time 0.154 (0.398) data 0.000 (0.199) loss 0.3999 (0.5388) acc 90.6250 (83.4375) lr 4.6417e-04 eta 0:01:43\n",
            "epoch [36/50] batch [15/18] time 0.155 (0.316) data 0.000 (0.133) loss 0.5850 (0.5477) acc 87.5000 (83.1250) lr 4.6417e-04 eta 0:01:20\n",
            "epoch [37/50] batch [5/18] time 0.251 (0.587) data 0.000 (0.324) loss 0.6221 (0.5885) acc 71.8750 (80.6250) lr 4.1221e-04 eta 0:02:24\n",
            "epoch [37/50] batch [10/18] time 0.154 (0.373) data 0.000 (0.162) loss 0.8643 (0.5881) acc 71.8750 (82.8125) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [37/50] batch [15/18] time 0.153 (0.300) data 0.000 (0.108) loss 0.4990 (0.5603) acc 78.1250 (83.1250) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [38/50] batch [5/18] time 0.151 (0.840) data 0.000 (0.600) loss 0.6138 (0.4708) acc 84.3750 (85.6250) lr 3.6258e-04 eta 0:03:12\n",
            "epoch [38/50] batch [10/18] time 0.155 (0.497) data 0.000 (0.300) loss 0.4021 (0.4991) acc 90.6250 (84.0625) lr 3.6258e-04 eta 0:01:51\n",
            "epoch [38/50] batch [15/18] time 0.154 (0.383) data 0.000 (0.200) loss 0.4685 (0.5207) acc 87.5000 (82.9167) lr 3.6258e-04 eta 0:01:23\n",
            "epoch [39/50] batch [5/18] time 0.216 (0.598) data 0.000 (0.346) loss 0.6436 (0.4956) acc 78.1250 (83.7500) lr 3.1545e-04 eta 0:02:06\n",
            "epoch [39/50] batch [10/18] time 0.153 (0.377) data 0.000 (0.173) loss 0.4185 (0.5478) acc 90.6250 (82.5000) lr 3.1545e-04 eta 0:01:17\n",
            "epoch [39/50] batch [15/18] time 0.154 (0.303) data 0.000 (0.115) loss 0.6851 (0.5594) acc 84.3750 (82.7083) lr 3.1545e-04 eta 0:01:00\n",
            "epoch [40/50] batch [5/18] time 0.161 (0.859) data 0.000 (0.505) loss 0.4492 (0.6213) acc 87.5000 (83.7500) lr 2.7103e-04 eta 0:02:45\n",
            "epoch [40/50] batch [10/18] time 0.165 (0.507) data 0.000 (0.253) loss 0.5933 (0.5968) acc 87.5000 (81.5625) lr 2.7103e-04 eta 0:01:35\n",
            "epoch [40/50] batch [15/18] time 0.152 (0.389) data 0.000 (0.169) loss 0.4253 (0.5578) acc 84.3750 (82.9167) lr 2.7103e-04 eta 0:01:11\n",
            "epoch [41/50] batch [5/18] time 0.238 (0.603) data 0.001 (0.300) loss 0.3423 (0.4402) acc 87.5000 (86.2500) lr 2.2949e-04 eta 0:01:45\n",
            "epoch [41/50] batch [10/18] time 0.152 (0.379) data 0.000 (0.150) loss 0.6226 (0.4616) acc 81.2500 (85.3125) lr 2.2949e-04 eta 0:01:04\n",
            "epoch [41/50] batch [15/18] time 0.152 (0.304) data 0.000 (0.100) loss 0.2512 (0.4660) acc 93.7500 (85.8333) lr 2.2949e-04 eta 0:00:50\n",
            "epoch [42/50] batch [5/18] time 0.256 (0.677) data 0.008 (0.365) loss 0.4021 (0.4994) acc 81.2500 (82.5000) lr 1.9098e-04 eta 0:01:46\n",
            "epoch [42/50] batch [10/18] time 0.154 (0.425) data 0.000 (0.183) loss 0.8398 (0.5786) acc 65.6250 (81.2500) lr 1.9098e-04 eta 0:01:04\n",
            "epoch [42/50] batch [15/18] time 0.153 (0.335) data 0.000 (0.122) loss 0.6357 (0.5357) acc 84.3750 (83.1250) lr 1.9098e-04 eta 0:00:49\n",
            "epoch [43/50] batch [5/18] time 0.173 (0.639) data 0.000 (0.385) loss 0.3772 (0.5242) acc 87.5000 (83.1250) lr 1.5567e-04 eta 0:01:28\n",
            "epoch [43/50] batch [10/18] time 0.166 (0.397) data 0.000 (0.192) loss 0.3506 (0.4848) acc 87.5000 (84.0625) lr 1.5567e-04 eta 0:00:53\n",
            "epoch [43/50] batch [15/18] time 0.152 (0.316) data 0.000 (0.128) loss 0.3188 (0.4677) acc 90.6250 (85.2083) lr 1.5567e-04 eta 0:00:40\n",
            "epoch [44/50] batch [5/18] time 0.161 (0.599) data 0.001 (0.341) loss 0.8477 (0.5777) acc 71.8750 (82.5000) lr 1.2369e-04 eta 0:01:12\n",
            "epoch [44/50] batch [10/18] time 0.162 (0.378) data 0.000 (0.171) loss 0.4902 (0.5472) acc 84.3750 (82.1875) lr 1.2369e-04 eta 0:00:43\n",
            "epoch [44/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.114) loss 0.4646 (0.5366) acc 90.6250 (82.9167) lr 1.2369e-04 eta 0:00:33\n",
            "epoch [45/50] batch [5/18] time 0.164 (0.691) data 0.002 (0.419) loss 0.3828 (0.5525) acc 87.5000 (83.7500) lr 9.5173e-05 eta 0:01:11\n",
            "epoch [45/50] batch [10/18] time 0.160 (0.423) data 0.000 (0.210) loss 0.5649 (0.4924) acc 78.1250 (85.6250) lr 9.5173e-05 eta 0:00:41\n",
            "epoch [45/50] batch [15/18] time 0.154 (0.333) data 0.000 (0.140) loss 0.3752 (0.4711) acc 84.3750 (86.2500) lr 9.5173e-05 eta 0:00:30\n",
            "epoch [46/50] batch [5/18] time 0.321 (0.596) data 0.002 (0.268) loss 0.5186 (0.5530) acc 78.1250 (83.1250) lr 7.0224e-05 eta 0:00:50\n",
            "epoch [46/50] batch [10/18] time 0.154 (0.376) data 0.000 (0.134) loss 0.3618 (0.4944) acc 90.6250 (86.2500) lr 7.0224e-05 eta 0:00:30\n",
            "epoch [46/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.090) loss 0.2493 (0.4785) acc 90.6250 (85.8333) lr 7.0224e-05 eta 0:00:22\n",
            "epoch [47/50] batch [5/18] time 0.161 (0.860) data 0.000 (0.618) loss 0.4629 (0.6225) acc 84.3750 (80.0000) lr 4.8943e-05 eta 0:00:57\n",
            "epoch [47/50] batch [10/18] time 0.153 (0.507) data 0.000 (0.309) loss 0.4995 (0.5412) acc 78.1250 (82.5000) lr 4.8943e-05 eta 0:00:31\n",
            "epoch [47/50] batch [15/18] time 0.153 (0.389) data 0.000 (0.206) loss 0.3696 (0.5238) acc 87.5000 (83.3333) lr 4.8943e-05 eta 0:00:22\n",
            "epoch [48/50] batch [5/18] time 0.175 (0.587) data 0.010 (0.325) loss 0.4548 (0.5646) acc 84.3750 (83.1250) lr 3.1417e-05 eta 0:00:28\n",
            "epoch [48/50] batch [10/18] time 0.158 (0.371) data 0.000 (0.163) loss 0.4558 (0.5083) acc 84.3750 (84.3750) lr 3.1417e-05 eta 0:00:16\n",
            "epoch [48/50] batch [15/18] time 0.153 (0.299) data 0.000 (0.109) loss 0.4868 (0.4913) acc 87.5000 (85.2083) lr 3.1417e-05 eta 0:00:11\n",
            "epoch [49/50] batch [5/18] time 0.233 (0.892) data 0.001 (0.445) loss 0.3450 (0.5282) acc 93.7500 (86.8750) lr 1.7713e-05 eta 0:00:27\n",
            "epoch [49/50] batch [10/18] time 0.152 (0.526) data 0.000 (0.223) loss 1.1191 (0.6210) acc 75.0000 (83.4375) lr 1.7713e-05 eta 0:00:13\n",
            "epoch [49/50] batch [15/18] time 0.152 (0.402) data 0.000 (0.148) loss 0.3540 (0.6017) acc 87.5000 (82.0833) lr 1.7713e-05 eta 0:00:08\n",
            "epoch [50/50] batch [5/18] time 0.157 (0.610) data 0.000 (0.339) loss 0.5532 (0.5465) acc 81.2500 (83.7500) lr 7.8853e-06 eta 0:00:07\n",
            "epoch [50/50] batch [10/18] time 0.163 (0.383) data 0.000 (0.169) loss 0.4172 (0.5072) acc 84.3750 (84.3750) lr 7.8853e-06 eta 0:00:03\n",
            "epoch [50/50] batch [15/18] time 0.154 (0.306) data 0.000 (0.113) loss 0.6567 (0.5224) acc 87.5000 (84.7917) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_16shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:26<00:00,  1.38it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,426\n",
            "* accuracy: 93.4%\n",
            "* error: 6.6%\n",
            "* macro_f1: 93.3%\n",
            "Elapsed: 0:05:12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display training results for 16-shot experiment\n",
        "!echo \"=== Training Log (last 30 lines) ===\"\n",
        "!tail -30 output/oxford_pets_16shot/log.txt\n",
        "\n",
        "!echo \"\\n=== Final Accuracy ===\"\n",
        "!grep \"accuracy:\" output/oxford_pets_16shot/log.txt | tail -3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a766de9-8e03-41ad-e327-289cd76b9dc2",
        "id": "dFlQrnlxl34I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Log (last 30 lines) ===\n",
            "epoch [44/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.114) loss 0.4646 (0.5366) acc 90.6250 (82.9167) lr 1.2369e-04 eta 0:00:33\n",
            "epoch [45/50] batch [5/18] time 0.164 (0.691) data 0.002 (0.419) loss 0.3828 (0.5525) acc 87.5000 (83.7500) lr 9.5173e-05 eta 0:01:11\n",
            "epoch [45/50] batch [10/18] time 0.160 (0.423) data 0.000 (0.210) loss 0.5649 (0.4924) acc 78.1250 (85.6250) lr 9.5173e-05 eta 0:00:41\n",
            "epoch [45/50] batch [15/18] time 0.154 (0.333) data 0.000 (0.140) loss 0.3752 (0.4711) acc 84.3750 (86.2500) lr 9.5173e-05 eta 0:00:30\n",
            "epoch [46/50] batch [5/18] time 0.321 (0.596) data 0.002 (0.268) loss 0.5186 (0.5530) acc 78.1250 (83.1250) lr 7.0224e-05 eta 0:00:50\n",
            "epoch [46/50] batch [10/18] time 0.154 (0.376) data 0.000 (0.134) loss 0.3618 (0.4944) acc 90.6250 (86.2500) lr 7.0224e-05 eta 0:00:30\n",
            "epoch [46/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.090) loss 0.2493 (0.4785) acc 90.6250 (85.8333) lr 7.0224e-05 eta 0:00:22\n",
            "epoch [47/50] batch [5/18] time 0.161 (0.860) data 0.000 (0.618) loss 0.4629 (0.6225) acc 84.3750 (80.0000) lr 4.8943e-05 eta 0:00:57\n",
            "epoch [47/50] batch [10/18] time 0.153 (0.507) data 0.000 (0.309) loss 0.4995 (0.5412) acc 78.1250 (82.5000) lr 4.8943e-05 eta 0:00:31\n",
            "epoch [47/50] batch [15/18] time 0.153 (0.389) data 0.000 (0.206) loss 0.3696 (0.5238) acc 87.5000 (83.3333) lr 4.8943e-05 eta 0:00:22\n",
            "epoch [48/50] batch [5/18] time 0.175 (0.587) data 0.010 (0.325) loss 0.4548 (0.5646) acc 84.3750 (83.1250) lr 3.1417e-05 eta 0:00:28\n",
            "epoch [48/50] batch [10/18] time 0.158 (0.371) data 0.000 (0.163) loss 0.4558 (0.5083) acc 84.3750 (84.3750) lr 3.1417e-05 eta 0:00:16\n",
            "epoch [48/50] batch [15/18] time 0.153 (0.299) data 0.000 (0.109) loss 0.4868 (0.4913) acc 87.5000 (85.2083) lr 3.1417e-05 eta 0:00:11\n",
            "epoch [49/50] batch [5/18] time 0.233 (0.892) data 0.001 (0.445) loss 0.3450 (0.5282) acc 93.7500 (86.8750) lr 1.7713e-05 eta 0:00:27\n",
            "epoch [49/50] batch [10/18] time 0.152 (0.526) data 0.000 (0.223) loss 1.1191 (0.6210) acc 75.0000 (83.4375) lr 1.7713e-05 eta 0:00:13\n",
            "epoch [49/50] batch [15/18] time 0.152 (0.402) data 0.000 (0.148) loss 0.3540 (0.6017) acc 87.5000 (82.0833) lr 1.7713e-05 eta 0:00:08\n",
            "epoch [50/50] batch [5/18] time 0.157 (0.610) data 0.000 (0.339) loss 0.5532 (0.5465) acc 81.2500 (83.7500) lr 7.8853e-06 eta 0:00:07\n",
            "epoch [50/50] batch [10/18] time 0.163 (0.383) data 0.000 (0.169) loss 0.4172 (0.5072) acc 84.3750 (84.3750) lr 7.8853e-06 eta 0:00:03\n",
            "epoch [50/50] batch [15/18] time 0.154 (0.306) data 0.000 (0.113) loss 0.6567 (0.5224) acc 87.5000 (84.7917) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_16shot/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,426\n",
            "* accuracy: 93.4%\n",
            "* error: 6.6%\n",
            "* macro_f1: 93.3%\n",
            "Elapsed: 0:05:12\n",
            "\\n=== Final Accuracy ===\n",
            "* accuracy: 93.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Comparison\n",
        "\n",
        "Let's compare the performance across all three experiments to observe how accuracy improves with more training samples per class.\n",
        "\n",
        "### Expected Trends:\n",
        "- **1-shot**: ~90-91% (limited data)\n",
        "- **4-shot**: ~92-93% (moderate improvement)\n",
        "- **16-shot**: ~93-94% (best performance)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UjhTO99o7kN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare accuracy across all experiments\n",
        "!echo \"═══════════════════════════════════════════\"\n",
        "!echo \"           RESULTS COMPARISON\"\n",
        "!echo \"═══════════════════════════════════════════\"\n",
        "\n",
        "!echo \"\\n📊 1-Shot Results:\"\n",
        "!grep \"=> result\" -A 6 output/oxford_pets_1shot/log.txt | tail -6\n",
        "\n",
        "!echo \"\\n📊 4-Shot Results:\"\n",
        "!grep \"=> result\" -A 6 output/oxford_pets_4shot/log.txt | tail -6\n",
        "\n",
        "!echo \"\\n📊 16-Shot Results:\"\n",
        "!grep \"=> result\" -A 6 output/oxford_pets_16shot/log.txt | tail -6\n"
      ],
      "metadata": {
        "id": "h8FeZ04ErMqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0430f3d9-db63-4902-ec3b-aaecc70a2c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "═══════════════════════════════════════════\n",
            "           RESULTS COMPARISON\n",
            "═══════════════════════════════════════════\n",
            "\\n📊 1-Shot Results:\n",
            "* total: 3,669\n",
            "* correct: 3,328\n",
            "* accuracy: 90.7%\n",
            "* error: 9.3%\n",
            "* macro_f1: 90.6%\n",
            "Elapsed: 0:01:06\n",
            "\\n📊 4-Shot Results:\n",
            "* total: 3,669\n",
            "* correct: 3,398\n",
            "* accuracy: 92.6%\n",
            "* error: 7.4%\n",
            "* macro_f1: 92.5%\n",
            "Elapsed: 0:01:54\n",
            "\\n📊 16-Shot Results:\n",
            "* total: 3,669\n",
            "* correct: 3,426\n",
            "* accuracy: 93.4%\n",
            "* error: 6.6%\n",
            "* macro_f1: 93.3%\n",
            "Elapsed: 0:05:12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display saved model checkpoints for 16-shot experiment\n",
        "!echo \"=== Saved Checkpoints ===\"\n",
        "!ls -lh output/oxford_pets_16shot/\n",
        "\n",
        "!echo \"\\n=== Prompt Learner Models ===\"\n",
        "!ls -lh output/oxford_pets_16shot/prompt_learner/"
      ],
      "metadata": {
        "id": "UmylDfKM7oad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d775023-9bf2-46dd-c8be-057ff3dcd3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Saved Checkpoints ===\n",
            "total 44K\n",
            "-rw-r--r-- 1 root root  30K Nov 15 19:34 log.txt\n",
            "drwxr-xr-x 2 root root 4.0K Nov 15 19:33 prompt_learner\n",
            "drwxr-xr-x 2 root root 4.0K Nov 15 19:29 tensorboard\n",
            "\\n=== Prompt Learner Models ===\n",
            "total 2.3M\n",
            "-rw-r--r-- 1 root root   17 Nov 15 19:33 checkpoint\n",
            "-rw-r--r-- 1 root root 2.3M Nov 15 19:33 model.pth.tar-50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the actual training configuration used (from log file)\n",
        "!echo \"=== Training Configuration (16-shot) ===\"\n",
        "!sed -n '/\\*\\* Config \\*\\*/,/Loading trainer/p' output/oxford_pets_16shot/log.txt | head -60"
      ],
      "metadata": {
        "id": "FUF-ifA_7trO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b70f2f-7400-441c-aacc-e7bdd2648e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Configuration (16-shot) ===\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results to Google Drive\n",
        "\n",
        "To preserve your trained models and results permanently, we'll save them to Google Drive.\n",
        "\n",
        "### What Will Be Saved:\n",
        "- Trained model checkpoints (`.pth.tar` files)\n",
        "- Training logs (`.txt` files)\n",
        "- Configuration files\n",
        "- All experiment outputs\n",
        "\n",
        "**Note**: Make sure you have enough space in Google Drive (~200-300 MB per experiment).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PtViOcW57wVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to save results permanently\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kEEldh1L7u1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cf1b30-52b0-4905-c79e-f967473b131b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy all experiment outputs to Google Drive\n",
        "!mkdir -p /content/drive/MyDrive/CoOp_OxfordPets_Results\n",
        "\n",
        "!cp -r /content/CoOp/output/* /content/drive/MyDrive/CoOp_OxfordPets_Results/\n",
        "\n",
        "!echo \"✓ All results saved to Google Drive!\"\n",
        "!echo \"📁 Location: MyDrive/CoOp_OxfordPets_Results/\""
      ],
      "metadata": {
        "id": "d6mVrSBF71p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f22519-276b-4606-e0c1-697b5c12cbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All results saved to Google Drive!\n",
            "📁 Location: MyDrive/CoOp_OxfordPets_Results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a compressed archive of all results for easy download\n",
        "!cd /content/CoOp && zip -r oxford_pets_results.zip output/\n",
        "\n",
        "!echo \"✓ Results compressed successfully!\"\n",
        "!ls -lh oxford_pets_results.zip"
      ],
      "metadata": {
        "id": "29pxGLhE75Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea88a07a-59b0-429f-85ab-d483198c61b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: output/ (stored 0%)\n",
            "  adding: output/oxford_pets_16shot/ (stored 0%)\n",
            "  adding: output/oxford_pets_16shot/log.txt (deflated 73%)\n",
            "  adding: output/oxford_pets_16shot/tensorboard/ (stored 0%)\n",
            "  adding: output/oxford_pets_16shot/tensorboard/events.out.tfevents.1763234940.d3c8ed2ed7b1.5568.0 (deflated 71%)\n",
            "  adding: output/oxford_pets_16shot/prompt_learner/ (stored 0%)\n",
            "  adding: output/oxford_pets_16shot/prompt_learner/model.pth.tar-50 (deflated 92%)\n",
            "  adding: output/oxford_pets_16shot/prompt_learner/checkpoint (stored 0%)\n",
            "  adding: output/oxford_pets_1shot/ (stored 0%)\n",
            "  adding: output/oxford_pets_1shot/log.txt (deflated 66%)\n",
            "  adding: output/oxford_pets_1shot/tensorboard/ (stored 0%)\n",
            "  adding: output/oxford_pets_1shot/tensorboard/events.out.tfevents.1763234644.d3c8ed2ed7b1.1768.0 (deflated 67%)\n",
            "  adding: output/oxford_pets_1shot/prompt_learner/ (stored 0%)\n",
            "  adding: output/oxford_pets_1shot/prompt_learner/model.pth.tar-50 (deflated 92%)\n",
            "  adding: output/oxford_pets_1shot/prompt_learner/checkpoint (stored 0%)\n",
            "  adding: output/oxford_pets_4shot/ (stored 0%)\n",
            "  adding: output/oxford_pets_4shot/log.txt (deflated 74%)\n",
            "  adding: output/oxford_pets_4shot/tensorboard/ (stored 0%)\n",
            "  adding: output/oxford_pets_4shot/tensorboard/events.out.tfevents.1763234770.d3c8ed2ed7b1.3434.0 (deflated 71%)\n",
            "  adding: output/oxford_pets_4shot/prompt_learner/ (stored 0%)\n",
            "  adding: output/oxford_pets_4shot/prompt_learner/model.pth.tar-50 (deflated 92%)\n",
            "  adding: output/oxford_pets_4shot/prompt_learner/checkpoint (stored 0%)\n",
            "✓ Results compressed successfully!\n",
            "-rw-r--r-- 1 root root 658K Nov 15 19:36 oxford_pets_results.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics table\n",
        "import pandas as pd\n",
        "\n",
        "# Extract accuracies from log files\n",
        "results = {\n",
        "    'Experiment': ['1-Shot', '4-Shot', '16-Shot'],\n",
        "    'Training Samples': [37, 148, 592],\n",
        "    'Accuracy (%)': [],\n",
        "    'Macro F1 (%)': []\n",
        "}\n",
        "\n",
        "# Parse results (simplified - you can enhance this)\n",
        "!echo \"=== EXPERIMENT SUMMARY ===\"\n",
        "!echo \"┌─────────────┬──────────────────┬──────────────┬─────────────┐\"\n",
        "!echo \"│ Experiment  │ Training Samples │ Accuracy (%) │ Macro F1 (%)│\"\n",
        "!echo \"├─────────────┼──────────────────┼──────────────┼─────────────┤\"\n",
        "!echo \"│ 1-Shot      │       37         │    ~90.7     │    ~90.6    │\"\n",
        "!echo \"│ 4-Shot      │      148         │    ~92.6     │    ~92.5    │\"\n",
        "!echo \"│ 16-Shot     │      592         │    ~93.0+    │    ~93.0+   │\"\n",
        "!echo \"└─────────────┴──────────────────┴──────────────┴─────────────┘\"\n"
      ],
      "metadata": {
        "id": "56oM5aEq77zQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed361b4d-f691-47fd-dcea-e389bdfb4350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EXPERIMENT SUMMARY ===\n",
            "┌─────────────┬──────────────────┬──────────────┬─────────────┐\n",
            "│ Experiment  │ Training Samples │ Accuracy (%) │ Macro F1 (%)│\n",
            "├─────────────┼──────────────────┼──────────────┼─────────────┤\n",
            "│ 1-Shot      │       37         │    ~90.7     │    ~90.6    │\n",
            "│ 4-Shot      │      148         │    ~92.6     │    ~92.5    │\n",
            "│ 16-Shot     │      592         │    ~93.0+    │    ~93.0+   │\n",
            "└─────────────┴──────────────────┴──────────────┴─────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Custom Hyperparameter Experiments\n",
        "\n",
        "Now that we've established baseline results, let's explore how different hyperparameters affect model performance.\n",
        "\n",
        "### Hyperparameters to Experiment With:\n",
        "1. **Context Length** (`N_CTX`): Number of learnable prompt tokens\n",
        "2. **Learning Rate** (`LR`): Training speed\n",
        "3. **Class-Specific Context** (`CSC`): Separate prompts per class\n",
        "4. **Backbone Model**: Different CLIP architectures\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WcDbmHLh8PeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Train with 8 context tokens instead of 16\n",
        "# Expected: Slightly lower accuracy but faster training\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_custom_ctx8 \\\n",
        "DATASET.NUM_SHOTS 16 \\\n",
        "TRAINER.COOP.N_CTX 8"
      ],
      "metadata": {
        "id": "c6hrHpdW8JqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e8a214-ae8e-49a6-e64d-bc98eb843721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:37:09.288187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763235429.311641    9743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763235429.318085    9743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763235429.334323    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235429.334349    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235429.334353    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235429.334356    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:37:09.339484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.COOP.N_CTX', '8']\n",
            "output_dir: output/oxford_pets_custom_ctx8\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_custom_ctx8\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 8\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/CoOp/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X\"\n",
            "Number of context words (tokens): 8\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_custom_ctx8/tensorboard)\n",
            "epoch [1/50] batch [5/18] time 0.153 (1.270) data 0.000 (0.748) loss 1.4941 (2.1771) acc 56.2500 (56.2500) lr 1.0000e-05 eta 0:18:56\n",
            "epoch [1/50] batch [10/18] time 0.155 (0.712) data 0.000 (0.374) loss 1.3154 (1.9056) acc 53.1250 (57.8125) lr 1.0000e-05 eta 0:10:33\n",
            "epoch [1/50] batch [15/18] time 0.155 (0.526) data 0.000 (0.250) loss 0.9888 (1.6565) acc 71.8750 (60.6250) lr 1.0000e-05 eta 0:07:45\n",
            "epoch [2/50] batch [5/18] time 0.160 (0.603) data 0.000 (0.348) loss 0.6348 (0.8098) acc 81.2500 (71.2500) lr 2.0000e-03 eta 0:08:48\n",
            "epoch [2/50] batch [10/18] time 0.171 (0.382) data 0.000 (0.174) loss 0.7197 (0.8716) acc 75.0000 (70.6250) lr 2.0000e-03 eta 0:05:32\n",
            "epoch [2/50] batch [15/18] time 0.155 (0.305) data 0.000 (0.116) loss 0.8149 (0.8755) acc 78.1250 (72.0833) lr 2.0000e-03 eta 0:04:24\n",
            "epoch [3/50] batch [5/18] time 0.355 (1.001) data 0.000 (0.531) loss 0.9634 (0.6471) acc 65.6250 (79.3750) lr 1.9980e-03 eta 0:14:19\n",
            "epoch [3/50] batch [10/18] time 0.155 (0.585) data 0.000 (0.266) loss 1.0029 (0.7562) acc 68.7500 (75.9375) lr 1.9980e-03 eta 0:08:19\n",
            "epoch [3/50] batch [15/18] time 0.156 (0.443) data 0.000 (0.177) loss 0.5278 (0.6898) acc 87.5000 (78.1250) lr 1.9980e-03 eta 0:06:16\n",
            "epoch [4/50] batch [5/18] time 0.281 (0.617) data 0.000 (0.300) loss 0.5415 (0.5994) acc 81.2500 (81.2500) lr 1.9921e-03 eta 0:08:39\n",
            "epoch [4/50] batch [10/18] time 0.156 (0.387) data 0.000 (0.150) loss 0.7100 (0.6506) acc 75.0000 (78.7500) lr 1.9921e-03 eta 0:05:23\n",
            "epoch [4/50] batch [15/18] time 0.156 (0.311) data 0.000 (0.100) loss 0.6689 (0.6424) acc 78.1250 (78.7500) lr 1.9921e-03 eta 0:04:18\n",
            "epoch [5/50] batch [5/18] time 0.307 (0.615) data 0.000 (0.306) loss 0.6191 (0.7468) acc 78.1250 (76.8750) lr 1.9823e-03 eta 0:08:25\n",
            "epoch [5/50] batch [10/18] time 0.157 (0.388) data 0.000 (0.153) loss 0.8164 (0.6880) acc 65.6250 (77.5000) lr 1.9823e-03 eta 0:05:17\n",
            "epoch [5/50] batch [15/18] time 0.156 (0.311) data 0.000 (0.102) loss 0.8989 (0.6776) acc 81.2500 (78.9583) lr 1.9823e-03 eta 0:04:13\n",
            "epoch [6/50] batch [5/18] time 0.154 (0.737) data 0.000 (0.458) loss 0.6318 (0.6427) acc 81.2500 (78.1250) lr 1.9686e-03 eta 0:09:53\n",
            "epoch [6/50] batch [10/18] time 0.163 (0.447) data 0.000 (0.229) loss 0.5303 (0.6976) acc 84.3750 (78.7500) lr 1.9686e-03 eta 0:05:57\n",
            "epoch [6/50] batch [15/18] time 0.153 (0.349) data 0.000 (0.153) loss 0.7129 (0.7325) acc 81.2500 (78.5417) lr 1.9686e-03 eta 0:04:37\n",
            "epoch [7/50] batch [5/18] time 0.265 (0.594) data 0.000 (0.291) loss 0.7241 (0.7081) acc 75.0000 (78.1250) lr 1.9511e-03 eta 0:07:47\n",
            "epoch [7/50] batch [10/18] time 0.152 (0.374) data 0.000 (0.146) loss 1.0000 (0.7356) acc 78.1250 (78.1250) lr 1.9511e-03 eta 0:04:52\n",
            "epoch [7/50] batch [15/18] time 0.153 (0.301) data 0.000 (0.097) loss 0.5869 (0.6788) acc 84.3750 (79.7917) lr 1.9511e-03 eta 0:03:53\n",
            "epoch [8/50] batch [5/18] time 0.421 (0.813) data 0.000 (0.353) loss 0.6802 (0.6996) acc 75.0000 (80.0000) lr 1.9298e-03 eta 0:10:25\n",
            "epoch [8/50] batch [10/18] time 0.168 (0.524) data 0.000 (0.177) loss 0.5620 (0.6418) acc 78.1250 (79.6875) lr 1.9298e-03 eta 0:06:40\n",
            "epoch [8/50] batch [15/18] time 0.155 (0.401) data 0.000 (0.118) loss 0.9702 (0.6249) acc 65.6250 (80.4167) lr 1.9298e-03 eta 0:05:04\n",
            "epoch [9/50] batch [5/18] time 0.291 (0.595) data 0.000 (0.289) loss 0.8330 (0.5881) acc 71.8750 (82.5000) lr 1.9048e-03 eta 0:07:26\n",
            "epoch [9/50] batch [10/18] time 0.152 (0.375) data 0.000 (0.145) loss 0.5762 (0.5945) acc 78.1250 (81.8750) lr 1.9048e-03 eta 0:04:39\n",
            "epoch [9/50] batch [15/18] time 0.153 (0.301) data 0.000 (0.097) loss 0.7300 (0.5833) acc 68.7500 (80.6250) lr 1.9048e-03 eta 0:03:43\n",
            "epoch [10/50] batch [5/18] time 0.226 (0.574) data 0.000 (0.302) loss 1.0049 (0.7478) acc 75.0000 (77.5000) lr 1.8763e-03 eta 0:07:00\n",
            "epoch [10/50] batch [10/18] time 0.152 (0.369) data 0.000 (0.151) loss 0.4983 (0.6319) acc 78.1250 (79.0625) lr 1.8763e-03 eta 0:04:28\n",
            "epoch [10/50] batch [15/18] time 0.154 (0.297) data 0.000 (0.101) loss 0.9458 (0.6440) acc 78.1250 (79.1667) lr 1.8763e-03 eta 0:03:35\n",
            "epoch [11/50] batch [5/18] time 0.167 (0.864) data 0.006 (0.550) loss 0.4766 (0.5257) acc 78.1250 (81.2500) lr 1.8443e-03 eta 0:10:18\n",
            "epoch [11/50] batch [10/18] time 0.155 (0.509) data 0.000 (0.275) loss 0.6826 (0.5529) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:06:01\n",
            "epoch [11/50] batch [15/18] time 0.153 (0.390) data 0.000 (0.183) loss 0.6050 (0.5446) acc 84.3750 (82.0833) lr 1.8443e-03 eta 0:04:35\n",
            "epoch [12/50] batch [5/18] time 0.161 (0.629) data 0.001 (0.381) loss 0.3423 (0.4625) acc 87.5000 (85.6250) lr 1.8090e-03 eta 0:07:18\n",
            "epoch [12/50] batch [10/18] time 0.155 (0.391) data 0.000 (0.191) loss 0.4019 (0.5418) acc 90.6250 (83.4375) lr 1.8090e-03 eta 0:04:30\n",
            "epoch [12/50] batch [15/18] time 0.153 (0.311) data 0.000 (0.127) loss 0.6367 (0.5242) acc 81.2500 (84.7917) lr 1.8090e-03 eta 0:03:33\n",
            "epoch [13/50] batch [5/18] time 0.232 (0.854) data 0.000 (0.409) loss 0.3477 (0.5020) acc 84.3750 (81.2500) lr 1.7705e-03 eta 0:09:39\n",
            "epoch [13/50] batch [10/18] time 0.152 (0.507) data 0.000 (0.205) loss 0.3855 (0.5476) acc 84.3750 (79.6875) lr 1.7705e-03 eta 0:05:41\n",
            "epoch [13/50] batch [15/18] time 0.151 (0.390) data 0.000 (0.136) loss 0.9087 (0.6200) acc 65.6250 (77.2917) lr 1.7705e-03 eta 0:04:20\n",
            "epoch [14/50] batch [5/18] time 0.185 (0.691) data 0.000 (0.402) loss 1.0479 (0.6771) acc 59.3750 (75.0000) lr 1.7290e-03 eta 0:07:36\n",
            "epoch [14/50] batch [10/18] time 0.160 (0.423) data 0.000 (0.201) loss 0.7568 (0.6076) acc 81.2500 (79.6875) lr 1.7290e-03 eta 0:04:37\n",
            "epoch [14/50] batch [15/18] time 0.153 (0.333) data 0.000 (0.134) loss 0.8081 (0.5903) acc 71.8750 (80.4167) lr 1.7290e-03 eta 0:03:36\n",
            "epoch [15/50] batch [5/18] time 0.162 (0.585) data 0.000 (0.301) loss 0.5532 (0.5803) acc 81.2500 (83.7500) lr 1.6845e-03 eta 0:06:16\n",
            "epoch [15/50] batch [10/18] time 0.163 (0.372) data 0.000 (0.151) loss 0.7686 (0.5531) acc 65.6250 (83.4375) lr 1.6845e-03 eta 0:03:57\n",
            "epoch [15/50] batch [15/18] time 0.153 (0.299) data 0.000 (0.101) loss 0.5947 (0.5552) acc 78.1250 (83.1250) lr 1.6845e-03 eta 0:03:09\n",
            "epoch [16/50] batch [5/18] time 0.175 (0.770) data 0.000 (0.504) loss 0.4419 (0.4946) acc 84.3750 (83.7500) lr 1.6374e-03 eta 0:08:01\n",
            "epoch [16/50] batch [10/18] time 0.165 (0.464) data 0.000 (0.252) loss 0.4414 (0.5505) acc 84.3750 (80.3125) lr 1.6374e-03 eta 0:04:47\n",
            "epoch [16/50] batch [15/18] time 0.154 (0.361) data 0.000 (0.168) loss 0.5049 (0.5828) acc 81.2500 (80.0000) lr 1.6374e-03 eta 0:03:41\n",
            "epoch [17/50] batch [5/18] time 0.228 (0.592) data 0.000 (0.307) loss 0.4749 (0.5344) acc 81.2500 (83.1250) lr 1.5878e-03 eta 0:05:59\n",
            "epoch [17/50] batch [10/18] time 0.154 (0.375) data 0.000 (0.154) loss 0.6685 (0.6011) acc 81.2500 (82.8125) lr 1.5878e-03 eta 0:03:45\n",
            "epoch [17/50] batch [15/18] time 0.155 (0.302) data 0.000 (0.102) loss 0.5117 (0.5672) acc 84.3750 (83.9583) lr 1.5878e-03 eta 0:03:00\n",
            "epoch [18/50] batch [5/18] time 0.394 (0.868) data 0.000 (0.374) loss 0.4983 (0.4230) acc 87.5000 (88.1250) lr 1.5358e-03 eta 0:08:31\n",
            "epoch [18/50] batch [10/18] time 0.155 (0.523) data 0.000 (0.187) loss 0.5205 (0.4818) acc 81.2500 (86.5625) lr 1.5358e-03 eta 0:05:05\n",
            "epoch [18/50] batch [15/18] time 0.155 (0.401) data 0.000 (0.125) loss 0.6309 (0.4870) acc 75.0000 (85.4167) lr 1.5358e-03 eta 0:03:52\n",
            "epoch [19/50] batch [5/18] time 0.348 (0.654) data 0.000 (0.304) loss 0.8374 (0.6706) acc 71.8750 (80.0000) lr 1.4818e-03 eta 0:06:13\n",
            "epoch [19/50] batch [10/18] time 0.154 (0.408) data 0.000 (0.152) loss 0.3723 (0.6546) acc 90.6250 (80.3125) lr 1.4818e-03 eta 0:03:51\n",
            "epoch [19/50] batch [15/18] time 0.154 (0.324) data 0.000 (0.102) loss 0.3062 (0.5970) acc 87.5000 (82.7083) lr 1.4818e-03 eta 0:03:01\n",
            "epoch [20/50] batch [5/18] time 0.194 (0.631) data 0.001 (0.348) loss 0.4661 (0.6814) acc 75.0000 (78.7500) lr 1.4258e-03 eta 0:05:48\n",
            "epoch [20/50] batch [10/18] time 0.158 (0.394) data 0.000 (0.174) loss 0.8643 (0.6924) acc 68.7500 (77.5000) lr 1.4258e-03 eta 0:03:35\n",
            "epoch [20/50] batch [15/18] time 0.155 (0.315) data 0.000 (0.116) loss 0.6489 (0.6318) acc 84.3750 (80.0000) lr 1.4258e-03 eta 0:02:50\n",
            "epoch [21/50] batch [5/18] time 0.199 (0.746) data 0.000 (0.506) loss 0.5723 (0.5115) acc 84.3750 (85.6250) lr 1.3681e-03 eta 0:06:39\n",
            "epoch [21/50] batch [10/18] time 0.162 (0.452) data 0.000 (0.253) loss 0.2991 (0.4815) acc 93.7500 (85.3125) lr 1.3681e-03 eta 0:03:59\n",
            "epoch [21/50] batch [15/18] time 0.151 (0.353) data 0.000 (0.169) loss 0.4766 (0.5193) acc 78.1250 (83.3333) lr 1.3681e-03 eta 0:03:05\n",
            "epoch [22/50] batch [5/18] time 0.209 (0.584) data 0.011 (0.316) loss 0.7070 (0.5432) acc 75.0000 (80.0000) lr 1.3090e-03 eta 0:05:01\n",
            "epoch [22/50] batch [10/18] time 0.152 (0.370) data 0.000 (0.158) loss 0.8711 (0.6150) acc 75.0000 (80.6250) lr 1.3090e-03 eta 0:03:09\n",
            "epoch [22/50] batch [15/18] time 0.154 (0.299) data 0.000 (0.105) loss 0.4585 (0.5921) acc 90.6250 (81.2500) lr 1.3090e-03 eta 0:02:31\n",
            "epoch [23/50] batch [5/18] time 0.288 (0.872) data 0.000 (0.426) loss 0.4221 (0.4926) acc 84.3750 (83.1250) lr 1.2487e-03 eta 0:07:15\n",
            "epoch [23/50] batch [10/18] time 0.152 (0.525) data 0.000 (0.214) loss 0.5620 (0.5155) acc 81.2500 (82.5000) lr 1.2487e-03 eta 0:04:19\n",
            "epoch [23/50] batch [15/18] time 0.152 (0.401) data 0.000 (0.143) loss 0.6064 (0.5276) acc 75.0000 (81.4583) lr 1.2487e-03 eta 0:03:16\n",
            "epoch [24/50] batch [5/18] time 0.219 (0.619) data 0.000 (0.384) loss 0.6953 (0.5198) acc 84.3750 (86.2500) lr 1.1874e-03 eta 0:04:57\n",
            "epoch [24/50] batch [10/18] time 0.156 (0.386) data 0.000 (0.192) loss 0.7437 (0.5064) acc 84.3750 (86.2500) lr 1.1874e-03 eta 0:03:03\n",
            "epoch [24/50] batch [15/18] time 0.151 (0.309) data 0.000 (0.128) loss 0.5791 (0.5031) acc 84.3750 (87.5000) lr 1.1874e-03 eta 0:02:25\n",
            "epoch [25/50] batch [5/18] time 0.155 (0.614) data 0.000 (0.339) loss 0.5776 (0.4920) acc 87.5000 (83.1250) lr 1.1253e-03 eta 0:04:44\n",
            "epoch [25/50] batch [10/18] time 0.158 (0.384) data 0.000 (0.169) loss 0.4409 (0.4600) acc 81.2500 (83.7500) lr 1.1253e-03 eta 0:02:56\n",
            "epoch [25/50] batch [15/18] time 0.151 (0.307) data 0.000 (0.113) loss 0.5200 (0.4988) acc 84.3750 (82.5000) lr 1.1253e-03 eta 0:02:19\n",
            "epoch [26/50] batch [5/18] time 0.164 (0.784) data 0.003 (0.467) loss 0.7163 (0.4866) acc 71.8750 (82.5000) lr 1.0628e-03 eta 0:05:48\n",
            "epoch [26/50] batch [10/18] time 0.154 (0.470) data 0.000 (0.234) loss 1.0312 (0.5802) acc 68.7500 (81.5625) lr 1.0628e-03 eta 0:03:26\n",
            "epoch [26/50] batch [15/18] time 0.153 (0.364) data 0.000 (0.156) loss 0.5283 (0.5945) acc 75.0000 (81.8750) lr 1.0628e-03 eta 0:02:38\n",
            "epoch [27/50] batch [5/18] time 0.228 (0.597) data 0.000 (0.311) loss 0.2844 (0.5120) acc 93.7500 (85.0000) lr 1.0000e-03 eta 0:04:14\n",
            "epoch [27/50] batch [10/18] time 0.151 (0.376) data 0.000 (0.156) loss 0.6558 (0.5228) acc 81.2500 (84.3750) lr 1.0000e-03 eta 0:02:38\n",
            "epoch [27/50] batch [15/18] time 0.153 (0.302) data 0.000 (0.104) loss 0.7407 (0.5666) acc 78.1250 (83.5417) lr 1.0000e-03 eta 0:02:05\n",
            "epoch [28/50] batch [5/18] time 0.381 (0.812) data 0.000 (0.331) loss 0.6011 (0.5073) acc 84.3750 (85.0000) lr 9.3721e-04 eta 0:05:32\n",
            "epoch [28/50] batch [10/18] time 0.153 (0.491) data 0.000 (0.166) loss 0.7280 (0.5114) acc 78.1250 (85.0000) lr 9.3721e-04 eta 0:03:18\n",
            "epoch [28/50] batch [15/18] time 0.152 (0.378) data 0.000 (0.111) loss 0.5107 (0.4945) acc 84.3750 (85.2083) lr 9.3721e-04 eta 0:02:30\n",
            "epoch [29/50] batch [5/18] time 0.261 (0.587) data 0.000 (0.298) loss 0.3438 (0.4555) acc 90.6250 (86.2500) lr 8.7467e-04 eta 0:03:49\n",
            "epoch [29/50] batch [10/18] time 0.153 (0.371) data 0.000 (0.149) loss 0.5840 (0.5044) acc 84.3750 (84.0625) lr 8.7467e-04 eta 0:02:23\n",
            "epoch [29/50] batch [15/18] time 0.153 (0.299) data 0.000 (0.100) loss 0.4985 (0.5237) acc 87.5000 (83.5417) lr 8.7467e-04 eta 0:01:53\n",
            "epoch [30/50] batch [5/18] time 0.163 (0.685) data 0.003 (0.348) loss 0.4727 (0.5410) acc 84.3750 (82.5000) lr 8.1262e-04 eta 0:04:15\n",
            "epoch [30/50] batch [10/18] time 0.161 (0.420) data 0.000 (0.174) loss 0.2959 (0.4821) acc 93.7500 (85.6250) lr 8.1262e-04 eta 0:02:34\n",
            "epoch [30/50] batch [15/18] time 0.152 (0.331) data 0.000 (0.116) loss 0.6245 (0.5177) acc 78.1250 (83.5417) lr 8.1262e-04 eta 0:02:00\n",
            "epoch [31/50] batch [5/18] time 0.320 (0.770) data 0.000 (0.453) loss 0.5464 (0.5375) acc 84.3750 (82.5000) lr 7.5131e-04 eta 0:04:33\n",
            "epoch [31/50] batch [10/18] time 0.154 (0.467) data 0.000 (0.227) loss 0.6626 (0.5330) acc 78.1250 (82.8125) lr 7.5131e-04 eta 0:02:43\n",
            "epoch [31/50] batch [15/18] time 0.155 (0.364) data 0.000 (0.151) loss 0.7241 (0.5833) acc 81.2500 (81.4583) lr 7.5131e-04 eta 0:02:05\n",
            "epoch [32/50] batch [5/18] time 0.161 (0.625) data 0.003 (0.370) loss 0.5425 (0.4842) acc 87.5000 (83.7500) lr 6.9098e-04 eta 0:03:30\n",
            "epoch [32/50] batch [10/18] time 0.160 (0.391) data 0.000 (0.185) loss 0.3140 (0.4517) acc 87.5000 (85.0000) lr 6.9098e-04 eta 0:02:09\n",
            "epoch [32/50] batch [15/18] time 0.153 (0.312) data 0.000 (0.124) loss 0.4448 (0.4640) acc 84.3750 (85.0000) lr 6.9098e-04 eta 0:01:41\n",
            "epoch [33/50] batch [5/18] time 0.476 (0.860) data 0.000 (0.343) loss 0.7393 (0.5211) acc 75.0000 (83.7500) lr 6.3188e-04 eta 0:04:34\n",
            "epoch [33/50] batch [10/18] time 0.154 (0.529) data 0.000 (0.172) loss 0.2020 (0.4297) acc 96.8750 (87.1875) lr 6.3188e-04 eta 0:02:45\n",
            "epoch [33/50] batch [15/18] time 0.153 (0.404) data 0.000 (0.115) loss 0.3264 (0.4617) acc 90.6250 (86.6667) lr 6.3188e-04 eta 0:02:04\n",
            "epoch [34/50] batch [5/18] time 0.217 (0.621) data 0.002 (0.317) loss 0.4421 (0.4585) acc 84.3750 (84.3750) lr 5.7422e-04 eta 0:03:06\n",
            "epoch [34/50] batch [10/18] time 0.156 (0.388) data 0.000 (0.159) loss 0.7007 (0.5052) acc 78.1250 (83.4375) lr 5.7422e-04 eta 0:01:54\n",
            "epoch [34/50] batch [15/18] time 0.154 (0.310) data 0.000 (0.106) loss 0.4844 (0.5112) acc 84.3750 (84.1667) lr 5.7422e-04 eta 0:01:30\n",
            "epoch [35/50] batch [5/18] time 0.224 (0.615) data 0.000 (0.326) loss 0.4668 (0.6006) acc 84.3750 (83.1250) lr 5.1825e-04 eta 0:02:54\n",
            "epoch [35/50] batch [10/18] time 0.155 (0.393) data 0.002 (0.164) loss 0.4346 (0.5883) acc 78.1250 (83.4375) lr 5.1825e-04 eta 0:01:49\n",
            "epoch [35/50] batch [15/18] time 0.156 (0.314) data 0.000 (0.110) loss 0.2168 (0.5341) acc 93.7500 (84.5833) lr 5.1825e-04 eta 0:01:25\n",
            "epoch [36/50] batch [5/18] time 0.188 (0.794) data 0.003 (0.546) loss 0.7827 (0.5355) acc 75.0000 (81.8750) lr 4.6417e-04 eta 0:03:30\n",
            "epoch [36/50] batch [10/18] time 0.165 (0.475) data 0.000 (0.273) loss 0.4451 (0.4938) acc 84.3750 (82.8125) lr 4.6417e-04 eta 0:02:03\n",
            "epoch [36/50] batch [15/18] time 0.153 (0.368) data 0.000 (0.182) loss 0.4023 (0.5139) acc 87.5000 (82.5000) lr 4.6417e-04 eta 0:01:33\n",
            "epoch [37/50] batch [5/18] time 0.152 (0.615) data 0.000 (0.343) loss 0.7319 (0.4940) acc 75.0000 (82.5000) lr 4.1221e-04 eta 0:02:32\n",
            "epoch [37/50] batch [10/18] time 0.158 (0.385) data 0.000 (0.172) loss 1.0273 (0.5443) acc 75.0000 (82.8125) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [37/50] batch [15/18] time 0.152 (0.308) data 0.000 (0.114) loss 0.6582 (0.5595) acc 84.3750 (83.1250) lr 4.1221e-04 eta 0:01:12\n",
            "epoch [38/50] batch [5/18] time 0.355 (0.872) data 0.000 (0.408) loss 0.2539 (0.4418) acc 93.7500 (88.1250) lr 3.6258e-04 eta 0:03:19\n",
            "epoch [38/50] batch [10/18] time 0.152 (0.530) data 0.000 (0.204) loss 0.5537 (0.4076) acc 81.2500 (88.1250) lr 3.6258e-04 eta 0:01:58\n",
            "epoch [38/50] batch [15/18] time 0.154 (0.405) data 0.000 (0.136) loss 0.7754 (0.4815) acc 78.1250 (86.0417) lr 3.6258e-04 eta 0:01:28\n",
            "epoch [39/50] batch [5/18] time 0.260 (0.606) data 0.000 (0.309) loss 0.7188 (0.5210) acc 71.8750 (84.3750) lr 3.1545e-04 eta 0:02:07\n",
            "epoch [39/50] batch [10/18] time 0.153 (0.381) data 0.000 (0.155) loss 0.6465 (0.5236) acc 81.2500 (84.3750) lr 3.1545e-04 eta 0:01:18\n",
            "epoch [39/50] batch [15/18] time 0.153 (0.305) data 0.000 (0.103) loss 0.5469 (0.5308) acc 81.2500 (84.1667) lr 3.1545e-04 eta 0:01:01\n",
            "epoch [40/50] batch [5/18] time 0.188 (0.597) data 0.000 (0.303) loss 0.7969 (0.5326) acc 84.3750 (85.6250) lr 2.7103e-04 eta 0:01:55\n",
            "epoch [40/50] batch [10/18] time 0.159 (0.380) data 0.000 (0.152) loss 0.6304 (0.5031) acc 84.3750 (85.6250) lr 2.7103e-04 eta 0:01:11\n",
            "epoch [40/50] batch [15/18] time 0.151 (0.304) data 0.000 (0.101) loss 0.5981 (0.5224) acc 87.5000 (84.5833) lr 2.7103e-04 eta 0:00:55\n",
            "epoch [41/50] batch [5/18] time 0.190 (0.800) data 0.000 (0.502) loss 0.4438 (0.5499) acc 87.5000 (85.0000) lr 2.2949e-04 eta 0:02:20\n",
            "epoch [41/50] batch [10/18] time 0.154 (0.478) data 0.000 (0.251) loss 0.3533 (0.4622) acc 90.6250 (86.8750) lr 2.2949e-04 eta 0:01:21\n",
            "epoch [41/50] batch [15/18] time 0.151 (0.370) data 0.000 (0.168) loss 0.4746 (0.4754) acc 81.2500 (85.8333) lr 2.2949e-04 eta 0:01:01\n",
            "epoch [42/50] batch [5/18] time 0.288 (0.601) data 0.006 (0.275) loss 0.5142 (0.4917) acc 90.6250 (87.5000) lr 1.9098e-04 eta 0:01:34\n",
            "epoch [42/50] batch [10/18] time 0.153 (0.378) data 0.000 (0.138) loss 0.5781 (0.5246) acc 84.3750 (87.1875) lr 1.9098e-04 eta 0:00:57\n",
            "epoch [42/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.092) loss 0.4680 (0.5316) acc 90.6250 (86.4583) lr 1.9098e-04 eta 0:00:44\n",
            "epoch [43/50] batch [5/18] time 0.233 (0.963) data 0.000 (0.543) loss 0.6621 (0.5421) acc 78.1250 (85.0000) lr 1.5567e-04 eta 0:02:13\n",
            "epoch [43/50] batch [10/18] time 0.153 (0.562) data 0.000 (0.272) loss 0.7710 (0.5397) acc 75.0000 (85.0000) lr 1.5567e-04 eta 0:01:15\n",
            "epoch [43/50] batch [15/18] time 0.152 (0.426) data 0.000 (0.182) loss 0.7095 (0.5729) acc 71.8750 (83.3333) lr 1.5567e-04 eta 0:00:54\n",
            "epoch [44/50] batch [5/18] time 0.160 (0.628) data 0.003 (0.389) loss 0.4902 (0.4489) acc 81.2500 (85.6250) lr 1.2369e-04 eta 0:01:15\n",
            "epoch [44/50] batch [10/18] time 0.154 (0.392) data 0.000 (0.195) loss 0.9028 (0.4927) acc 71.8750 (85.0000) lr 1.2369e-04 eta 0:00:45\n",
            "epoch [44/50] batch [15/18] time 0.153 (0.312) data 0.000 (0.130) loss 0.4702 (0.4543) acc 87.5000 (86.6667) lr 1.2369e-04 eta 0:00:34\n",
            "epoch [45/50] batch [5/18] time 0.309 (0.648) data 0.000 (0.316) loss 0.7061 (0.5420) acc 84.3750 (83.1250) lr 9.5173e-05 eta 0:01:06\n",
            "epoch [45/50] batch [10/18] time 0.154 (0.413) data 0.000 (0.158) loss 0.2219 (0.4670) acc 96.8750 (86.2500) lr 9.5173e-05 eta 0:00:40\n",
            "epoch [45/50] batch [15/18] time 0.154 (0.327) data 0.000 (0.106) loss 0.5864 (0.4425) acc 78.1250 (86.4583) lr 9.5173e-05 eta 0:00:30\n",
            "epoch [46/50] batch [5/18] time 0.382 (0.735) data 0.016 (0.412) loss 0.5015 (0.5430) acc 87.5000 (83.1250) lr 7.0224e-05 eta 0:01:02\n",
            "epoch [46/50] batch [10/18] time 0.153 (0.445) data 0.000 (0.206) loss 0.7051 (0.5591) acc 75.0000 (82.8125) lr 7.0224e-05 eta 0:00:35\n",
            "epoch [46/50] batch [15/18] time 0.153 (0.349) data 0.000 (0.138) loss 0.4016 (0.5120) acc 87.5000 (84.5833) lr 7.0224e-05 eta 0:00:26\n",
            "epoch [47/50] batch [5/18] time 0.295 (0.581) data 0.000 (0.304) loss 0.4355 (0.4870) acc 87.5000 (88.1250) lr 4.8943e-05 eta 0:00:38\n",
            "epoch [47/50] batch [10/18] time 0.153 (0.371) data 0.000 (0.153) loss 0.7119 (0.5425) acc 81.2500 (85.6250) lr 4.8943e-05 eta 0:00:22\n",
            "epoch [47/50] batch [15/18] time 0.153 (0.299) data 0.000 (0.102) loss 0.6099 (0.5074) acc 81.2500 (85.6250) lr 4.8943e-05 eta 0:00:17\n",
            "epoch [48/50] batch [5/18] time 0.154 (0.939) data 0.002 (0.662) loss 0.3887 (0.4466) acc 87.5000 (85.6250) lr 3.1417e-05 eta 0:00:45\n",
            "epoch [48/50] batch [10/18] time 0.156 (0.547) data 0.000 (0.331) loss 0.4487 (0.5079) acc 81.2500 (84.3750) lr 3.1417e-05 eta 0:00:24\n",
            "epoch [48/50] batch [15/18] time 0.153 (0.416) data 0.000 (0.221) loss 0.5425 (0.5413) acc 84.3750 (83.9583) lr 3.1417e-05 eta 0:00:16\n",
            "epoch [49/50] batch [5/18] time 0.163 (0.614) data 0.000 (0.333) loss 0.6006 (0.5576) acc 84.3750 (83.1250) lr 1.7713e-05 eta 0:00:19\n",
            "epoch [49/50] batch [10/18] time 0.161 (0.386) data 0.000 (0.167) loss 0.4021 (0.4687) acc 84.3750 (85.6250) lr 1.7713e-05 eta 0:00:10\n",
            "epoch [49/50] batch [15/18] time 0.152 (0.308) data 0.000 (0.111) loss 0.6021 (0.5484) acc 84.3750 (83.7500) lr 1.7713e-05 eta 0:00:06\n",
            "epoch [50/50] batch [5/18] time 0.421 (0.724) data 0.000 (0.281) loss 0.6382 (0.6336) acc 78.1250 (81.8750) lr 7.8853e-06 eta 0:00:09\n",
            "epoch [50/50] batch [10/18] time 0.154 (0.462) data 0.002 (0.141) loss 0.7153 (0.5646) acc 78.1250 (83.7500) lr 7.8853e-06 eta 0:00:03\n",
            "epoch [50/50] batch [15/18] time 0.153 (0.360) data 0.000 (0.094) loss 0.6743 (0.5651) acc 75.0000 (83.3333) lr 7.8853e-06 eta 0:00:01\n",
            "Checkpoint saved to output/oxford_pets_custom_ctx8/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:26<00:00,  1.40it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,411\n",
            "* accuracy: 93.0%\n",
            "* error: 7.0%\n",
            "* macro_f1: 92.9%\n",
            "Elapsed: 0:05:17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Enable class-specific context (CSC)\n",
        "# Each class gets its own learnable prompt\n",
        "# Expected: Potentially higher accuracy but more parameters\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_custom_csc \\\n",
        "DATASET.NUM_SHOTS 16 \\\n",
        "TRAINER.COOP.CSC True"
      ],
      "metadata": {
        "id": "v3u7qCA08SZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f11991e-2e79-4b80-cf23-5a0a8d9da0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:43:03.730531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763235783.751785   13056 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763235783.758414   13056 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763235783.775454   13056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235783.775482   13056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235783.775486   13056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763235783.775489   13056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:43:03.780336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.COOP.CSC', 'True']\n",
            "output_dir: output/oxford_pets_custom_csc\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_custom_csc\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: True\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/CoOp/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing class-specific contexts\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_custom_csc/tensorboard)\n",
            "epoch [1/50] batch [5/18] time 0.151 (1.399) data 0.000 (0.646) loss 2.1914 (2.4293) acc 43.7500 (44.3750) lr 1.0000e-05 eta 0:20:52\n",
            "epoch [1/50] batch [10/18] time 0.153 (0.776) data 0.000 (0.323) loss 2.0176 (2.2428) acc 53.1250 (50.6250) lr 1.0000e-05 eta 0:11:30\n",
            "epoch [1/50] batch [15/18] time 0.152 (0.571) data 0.000 (0.216) loss 1.8613 (2.1778) acc 59.3750 (51.0417) lr 1.0000e-05 eta 0:08:24\n",
            "epoch [2/50] batch [5/18] time 0.161 (0.628) data 0.000 (0.393) loss 0.9043 (1.7188) acc 78.1250 (58.7500) lr 2.0000e-03 eta 0:09:11\n",
            "epoch [2/50] batch [10/18] time 0.162 (0.393) data 0.000 (0.197) loss 1.4531 (1.5455) acc 59.3750 (62.1875) lr 2.0000e-03 eta 0:05:42\n",
            "epoch [2/50] batch [15/18] time 0.154 (0.314) data 0.000 (0.131) loss 0.8120 (1.4271) acc 78.1250 (63.3333) lr 2.0000e-03 eta 0:04:32\n",
            "epoch [3/50] batch [5/18] time 0.439 (0.798) data 0.002 (0.276) loss 0.5552 (0.7406) acc 87.5000 (74.3750) lr 1.9980e-03 eta 0:11:25\n",
            "epoch [3/50] batch [10/18] time 0.157 (0.495) data 0.002 (0.138) loss 0.9448 (0.7567) acc 71.8750 (75.0000) lr 1.9980e-03 eta 0:07:02\n",
            "epoch [3/50] batch [15/18] time 0.156 (0.383) data 0.000 (0.092) loss 0.5615 (0.7627) acc 84.3750 (75.4167) lr 1.9980e-03 eta 0:05:25\n",
            "epoch [4/50] batch [5/18] time 0.160 (0.618) data 0.000 (0.374) loss 0.3357 (0.5524) acc 93.7500 (83.7500) lr 1.9921e-03 eta 0:08:39\n",
            "epoch [4/50] batch [10/18] time 0.156 (0.388) data 0.000 (0.187) loss 0.5312 (0.5811) acc 81.2500 (82.8125) lr 1.9921e-03 eta 0:05:24\n",
            "epoch [4/50] batch [15/18] time 0.156 (0.311) data 0.000 (0.125) loss 0.9229 (0.6120) acc 81.2500 (82.7083) lr 1.9921e-03 eta 0:04:18\n",
            "epoch [5/50] batch [5/18] time 0.161 (0.649) data 0.000 (0.398) loss 0.8467 (0.4846) acc 75.0000 (85.6250) lr 1.9823e-03 eta 0:08:53\n",
            "epoch [5/50] batch [10/18] time 0.159 (0.405) data 0.000 (0.199) loss 0.4368 (0.4442) acc 87.5000 (86.2500) lr 1.9823e-03 eta 0:05:31\n",
            "epoch [5/50] batch [15/18] time 0.162 (0.322) data 0.000 (0.133) loss 0.6050 (0.5330) acc 81.2500 (83.3333) lr 1.9823e-03 eta 0:04:22\n",
            "epoch [6/50] batch [5/18] time 0.200 (0.869) data 0.000 (0.577) loss 0.7207 (0.4853) acc 81.2500 (84.3750) lr 1.9686e-03 eta 0:11:39\n",
            "epoch [6/50] batch [10/18] time 0.166 (0.514) data 0.000 (0.289) loss 0.3240 (0.4923) acc 90.6250 (84.6875) lr 1.9686e-03 eta 0:06:50\n",
            "epoch [6/50] batch [15/18] time 0.154 (0.395) data 0.000 (0.192) loss 0.4470 (0.5143) acc 87.5000 (84.3750) lr 1.9686e-03 eta 0:05:13\n",
            "epoch [7/50] batch [5/18] time 0.213 (0.611) data 0.000 (0.347) loss 0.3889 (0.2927) acc 93.7500 (93.1250) lr 1.9511e-03 eta 0:08:00\n",
            "epoch [7/50] batch [10/18] time 0.161 (0.384) data 0.000 (0.174) loss 0.3896 (0.3890) acc 87.5000 (89.6875) lr 1.9511e-03 eta 0:05:00\n",
            "epoch [7/50] batch [15/18] time 0.158 (0.309) data 0.000 (0.116) loss 0.6333 (0.4023) acc 78.1250 (87.7083) lr 1.9511e-03 eta 0:03:59\n",
            "epoch [8/50] batch [5/18] time 0.498 (0.851) data 0.017 (0.351) loss 0.1807 (0.2857) acc 93.7500 (94.3750) lr 1.9298e-03 eta 0:10:54\n",
            "epoch [8/50] batch [10/18] time 0.161 (0.533) data 0.001 (0.176) loss 0.4602 (0.3377) acc 81.2500 (90.3125) lr 1.9298e-03 eta 0:06:47\n",
            "epoch [8/50] batch [15/18] time 0.154 (0.408) data 0.000 (0.118) loss 0.4363 (0.4023) acc 90.6250 (87.9167) lr 1.9298e-03 eta 0:05:09\n",
            "epoch [9/50] batch [5/18] time 0.197 (0.602) data 0.000 (0.286) loss 0.2766 (0.3349) acc 90.6250 (88.7500) lr 1.9048e-03 eta 0:07:32\n",
            "epoch [9/50] batch [10/18] time 0.158 (0.380) data 0.000 (0.143) loss 0.3979 (0.4500) acc 87.5000 (85.9375) lr 1.9048e-03 eta 0:04:43\n",
            "epoch [9/50] batch [15/18] time 0.153 (0.305) data 0.000 (0.096) loss 0.3149 (0.4225) acc 87.5000 (86.4583) lr 1.9048e-03 eta 0:03:45\n",
            "epoch [10/50] batch [5/18] time 0.260 (0.584) data 0.000 (0.289) loss 0.5117 (0.3968) acc 84.3750 (87.5000) lr 1.8763e-03 eta 0:07:07\n",
            "epoch [10/50] batch [10/18] time 0.154 (0.371) data 0.000 (0.145) loss 0.3081 (0.4191) acc 90.6250 (85.6250) lr 1.8763e-03 eta 0:04:29\n",
            "epoch [10/50] batch [15/18] time 0.154 (0.299) data 0.000 (0.097) loss 0.5044 (0.4211) acc 84.3750 (86.0417) lr 1.8763e-03 eta 0:03:36\n",
            "epoch [11/50] batch [5/18] time 0.215 (0.765) data 0.000 (0.492) loss 0.5601 (0.3215) acc 81.2500 (90.0000) lr 1.8443e-03 eta 0:09:07\n",
            "epoch [11/50] batch [10/18] time 0.153 (0.462) data 0.000 (0.247) loss 0.1160 (0.3421) acc 100.0000 (90.3125) lr 1.8443e-03 eta 0:05:28\n",
            "epoch [11/50] batch [15/18] time 0.151 (0.359) data 0.000 (0.165) loss 0.4490 (0.3771) acc 84.3750 (89.5833) lr 1.8443e-03 eta 0:04:13\n",
            "epoch [12/50] batch [5/18] time 0.159 (0.624) data 0.001 (0.349) loss 0.3738 (0.3258) acc 90.6250 (90.6250) lr 1.8090e-03 eta 0:07:15\n",
            "epoch [12/50] batch [10/18] time 0.161 (0.389) data 0.000 (0.175) loss 0.4714 (0.3480) acc 87.5000 (89.6875) lr 1.8090e-03 eta 0:04:29\n",
            "epoch [12/50] batch [15/18] time 0.152 (0.310) data 0.000 (0.116) loss 0.4346 (0.3950) acc 87.5000 (87.9167) lr 1.8090e-03 eta 0:03:33\n",
            "epoch [13/50] batch [5/18] time 0.451 (0.934) data 0.000 (0.396) loss 0.3455 (0.3260) acc 87.5000 (88.1250) lr 1.7705e-03 eta 0:10:33\n",
            "epoch [13/50] batch [10/18] time 0.151 (0.547) data 0.000 (0.198) loss 0.1985 (0.3575) acc 93.7500 (88.1250) lr 1.7705e-03 eta 0:06:08\n",
            "epoch [13/50] batch [15/18] time 0.152 (0.416) data 0.000 (0.132) loss 0.3728 (0.3899) acc 87.5000 (87.7083) lr 1.7705e-03 eta 0:04:38\n",
            "epoch [14/50] batch [5/18] time 0.169 (0.607) data 0.002 (0.337) loss 0.2159 (0.3764) acc 90.6250 (86.8750) lr 1.7290e-03 eta 0:06:41\n",
            "epoch [14/50] batch [10/18] time 0.159 (0.380) data 0.000 (0.169) loss 0.4565 (0.4056) acc 84.3750 (86.5625) lr 1.7290e-03 eta 0:04:09\n",
            "epoch [14/50] batch [15/18] time 0.152 (0.304) data 0.000 (0.113) loss 0.2588 (0.3676) acc 90.6250 (88.1250) lr 1.7290e-03 eta 0:03:18\n",
            "epoch [15/50] batch [5/18] time 0.155 (0.635) data 0.000 (0.391) loss 0.2649 (0.3863) acc 96.8750 (88.1250) lr 1.6845e-03 eta 0:06:48\n",
            "epoch [15/50] batch [10/18] time 0.166 (0.396) data 0.000 (0.196) loss 0.2808 (0.3203) acc 93.7500 (90.9375) lr 1.6845e-03 eta 0:04:12\n",
            "epoch [15/50] batch [15/18] time 0.153 (0.315) data 0.000 (0.130) loss 0.3494 (0.3647) acc 78.1250 (88.5417) lr 1.6845e-03 eta 0:03:19\n",
            "epoch [16/50] batch [5/18] time 0.156 (0.719) data 0.000 (0.446) loss 0.4099 (0.4180) acc 87.5000 (87.5000) lr 1.6374e-03 eta 0:07:29\n",
            "epoch [16/50] batch [10/18] time 0.153 (0.436) data 0.000 (0.223) loss 0.4304 (0.3971) acc 87.5000 (89.0625) lr 1.6374e-03 eta 0:04:30\n",
            "epoch [16/50] batch [15/18] time 0.154 (0.342) data 0.000 (0.149) loss 0.2329 (0.3674) acc 93.7500 (89.3750) lr 1.6374e-03 eta 0:03:30\n",
            "epoch [17/50] batch [5/18] time 0.269 (0.650) data 0.000 (0.369) loss 0.3562 (0.3190) acc 93.7500 (91.2500) lr 1.5878e-03 eta 0:06:34\n",
            "epoch [17/50] batch [10/18] time 0.154 (0.405) data 0.000 (0.185) loss 0.3579 (0.3661) acc 90.6250 (89.3750) lr 1.5878e-03 eta 0:04:03\n",
            "epoch [17/50] batch [15/18] time 0.153 (0.322) data 0.000 (0.123) loss 0.3555 (0.3334) acc 90.6250 (90.4167) lr 1.5878e-03 eta 0:03:12\n",
            "epoch [18/50] batch [5/18] time 0.239 (1.002) data 0.000 (0.538) loss 0.4810 (0.3236) acc 87.5000 (93.1250) lr 1.5358e-03 eta 0:09:50\n",
            "epoch [18/50] batch [10/18] time 0.155 (0.582) data 0.000 (0.269) loss 0.4692 (0.3567) acc 90.6250 (91.5625) lr 1.5358e-03 eta 0:05:40\n",
            "epoch [18/50] batch [15/18] time 0.155 (0.440) data 0.000 (0.180) loss 0.4050 (0.3340) acc 90.6250 (92.2917) lr 1.5358e-03 eta 0:04:14\n",
            "epoch [19/50] batch [5/18] time 0.163 (0.629) data 0.000 (0.358) loss 0.1465 (0.3046) acc 96.8750 (92.5000) lr 1.4818e-03 eta 0:05:59\n",
            "epoch [19/50] batch [10/18] time 0.161 (0.393) data 0.000 (0.179) loss 0.1177 (0.3298) acc 100.0000 (92.1875) lr 1.4818e-03 eta 0:03:42\n",
            "epoch [19/50] batch [15/18] time 0.156 (0.314) data 0.000 (0.120) loss 0.3809 (0.3287) acc 87.5000 (91.6667) lr 1.4818e-03 eta 0:02:56\n",
            "epoch [20/50] batch [5/18] time 0.181 (0.695) data 0.000 (0.400) loss 0.4016 (0.2860) acc 90.6250 (92.5000) lr 1.4258e-03 eta 0:06:24\n",
            "epoch [20/50] batch [10/18] time 0.155 (0.433) data 0.000 (0.200) loss 0.1974 (0.2771) acc 93.7500 (92.1875) lr 1.4258e-03 eta 0:03:57\n",
            "epoch [20/50] batch [15/18] time 0.154 (0.341) data 0.000 (0.134) loss 0.1934 (0.3047) acc 90.6250 (91.2500) lr 1.4258e-03 eta 0:03:05\n",
            "epoch [21/50] batch [5/18] time 0.254 (0.657) data 0.000 (0.372) loss 0.1603 (0.2462) acc 90.6250 (93.1250) lr 1.3681e-03 eta 0:05:51\n",
            "epoch [21/50] batch [10/18] time 0.153 (0.410) data 0.000 (0.186) loss 0.2313 (0.2884) acc 96.8750 (92.8125) lr 1.3681e-03 eta 0:03:37\n",
            "epoch [21/50] batch [15/18] time 0.154 (0.326) data 0.000 (0.124) loss 0.2720 (0.2916) acc 90.6250 (92.5000) lr 1.3681e-03 eta 0:02:50\n",
            "epoch [22/50] batch [5/18] time 0.161 (0.696) data 0.000 (0.377) loss 0.4587 (0.3139) acc 78.1250 (90.6250) lr 1.3090e-03 eta 0:05:59\n",
            "epoch [22/50] batch [10/18] time 0.163 (0.426) data 0.000 (0.189) loss 0.1412 (0.3707) acc 96.8750 (89.6875) lr 1.3090e-03 eta 0:03:38\n",
            "epoch [22/50] batch [15/18] time 0.155 (0.336) data 0.000 (0.126) loss 0.3975 (0.3518) acc 87.5000 (90.2083) lr 1.3090e-03 eta 0:02:50\n",
            "epoch [23/50] batch [5/18] time 0.166 (0.941) data 0.005 (0.593) loss 0.2729 (0.3578) acc 87.5000 (86.8750) lr 1.2487e-03 eta 0:07:49\n",
            "epoch [23/50] batch [10/18] time 0.164 (0.550) data 0.000 (0.297) loss 0.2859 (0.3390) acc 87.5000 (89.0625) lr 1.2487e-03 eta 0:04:31\n",
            "epoch [23/50] batch [15/18] time 0.153 (0.418) data 0.000 (0.198) loss 0.4890 (0.3217) acc 84.3750 (90.0000) lr 1.2487e-03 eta 0:03:24\n",
            "epoch [24/50] batch [5/18] time 0.168 (0.602) data 0.000 (0.319) loss 0.5566 (0.2992) acc 81.2500 (91.8750) lr 1.1874e-03 eta 0:04:49\n",
            "epoch [24/50] batch [10/18] time 0.163 (0.382) data 0.000 (0.159) loss 0.1820 (0.3215) acc 96.8750 (90.9375) lr 1.1874e-03 eta 0:03:01\n",
            "epoch [24/50] batch [15/18] time 0.154 (0.306) data 0.000 (0.106) loss 0.7144 (0.3487) acc 81.2500 (90.8333) lr 1.1874e-03 eta 0:02:24\n",
            "epoch [25/50] batch [5/18] time 0.208 (0.802) data 0.000 (0.422) loss 0.2683 (0.1937) acc 87.5000 (93.7500) lr 1.1253e-03 eta 0:06:11\n",
            "epoch [25/50] batch [10/18] time 0.154 (0.483) data 0.000 (0.211) loss 0.5889 (0.3012) acc 87.5000 (90.9375) lr 1.1253e-03 eta 0:03:41\n",
            "epoch [25/50] batch [15/18] time 0.155 (0.375) data 0.000 (0.141) loss 0.4131 (0.2908) acc 84.3750 (90.8333) lr 1.1253e-03 eta 0:02:49\n",
            "epoch [26/50] batch [5/18] time 0.258 (0.629) data 0.000 (0.323) loss 0.1917 (0.3179) acc 96.8750 (91.2500) lr 1.0628e-03 eta 0:04:40\n",
            "epoch [26/50] batch [10/18] time 0.154 (0.392) data 0.000 (0.162) loss 0.3354 (0.3400) acc 90.6250 (90.9375) lr 1.0628e-03 eta 0:02:52\n",
            "epoch [26/50] batch [15/18] time 0.154 (0.313) data 0.000 (0.108) loss 0.6958 (0.3422) acc 78.1250 (90.4167) lr 1.0628e-03 eta 0:02:16\n",
            "epoch [27/50] batch [5/18] time 0.421 (0.624) data 0.000 (0.328) loss 0.2891 (0.2903) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:04:26\n",
            "epoch [27/50] batch [10/18] time 0.153 (0.403) data 0.000 (0.164) loss 0.3667 (0.2780) acc 87.5000 (91.2500) lr 1.0000e-03 eta 0:02:50\n",
            "epoch [27/50] batch [15/18] time 0.153 (0.320) data 0.000 (0.109) loss 0.3008 (0.2732) acc 93.7500 (91.2500) lr 1.0000e-03 eta 0:02:13\n",
            "epoch [28/50] batch [5/18] time 0.159 (0.857) data 0.002 (0.605) loss 0.4167 (0.2631) acc 90.6250 (93.7500) lr 9.3721e-04 eta 0:05:50\n",
            "epoch [28/50] batch [10/18] time 0.156 (0.507) data 0.000 (0.303) loss 0.4304 (0.2603) acc 90.6250 (94.0625) lr 9.3721e-04 eta 0:03:24\n",
            "epoch [28/50] batch [15/18] time 0.152 (0.389) data 0.000 (0.202) loss 0.2046 (0.2502) acc 93.7500 (93.5417) lr 9.3721e-04 eta 0:02:35\n",
            "epoch [29/50] batch [5/18] time 0.268 (0.604) data 0.000 (0.299) loss 0.3000 (0.2018) acc 93.7500 (96.2500) lr 8.7467e-04 eta 0:03:56\n",
            "epoch [29/50] batch [10/18] time 0.154 (0.385) data 0.000 (0.150) loss 0.3809 (0.2386) acc 90.6250 (95.0000) lr 8.7467e-04 eta 0:02:28\n",
            "epoch [29/50] batch [15/18] time 0.154 (0.308) data 0.000 (0.100) loss 0.3789 (0.2498) acc 87.5000 (93.3333) lr 8.7467e-04 eta 0:01:57\n",
            "epoch [30/50] batch [5/18] time 0.253 (0.876) data 0.000 (0.426) loss 0.1471 (0.2545) acc 93.7500 (93.1250) lr 8.1262e-04 eta 0:05:26\n",
            "epoch [30/50] batch [10/18] time 0.153 (0.524) data 0.000 (0.213) loss 0.3018 (0.2809) acc 90.6250 (92.5000) lr 8.1262e-04 eta 0:03:12\n",
            "epoch [30/50] batch [15/18] time 0.153 (0.401) data 0.000 (0.142) loss 0.1929 (0.3168) acc 93.7500 (91.4583) lr 8.1262e-04 eta 0:02:25\n",
            "epoch [31/50] batch [5/18] time 0.160 (0.657) data 0.000 (0.386) loss 0.1978 (0.2131) acc 93.7500 (95.6250) lr 7.5131e-04 eta 0:03:53\n",
            "epoch [31/50] batch [10/18] time 0.159 (0.407) data 0.000 (0.193) loss 0.2031 (0.2579) acc 93.7500 (94.0625) lr 7.5131e-04 eta 0:02:22\n",
            "epoch [31/50] batch [15/18] time 0.154 (0.322) data 0.000 (0.129) loss 0.1222 (0.2727) acc 100.0000 (93.1250) lr 7.5131e-04 eta 0:01:51\n",
            "epoch [32/50] batch [5/18] time 0.180 (0.636) data 0.000 (0.363) loss 0.1458 (0.1921) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:03:34\n",
            "epoch [32/50] batch [10/18] time 0.164 (0.396) data 0.001 (0.182) loss 0.3247 (0.2032) acc 90.6250 (96.5625) lr 6.9098e-04 eta 0:02:11\n",
            "epoch [32/50] batch [15/18] time 0.154 (0.316) data 0.000 (0.121) loss 0.1107 (0.1905) acc 100.0000 (96.8750) lr 6.9098e-04 eta 0:01:43\n",
            "epoch [33/50] batch [5/18] time 0.176 (0.812) data 0.007 (0.545) loss 0.1887 (0.1934) acc 93.7500 (94.3750) lr 6.3188e-04 eta 0:04:18\n",
            "epoch [33/50] batch [10/18] time 0.170 (0.485) data 0.000 (0.273) loss 0.4966 (0.2483) acc 90.6250 (93.7500) lr 6.3188e-04 eta 0:02:32\n",
            "epoch [33/50] batch [15/18] time 0.154 (0.375) data 0.000 (0.182) loss 0.3569 (0.2619) acc 90.6250 (93.5417) lr 6.3188e-04 eta 0:01:55\n",
            "epoch [34/50] batch [5/18] time 0.250 (0.614) data 0.000 (0.340) loss 0.3240 (0.3183) acc 93.7500 (90.0000) lr 5.7422e-04 eta 0:03:04\n",
            "epoch [34/50] batch [10/18] time 0.152 (0.385) data 0.000 (0.170) loss 0.2277 (0.2995) acc 96.8750 (90.9375) lr 5.7422e-04 eta 0:01:53\n",
            "epoch [34/50] batch [15/18] time 0.155 (0.309) data 0.000 (0.114) loss 0.3809 (0.2780) acc 90.6250 (91.4583) lr 5.7422e-04 eta 0:01:29\n",
            "epoch [35/50] batch [5/18] time 0.460 (0.896) data 0.001 (0.385) loss 0.4604 (0.2434) acc 87.5000 (94.3750) lr 5.1825e-04 eta 0:04:13\n",
            "epoch [35/50] batch [10/18] time 0.155 (0.537) data 0.000 (0.193) loss 0.2223 (0.2414) acc 96.8750 (93.7500) lr 5.1825e-04 eta 0:02:29\n",
            "epoch [35/50] batch [15/18] time 0.155 (0.410) data 0.000 (0.129) loss 0.4456 (0.2563) acc 87.5000 (92.7083) lr 5.1825e-04 eta 0:01:52\n",
            "epoch [36/50] batch [5/18] time 0.230 (0.611) data 0.002 (0.321) loss 0.2369 (0.2346) acc 90.6250 (93.1250) lr 4.6417e-04 eta 0:02:41\n",
            "epoch [36/50] batch [10/18] time 0.154 (0.384) data 0.000 (0.161) loss 0.3740 (0.2691) acc 93.7500 (93.1250) lr 4.6417e-04 eta 0:01:39\n",
            "epoch [36/50] batch [15/18] time 0.153 (0.308) data 0.000 (0.107) loss 0.2881 (0.2898) acc 90.6250 (91.6667) lr 4.6417e-04 eta 0:01:18\n",
            "epoch [37/50] batch [5/18] time 0.306 (0.546) data 0.005 (0.253) loss 0.5605 (0.2050) acc 84.3750 (95.6250) lr 4.1221e-04 eta 0:02:14\n",
            "epoch [37/50] batch [10/18] time 0.156 (0.376) data 0.001 (0.127) loss 0.1272 (0.2016) acc 96.8750 (94.0625) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [37/50] batch [15/18] time 0.154 (0.303) data 0.000 (0.085) loss 0.2383 (0.2226) acc 93.7500 (93.5417) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [38/50] batch [5/18] time 0.231 (0.860) data 0.006 (0.559) loss 0.2183 (0.2660) acc 90.6250 (93.1250) lr 3.6258e-04 eta 0:03:16\n",
            "epoch [38/50] batch [10/18] time 0.157 (0.507) data 0.000 (0.280) loss 0.2507 (0.2544) acc 90.6250 (92.8125) lr 3.6258e-04 eta 0:01:53\n",
            "epoch [38/50] batch [15/18] time 0.152 (0.390) data 0.000 (0.187) loss 0.2476 (0.2500) acc 93.7500 (93.3333) lr 3.6258e-04 eta 0:01:25\n",
            "epoch [39/50] batch [5/18] time 0.214 (0.602) data 0.008 (0.276) loss 0.4456 (0.2136) acc 87.5000 (94.3750) lr 3.1545e-04 eta 0:02:07\n",
            "epoch [39/50] batch [10/18] time 0.153 (0.382) data 0.000 (0.139) loss 0.3254 (0.2629) acc 87.5000 (92.1875) lr 3.1545e-04 eta 0:01:18\n",
            "epoch [39/50] batch [15/18] time 0.154 (0.306) data 0.000 (0.093) loss 0.1001 (0.2204) acc 96.8750 (93.7500) lr 3.1545e-04 eta 0:01:01\n",
            "epoch [40/50] batch [5/18] time 0.362 (0.947) data 0.000 (0.445) loss 0.2284 (0.2410) acc 93.7500 (92.5000) lr 2.7103e-04 eta 0:03:02\n",
            "epoch [40/50] batch [10/18] time 0.153 (0.559) data 0.000 (0.223) loss 0.0524 (0.2826) acc 100.0000 (92.8125) lr 2.7103e-04 eta 0:01:45\n",
            "epoch [40/50] batch [15/18] time 0.154 (0.425) data 0.000 (0.149) loss 0.2249 (0.2591) acc 93.7500 (93.3333) lr 2.7103e-04 eta 0:01:17\n",
            "epoch [41/50] batch [5/18] time 0.177 (0.628) data 0.001 (0.336) loss 0.3127 (0.2079) acc 90.6250 (94.3750) lr 2.2949e-04 eta 0:01:49\n",
            "epoch [41/50] batch [10/18] time 0.166 (0.393) data 0.000 (0.168) loss 0.0729 (0.2102) acc 100.0000 (94.3750) lr 2.2949e-04 eta 0:01:06\n",
            "epoch [41/50] batch [15/18] time 0.152 (0.313) data 0.000 (0.112) loss 0.3025 (0.2443) acc 93.7500 (93.3333) lr 2.2949e-04 eta 0:00:51\n",
            "epoch [42/50] batch [5/18] time 0.159 (0.625) data 0.000 (0.333) loss 0.0691 (0.2834) acc 100.0000 (90.6250) lr 1.9098e-04 eta 0:01:38\n",
            "epoch [42/50] batch [10/18] time 0.164 (0.395) data 0.000 (0.167) loss 0.3208 (0.2866) acc 87.5000 (90.6250) lr 1.9098e-04 eta 0:00:59\n",
            "epoch [42/50] batch [15/18] time 0.154 (0.315) data 0.000 (0.111) loss 0.2515 (0.2781) acc 93.7500 (91.4583) lr 1.9098e-04 eta 0:00:46\n",
            "epoch [43/50] batch [5/18] time 0.214 (0.801) data 0.000 (0.531) loss 0.0984 (0.2079) acc 100.0000 (94.3750) lr 1.5567e-04 eta 0:01:51\n",
            "epoch [43/50] batch [10/18] time 0.154 (0.479) data 0.000 (0.266) loss 0.3269 (0.2698) acc 87.5000 (93.4375) lr 1.5567e-04 eta 0:01:04\n",
            "epoch [43/50] batch [15/18] time 0.153 (0.371) data 0.000 (0.177) loss 0.3235 (0.2639) acc 90.6250 (93.3333) lr 1.5567e-04 eta 0:00:47\n",
            "epoch [44/50] batch [5/18] time 0.193 (0.629) data 0.000 (0.323) loss 0.3228 (0.2355) acc 87.5000 (93.7500) lr 1.2369e-04 eta 0:01:16\n",
            "epoch [44/50] batch [10/18] time 0.160 (0.394) data 0.000 (0.162) loss 0.1036 (0.2037) acc 100.0000 (95.0000) lr 1.2369e-04 eta 0:00:45\n",
            "epoch [44/50] batch [15/18] time 0.155 (0.315) data 0.000 (0.108) loss 0.2344 (0.2126) acc 93.7500 (94.5833) lr 1.2369e-04 eta 0:00:34\n",
            "epoch [45/50] batch [5/18] time 0.248 (1.025) data 0.000 (0.630) loss 0.1658 (0.2297) acc 96.8750 (95.6250) lr 9.5173e-05 eta 0:01:45\n",
            "epoch [45/50] batch [10/18] time 0.152 (0.591) data 0.000 (0.315) loss 0.5728 (0.2623) acc 87.5000 (94.0625) lr 9.5173e-05 eta 0:00:57\n",
            "epoch [45/50] batch [15/18] time 0.154 (0.446) data 0.000 (0.210) loss 0.2568 (0.2420) acc 87.5000 (93.7500) lr 9.5173e-05 eta 0:00:41\n",
            "epoch [46/50] batch [5/18] time 0.153 (0.646) data 0.000 (0.355) loss 0.5557 (0.3505) acc 87.5000 (90.6250) lr 7.0224e-05 eta 0:00:54\n",
            "epoch [46/50] batch [10/18] time 0.170 (0.402) data 0.000 (0.178) loss 0.2517 (0.2767) acc 93.7500 (92.5000) lr 7.0224e-05 eta 0:00:32\n",
            "epoch [46/50] batch [15/18] time 0.154 (0.319) data 0.000 (0.119) loss 0.2971 (0.3057) acc 93.7500 (91.8750) lr 7.0224e-05 eta 0:00:23\n",
            "epoch [47/50] batch [5/18] time 0.190 (0.715) data 0.002 (0.401) loss 0.1656 (0.2477) acc 93.7500 (94.3750) lr 4.8943e-05 eta 0:00:47\n",
            "epoch [47/50] batch [10/18] time 0.154 (0.443) data 0.000 (0.201) loss 0.2861 (0.2436) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:27\n",
            "epoch [47/50] batch [15/18] time 0.156 (0.348) data 0.000 (0.134) loss 0.1837 (0.2551) acc 93.7500 (92.9167) lr 4.8943e-05 eta 0:00:19\n",
            "epoch [48/50] batch [5/18] time 0.358 (0.702) data 0.011 (0.359) loss 0.1882 (0.1705) acc 93.7500 (94.3750) lr 3.1417e-05 eta 0:00:34\n",
            "epoch [48/50] batch [10/18] time 0.154 (0.434) data 0.000 (0.181) loss 0.2073 (0.2417) acc 93.7500 (92.8125) lr 3.1417e-05 eta 0:00:19\n",
            "epoch [48/50] batch [15/18] time 0.155 (0.341) data 0.000 (0.121) loss 0.3242 (0.2537) acc 93.7500 (93.1250) lr 3.1417e-05 eta 0:00:13\n",
            "epoch [49/50] batch [5/18] time 0.194 (0.602) data 0.000 (0.310) loss 0.2732 (0.1889) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:00:18\n",
            "epoch [49/50] batch [10/18] time 0.156 (0.384) data 0.000 (0.155) loss 0.1752 (0.1892) acc 93.7500 (95.6250) lr 1.7713e-05 eta 0:00:09\n",
            "epoch [49/50] batch [15/18] time 0.155 (0.308) data 0.000 (0.104) loss 0.1979 (0.2117) acc 93.7500 (95.0000) lr 1.7713e-05 eta 0:00:06\n",
            "epoch [50/50] batch [5/18] time 0.157 (0.978) data 0.000 (0.636) loss 0.3210 (0.2626) acc 93.7500 (93.7500) lr 7.8853e-06 eta 0:00:12\n",
            "epoch [50/50] batch [10/18] time 0.155 (0.566) data 0.000 (0.318) loss 0.3750 (0.2343) acc 90.6250 (94.6875) lr 7.8853e-06 eta 0:00:04\n",
            "epoch [50/50] batch [15/18] time 0.152 (0.429) data 0.000 (0.212) loss 0.2871 (0.2818) acc 93.7500 (93.1250) lr 7.8853e-06 eta 0:00:01\n",
            "Checkpoint saved to output/oxford_pets_custom_csc/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:28<00:00,  1.31it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,306\n",
            "* accuracy: 90.1%\n",
            "* error: 9.9%\n",
            "* macro_f1: 90.0%\n",
            "Elapsed: 0:05:25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Train with lower learning rate (0.001 instead of 0.002)\n",
        "# Expected: Slower convergence but potentially better final accuracy\n",
        "!python train.py \\\n",
        "--root /content/CoOp/data \\\n",
        "--seed 1 \\\n",
        "--trainer CoOp \\\n",
        "--dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "--config-file configs/trainers/CoOp/vit_b16_ep50.yaml \\\n",
        "--output-dir output/oxford_pets_custom_lr \\\n",
        "DATASET.NUM_SHOTS 16 \\\n",
        "OPTIM.LR 0.001"
      ],
      "metadata": {
        "id": "GF31kfnE8WML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b521aecc-5dcf-4176-e3ba-c329a657d361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 19:49:34.401970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763236174.423494   16516 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763236174.430170   16516 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763236174.446099   16516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236174.446125   16516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236174.446131   16516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236174.446135   16516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:49:34.450901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'OPTIM.LR', '0.001']\n",
            "output_dir: output/oxford_pets_custom_lr\n",
            "resume: \n",
            "root: /content/CoOp/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/CoOp/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.001\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets_custom_lr\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.6\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.41\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.0.2\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.3\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.13\n",
            "[pip3] optree==0.17.0\n",
            "[pip3] torch==2.8.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.8.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.23.0+cu126\n",
            "[pip3] triton==3.4.0\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/CoOp/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/CoOp/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets_custom_lr/tensorboard)\n",
            "epoch [1/50] batch [5/18] time 0.154 (0.968) data 0.000 (0.413) loss 2.0918 (2.4035) acc 53.1250 (43.1250) lr 1.0000e-05 eta 0:14:26\n",
            "epoch [1/50] batch [10/18] time 0.155 (0.561) data 0.000 (0.207) loss 2.3848 (2.4148) acc 40.6250 (43.1250) lr 1.0000e-05 eta 0:08:19\n",
            "epoch [1/50] batch [15/18] time 0.153 (0.425) data 0.000 (0.138) loss 1.1406 (2.0287) acc 65.6250 (50.4167) lr 1.0000e-05 eta 0:06:16\n",
            "epoch [2/50] batch [5/18] time 0.162 (0.691) data 0.001 (0.441) loss 0.6650 (0.9504) acc 84.3750 (73.7500) lr 1.0000e-03 eta 0:10:05\n",
            "epoch [2/50] batch [10/18] time 0.157 (0.424) data 0.000 (0.221) loss 0.7563 (0.8582) acc 65.6250 (73.7500) lr 1.0000e-03 eta 0:06:09\n",
            "epoch [2/50] batch [15/18] time 0.155 (0.334) data 0.000 (0.147) loss 0.7427 (0.8098) acc 75.0000 (74.5833) lr 1.0000e-03 eta 0:04:49\n",
            "epoch [3/50] batch [5/18] time 0.170 (0.677) data 0.002 (0.395) loss 0.6318 (0.7631) acc 78.1250 (76.2500) lr 9.9901e-04 eta 0:09:41\n",
            "epoch [3/50] batch [10/18] time 0.174 (0.418) data 0.000 (0.198) loss 0.5015 (0.6995) acc 78.1250 (79.0625) lr 9.9901e-04 eta 0:05:57\n",
            "epoch [3/50] batch [15/18] time 0.158 (0.331) data 0.000 (0.132) loss 0.9019 (0.7136) acc 78.1250 (79.7917) lr 9.9901e-04 eta 0:04:41\n",
            "epoch [4/50] batch [5/18] time 0.170 (0.927) data 0.002 (0.594) loss 0.4810 (0.6488) acc 93.7500 (85.0000) lr 9.9606e-04 eta 0:12:59\n",
            "epoch [4/50] batch [10/18] time 0.173 (0.544) data 0.000 (0.297) loss 0.4763 (0.6524) acc 90.6250 (82.5000) lr 9.9606e-04 eta 0:07:34\n",
            "epoch [4/50] batch [15/18] time 0.157 (0.415) data 0.000 (0.198) loss 0.8164 (0.6462) acc 78.1250 (81.4583) lr 9.9606e-04 eta 0:05:44\n",
            "epoch [5/50] batch [5/18] time 0.160 (0.621) data 0.000 (0.391) loss 0.3303 (0.6603) acc 87.5000 (81.8750) lr 9.9114e-04 eta 0:08:30\n",
            "epoch [5/50] batch [10/18] time 0.156 (0.390) data 0.000 (0.195) loss 0.5518 (0.6618) acc 81.2500 (80.9375) lr 9.9114e-04 eta 0:05:18\n",
            "epoch [5/50] batch [15/18] time 0.157 (0.312) data 0.000 (0.130) loss 0.4836 (0.6292) acc 87.5000 (81.6667) lr 9.9114e-04 eta 0:04:13\n",
            "epoch [6/50] batch [5/18] time 0.456 (0.662) data 0.000 (0.255) loss 0.8291 (0.7233) acc 68.7500 (75.0000) lr 9.8429e-04 eta 0:08:52\n",
            "epoch [6/50] batch [10/18] time 0.156 (0.430) data 0.001 (0.128) loss 0.6289 (0.6422) acc 78.1250 (78.1250) lr 9.8429e-04 eta 0:05:43\n",
            "epoch [6/50] batch [15/18] time 0.158 (0.339) data 0.000 (0.085) loss 0.7300 (0.6587) acc 68.7500 (78.1250) lr 9.8429e-04 eta 0:04:29\n",
            "epoch [7/50] batch [5/18] time 0.160 (0.640) data 0.000 (0.394) loss 0.3792 (0.5960) acc 90.6250 (80.0000) lr 9.7553e-04 eta 0:08:23\n",
            "epoch [7/50] batch [10/18] time 0.158 (0.399) data 0.000 (0.197) loss 0.8799 (0.6127) acc 68.7500 (80.6250) lr 9.7553e-04 eta 0:05:11\n",
            "epoch [7/50] batch [15/18] time 0.154 (0.317) data 0.000 (0.131) loss 0.7007 (0.6137) acc 78.1250 (80.8333) lr 9.7553e-04 eta 0:04:06\n",
            "epoch [8/50] batch [5/18] time 0.155 (0.685) data 0.000 (0.381) loss 0.3601 (0.6547) acc 90.6250 (78.7500) lr 9.6489e-04 eta 0:08:46\n",
            "epoch [8/50] batch [10/18] time 0.158 (0.420) data 0.000 (0.191) loss 0.6548 (0.6478) acc 84.3750 (80.0000) lr 9.6489e-04 eta 0:05:20\n",
            "epoch [8/50] batch [15/18] time 0.152 (0.331) data 0.000 (0.127) loss 0.5142 (0.6068) acc 78.1250 (81.0417) lr 9.6489e-04 eta 0:04:11\n",
            "epoch [9/50] batch [5/18] time 0.152 (0.930) data 0.000 (0.676) loss 0.3403 (0.5640) acc 84.3750 (80.6250) lr 9.5241e-04 eta 0:11:38\n",
            "epoch [9/50] batch [10/18] time 0.153 (0.541) data 0.000 (0.338) loss 0.8311 (0.6640) acc 78.1250 (78.4375) lr 9.5241e-04 eta 0:06:43\n",
            "epoch [9/50] batch [15/18] time 0.151 (0.411) data 0.000 (0.225) loss 0.7920 (0.6235) acc 75.0000 (79.5833) lr 9.5241e-04 eta 0:05:04\n",
            "epoch [10/50] batch [5/18] time 0.225 (0.588) data 0.006 (0.304) loss 0.4075 (0.6516) acc 87.5000 (81.8750) lr 9.3815e-04 eta 0:07:10\n",
            "epoch [10/50] batch [10/18] time 0.153 (0.373) data 0.000 (0.152) loss 0.6338 (0.6082) acc 81.2500 (82.8125) lr 9.3815e-04 eta 0:04:31\n",
            "epoch [10/50] batch [15/18] time 0.153 (0.300) data 0.000 (0.102) loss 0.7183 (0.5974) acc 78.1250 (81.8750) lr 9.3815e-04 eta 0:03:37\n",
            "epoch [11/50] batch [5/18] time 0.185 (0.789) data 0.003 (0.396) loss 0.6528 (0.5278) acc 81.2500 (84.3750) lr 9.2216e-04 eta 0:09:23\n",
            "epoch [11/50] batch [10/18] time 0.152 (0.475) data 0.000 (0.198) loss 0.4626 (0.5198) acc 90.6250 (84.6875) lr 9.2216e-04 eta 0:05:37\n",
            "epoch [11/50] batch [15/18] time 0.154 (0.368) data 0.000 (0.132) loss 0.7144 (0.5699) acc 75.0000 (82.0833) lr 9.2216e-04 eta 0:04:19\n",
            "epoch [12/50] batch [5/18] time 0.154 (0.602) data 0.002 (0.376) loss 0.4099 (0.4928) acc 84.3750 (83.7500) lr 9.0451e-04 eta 0:06:59\n",
            "epoch [12/50] batch [10/18] time 0.158 (0.379) data 0.000 (0.189) loss 0.5732 (0.4980) acc 81.2500 (83.1250) lr 9.0451e-04 eta 0:04:22\n",
            "epoch [12/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.126) loss 0.3765 (0.4876) acc 84.3750 (83.9583) lr 9.0451e-04 eta 0:03:28\n",
            "epoch [13/50] batch [5/18] time 0.324 (0.571) data 0.000 (0.256) loss 0.5835 (0.5325) acc 81.2500 (82.5000) lr 8.8526e-04 eta 0:06:28\n",
            "epoch [13/50] batch [10/18] time 0.151 (0.367) data 0.000 (0.128) loss 0.5898 (0.5628) acc 84.3750 (83.1250) lr 8.8526e-04 eta 0:04:07\n",
            "epoch [13/50] batch [15/18] time 0.151 (0.296) data 0.000 (0.086) loss 0.2832 (0.5203) acc 96.8750 (83.9583) lr 8.8526e-04 eta 0:03:18\n",
            "epoch [14/50] batch [5/18] time 0.202 (0.902) data 0.002 (0.523) loss 0.4797 (0.5547) acc 87.5000 (83.1250) lr 8.6448e-04 eta 0:09:55\n",
            "epoch [14/50] batch [10/18] time 0.157 (0.528) data 0.000 (0.261) loss 0.3591 (0.5461) acc 87.5000 (83.4375) lr 8.6448e-04 eta 0:05:46\n",
            "epoch [14/50] batch [15/18] time 0.152 (0.403) data 0.000 (0.174) loss 0.6353 (0.5239) acc 78.1250 (83.9583) lr 8.6448e-04 eta 0:04:22\n",
            "epoch [15/50] batch [5/18] time 0.191 (0.591) data 0.000 (0.332) loss 0.3149 (0.6526) acc 96.8750 (81.8750) lr 8.4227e-04 eta 0:06:20\n",
            "epoch [15/50] batch [10/18] time 0.161 (0.373) data 0.000 (0.166) loss 0.5244 (0.5642) acc 81.2500 (84.0625) lr 8.4227e-04 eta 0:03:58\n",
            "epoch [15/50] batch [15/18] time 0.153 (0.300) data 0.000 (0.111) loss 0.5078 (0.5569) acc 84.3750 (83.9583) lr 8.4227e-04 eta 0:03:09\n",
            "epoch [16/50] batch [5/18] time 0.353 (0.719) data 0.000 (0.313) loss 1.0518 (0.6844) acc 75.0000 (78.7500) lr 8.1871e-04 eta 0:07:29\n",
            "epoch [16/50] batch [10/18] time 0.155 (0.445) data 0.000 (0.157) loss 0.5366 (0.5465) acc 78.1250 (83.4375) lr 8.1871e-04 eta 0:04:35\n",
            "epoch [16/50] batch [15/18] time 0.155 (0.349) data 0.000 (0.105) loss 0.4045 (0.5151) acc 84.3750 (83.9583) lr 8.1871e-04 eta 0:03:34\n",
            "epoch [17/50] batch [5/18] time 0.165 (0.601) data 0.000 (0.327) loss 0.2408 (0.4822) acc 93.7500 (85.6250) lr 7.9389e-04 eta 0:06:04\n",
            "epoch [17/50] batch [10/18] time 0.165 (0.379) data 0.000 (0.164) loss 0.8423 (0.5029) acc 75.0000 (84.6875) lr 7.9389e-04 eta 0:03:48\n",
            "epoch [17/50] batch [15/18] time 0.154 (0.304) data 0.000 (0.109) loss 0.7881 (0.5247) acc 78.1250 (84.3750) lr 7.9389e-04 eta 0:03:01\n",
            "epoch [18/50] batch [5/18] time 0.166 (0.601) data 0.000 (0.298) loss 0.8032 (0.6116) acc 78.1250 (81.8750) lr 7.6791e-04 eta 0:05:54\n",
            "epoch [18/50] batch [10/18] time 0.159 (0.378) data 0.000 (0.149) loss 0.3145 (0.5757) acc 87.5000 (82.5000) lr 7.6791e-04 eta 0:03:41\n",
            "epoch [18/50] batch [15/18] time 0.154 (0.304) data 0.000 (0.099) loss 0.7017 (0.5731) acc 87.5000 (82.9167) lr 7.6791e-04 eta 0:02:55\n",
            "epoch [19/50] batch [5/18] time 0.160 (0.995) data 0.000 (0.672) loss 0.3755 (0.3684) acc 93.7500 (89.3750) lr 7.4088e-04 eta 0:09:28\n",
            "epoch [19/50] batch [10/18] time 0.158 (0.575) data 0.000 (0.336) loss 0.5928 (0.4983) acc 75.0000 (84.0625) lr 7.4088e-04 eta 0:05:25\n",
            "epoch [19/50] batch [15/18] time 0.154 (0.434) data 0.000 (0.224) loss 0.7314 (0.4945) acc 68.7500 (83.9583) lr 7.4088e-04 eta 0:04:03\n",
            "epoch [20/50] batch [5/18] time 0.282 (0.593) data 0.000 (0.295) loss 0.1434 (0.3625) acc 96.8750 (90.6250) lr 7.1289e-04 eta 0:05:27\n",
            "epoch [20/50] batch [10/18] time 0.154 (0.374) data 0.000 (0.147) loss 0.7456 (0.4846) acc 75.0000 (85.6250) lr 7.1289e-04 eta 0:03:24\n",
            "epoch [20/50] batch [15/18] time 0.154 (0.301) data 0.000 (0.098) loss 0.4863 (0.4947) acc 84.3750 (84.3750) lr 7.1289e-04 eta 0:02:43\n",
            "epoch [21/50] batch [5/18] time 0.370 (0.708) data 0.000 (0.276) loss 0.3103 (0.4198) acc 90.6250 (85.6250) lr 6.8406e-04 eta 0:06:18\n",
            "epoch [21/50] batch [10/18] time 0.152 (0.447) data 0.000 (0.138) loss 0.7710 (0.4926) acc 81.2500 (85.3125) lr 6.8406e-04 eta 0:03:56\n",
            "epoch [21/50] batch [15/18] time 0.155 (0.350) data 0.000 (0.092) loss 0.3979 (0.5165) acc 81.2500 (83.1250) lr 6.8406e-04 eta 0:03:03\n",
            "epoch [22/50] batch [5/18] time 0.166 (0.617) data 0.002 (0.334) loss 0.3347 (0.3632) acc 87.5000 (89.3750) lr 6.5451e-04 eta 0:05:18\n",
            "epoch [22/50] batch [10/18] time 0.162 (0.387) data 0.000 (0.167) loss 0.2815 (0.4139) acc 93.7500 (87.8125) lr 6.5451e-04 eta 0:03:17\n",
            "epoch [22/50] batch [15/18] time 0.153 (0.309) data 0.000 (0.112) loss 0.4858 (0.4606) acc 81.2500 (85.4167) lr 6.5451e-04 eta 0:02:36\n",
            "epoch [23/50] batch [5/18] time 0.234 (0.571) data 0.000 (0.312) loss 0.6396 (0.4653) acc 75.0000 (86.2500) lr 6.2434e-04 eta 0:04:45\n",
            "epoch [23/50] batch [10/18] time 0.154 (0.363) data 0.000 (0.156) loss 0.4651 (0.4945) acc 81.2500 (84.3750) lr 6.2434e-04 eta 0:02:59\n",
            "epoch [23/50] batch [15/18] time 0.153 (0.294) data 0.000 (0.104) loss 0.4866 (0.5022) acc 81.2500 (83.9583) lr 6.2434e-04 eta 0:02:23\n",
            "epoch [24/50] batch [5/18] time 0.161 (0.975) data 0.003 (0.589) loss 0.5244 (0.4674) acc 81.2500 (85.0000) lr 5.9369e-04 eta 0:07:49\n",
            "epoch [24/50] batch [10/18] time 0.154 (0.564) data 0.000 (0.295) loss 0.4265 (0.4645) acc 87.5000 (84.6875) lr 5.9369e-04 eta 0:04:28\n",
            "epoch [24/50] batch [15/18] time 0.151 (0.427) data 0.000 (0.197) loss 0.4731 (0.4361) acc 87.5000 (86.0417) lr 5.9369e-04 eta 0:03:21\n",
            "epoch [25/50] batch [5/18] time 0.223 (0.579) data 0.002 (0.308) loss 0.3020 (0.4587) acc 90.6250 (88.1250) lr 5.6267e-04 eta 0:04:27\n",
            "epoch [25/50] batch [10/18] time 0.153 (0.367) data 0.000 (0.154) loss 0.3521 (0.5210) acc 90.6250 (85.0000) lr 5.6267e-04 eta 0:02:48\n",
            "epoch [25/50] batch [15/18] time 0.154 (0.296) data 0.000 (0.103) loss 0.5581 (0.5723) acc 87.5000 (84.1667) lr 5.6267e-04 eta 0:02:14\n",
            "epoch [26/50] batch [5/18] time 0.176 (0.760) data 0.000 (0.390) loss 0.5073 (0.5282) acc 78.1250 (83.7500) lr 5.3140e-04 eta 0:05:38\n",
            "epoch [26/50] batch [10/18] time 0.152 (0.462) data 0.000 (0.195) loss 0.6143 (0.5686) acc 81.2500 (82.5000) lr 5.3140e-04 eta 0:03:23\n",
            "epoch [26/50] batch [15/18] time 0.154 (0.360) data 0.000 (0.130) loss 0.5264 (0.5689) acc 84.3750 (82.9167) lr 5.3140e-04 eta 0:02:36\n",
            "epoch [27/50] batch [5/18] time 0.164 (0.602) data 0.000 (0.349) loss 0.3403 (0.4533) acc 93.7500 (88.7500) lr 5.0000e-04 eta 0:04:16\n",
            "epoch [27/50] batch [10/18] time 0.160 (0.378) data 0.000 (0.175) loss 0.2461 (0.4582) acc 96.8750 (87.5000) lr 5.0000e-04 eta 0:02:39\n",
            "epoch [27/50] batch [15/18] time 0.153 (0.303) data 0.000 (0.117) loss 0.3809 (0.4478) acc 81.2500 (86.2500) lr 5.0000e-04 eta 0:02:06\n",
            "epoch [28/50] batch [5/18] time 0.225 (0.576) data 0.000 (0.328) loss 0.3752 (0.4888) acc 84.3750 (85.0000) lr 4.6860e-04 eta 0:03:55\n",
            "epoch [28/50] batch [10/18] time 0.153 (0.366) data 0.000 (0.165) loss 0.4373 (0.4575) acc 90.6250 (86.2500) lr 4.6860e-04 eta 0:02:28\n",
            "epoch [28/50] batch [15/18] time 0.154 (0.296) data 0.000 (0.110) loss 0.3584 (0.4624) acc 84.3750 (85.8333) lr 4.6860e-04 eta 0:01:58\n",
            "epoch [29/50] batch [5/18] time 0.161 (0.913) data 0.002 (0.590) loss 0.4268 (0.5432) acc 84.3750 (83.7500) lr 4.3733e-04 eta 0:05:56\n",
            "epoch [29/50] batch [10/18] time 0.154 (0.532) data 0.000 (0.295) loss 0.5967 (0.5235) acc 78.1250 (82.1875) lr 4.3733e-04 eta 0:03:25\n",
            "epoch [29/50] batch [15/18] time 0.155 (0.406) data 0.000 (0.197) loss 0.4722 (0.5261) acc 90.6250 (83.1250) lr 4.3733e-04 eta 0:02:34\n",
            "epoch [30/50] batch [5/18] time 0.159 (0.618) data 0.000 (0.362) loss 0.7964 (0.5382) acc 71.8750 (83.1250) lr 4.0631e-04 eta 0:03:50\n",
            "epoch [30/50] batch [10/18] time 0.158 (0.387) data 0.000 (0.181) loss 0.6147 (0.5092) acc 84.3750 (83.4375) lr 4.0631e-04 eta 0:02:22\n",
            "epoch [30/50] batch [15/18] time 0.155 (0.309) data 0.000 (0.121) loss 0.5742 (0.5004) acc 84.3750 (83.7500) lr 4.0631e-04 eta 0:01:52\n",
            "epoch [31/50] batch [5/18] time 0.455 (0.687) data 0.000 (0.266) loss 0.2708 (0.3986) acc 87.5000 (85.0000) lr 3.7566e-04 eta 0:04:03\n",
            "epoch [31/50] batch [10/18] time 0.154 (0.448) data 0.000 (0.133) loss 0.2423 (0.4409) acc 90.6250 (85.0000) lr 3.7566e-04 eta 0:02:36\n",
            "epoch [31/50] batch [15/18] time 0.152 (0.351) data 0.000 (0.089) loss 0.3145 (0.4204) acc 90.6250 (85.8333) lr 3.7566e-04 eta 0:02:00\n",
            "epoch [32/50] batch [5/18] time 0.186 (0.576) data 0.000 (0.332) loss 0.5513 (0.4662) acc 78.1250 (85.6250) lr 3.4549e-04 eta 0:03:14\n",
            "epoch [32/50] batch [10/18] time 0.162 (0.367) data 0.000 (0.166) loss 0.7603 (0.5109) acc 78.1250 (83.1250) lr 3.4549e-04 eta 0:02:01\n",
            "epoch [32/50] batch [15/18] time 0.154 (0.296) data 0.000 (0.111) loss 0.7163 (0.4836) acc 71.8750 (83.7500) lr 3.4549e-04 eta 0:01:36\n",
            "epoch [33/50] batch [5/18] time 0.158 (0.626) data 0.000 (0.367) loss 0.6899 (0.4168) acc 81.2500 (88.7500) lr 3.1594e-04 eta 0:03:19\n",
            "epoch [33/50] batch [10/18] time 0.161 (0.391) data 0.000 (0.184) loss 0.3652 (0.4727) acc 90.6250 (87.1875) lr 3.1594e-04 eta 0:02:02\n",
            "epoch [33/50] batch [15/18] time 0.151 (0.312) data 0.000 (0.122) loss 0.5869 (0.4693) acc 84.3750 (87.2917) lr 3.1594e-04 eta 0:01:36\n",
            "epoch [34/50] batch [5/18] time 0.167 (0.902) data 0.000 (0.522) loss 0.7593 (0.5390) acc 71.8750 (85.0000) lr 2.8711e-04 eta 0:04:31\n",
            "epoch [34/50] batch [10/18] time 0.153 (0.528) data 0.000 (0.261) loss 0.3174 (0.4525) acc 90.6250 (86.8750) lr 2.8711e-04 eta 0:02:36\n",
            "epoch [34/50] batch [15/18] time 0.152 (0.403) data 0.000 (0.174) loss 0.6436 (0.4704) acc 78.1250 (86.2500) lr 2.8711e-04 eta 0:01:57\n",
            "epoch [35/50] batch [5/18] time 0.244 (0.630) data 0.005 (0.366) loss 0.3760 (0.4707) acc 84.3750 (83.1250) lr 2.5912e-04 eta 0:02:58\n",
            "epoch [35/50] batch [10/18] time 0.153 (0.392) data 0.000 (0.183) loss 0.3772 (0.4644) acc 87.5000 (83.4375) lr 2.5912e-04 eta 0:01:48\n",
            "epoch [35/50] batch [15/18] time 0.153 (0.313) data 0.000 (0.122) loss 0.6782 (0.4854) acc 81.2500 (83.3333) lr 2.5912e-04 eta 0:01:25\n",
            "epoch [36/50] batch [5/18] time 0.261 (0.715) data 0.000 (0.291) loss 0.7510 (0.6146) acc 75.0000 (80.6250) lr 2.3209e-04 eta 0:03:09\n",
            "epoch [36/50] batch [10/18] time 0.153 (0.438) data 0.000 (0.146) loss 0.4197 (0.5279) acc 90.6250 (83.7500) lr 2.3209e-04 eta 0:01:53\n",
            "epoch [36/50] batch [15/18] time 0.155 (0.344) data 0.000 (0.097) loss 0.5054 (0.5341) acc 96.8750 (84.3750) lr 2.3209e-04 eta 0:01:27\n",
            "epoch [37/50] batch [5/18] time 0.165 (0.631) data 0.001 (0.392) loss 0.5410 (0.5403) acc 81.2500 (82.5000) lr 2.0611e-04 eta 0:02:35\n",
            "epoch [37/50] batch [10/18] time 0.157 (0.395) data 0.000 (0.196) loss 0.7881 (0.5444) acc 75.0000 (83.4375) lr 2.0611e-04 eta 0:01:35\n",
            "epoch [37/50] batch [15/18] time 0.153 (0.314) data 0.000 (0.131) loss 0.4482 (0.5305) acc 87.5000 (83.9583) lr 2.0611e-04 eta 0:01:14\n",
            "epoch [38/50] batch [5/18] time 0.237 (0.577) data 0.000 (0.301) loss 0.5996 (0.4480) acc 78.1250 (85.6250) lr 1.8129e-04 eta 0:02:12\n",
            "epoch [38/50] batch [10/18] time 0.153 (0.367) data 0.000 (0.151) loss 0.4165 (0.4781) acc 87.5000 (85.0000) lr 1.8129e-04 eta 0:01:22\n",
            "epoch [38/50] batch [15/18] time 0.153 (0.297) data 0.000 (0.101) loss 0.4531 (0.4930) acc 84.3750 (84.3750) lr 1.8129e-04 eta 0:01:04\n",
            "epoch [39/50] batch [5/18] time 0.208 (0.965) data 0.002 (0.582) loss 0.5854 (0.4727) acc 81.2500 (86.8750) lr 1.5773e-04 eta 0:03:23\n",
            "epoch [39/50] batch [10/18] time 0.155 (0.560) data 0.000 (0.291) loss 0.4167 (0.5062) acc 93.7500 (86.5625) lr 1.5773e-04 eta 0:01:55\n",
            "epoch [39/50] batch [15/18] time 0.152 (0.425) data 0.000 (0.194) loss 0.7036 (0.5305) acc 81.2500 (85.6250) lr 1.5773e-04 eta 0:01:25\n",
            "epoch [40/50] batch [5/18] time 0.240 (0.566) data 0.000 (0.254) loss 0.4995 (0.6057) acc 87.5000 (83.7500) lr 1.3552e-04 eta 0:01:49\n",
            "epoch [40/50] batch [10/18] time 0.152 (0.366) data 0.000 (0.127) loss 0.4500 (0.5772) acc 90.6250 (82.8125) lr 1.3552e-04 eta 0:01:08\n",
            "epoch [40/50] batch [15/18] time 0.152 (0.296) data 0.000 (0.085) loss 0.4136 (0.5301) acc 84.3750 (84.3750) lr 1.3552e-04 eta 0:00:54\n",
            "epoch [41/50] batch [5/18] time 0.250 (0.561) data 0.000 (0.262) loss 0.3359 (0.4059) acc 87.5000 (86.8750) lr 1.1474e-04 eta 0:01:38\n",
            "epoch [41/50] batch [10/18] time 0.153 (0.368) data 0.000 (0.131) loss 0.5864 (0.4361) acc 84.3750 (85.3125) lr 1.1474e-04 eta 0:01:02\n",
            "epoch [41/50] batch [15/18] time 0.155 (0.298) data 0.000 (0.087) loss 0.1857 (0.4261) acc 96.8750 (85.8333) lr 1.1474e-04 eta 0:00:49\n",
            "epoch [42/50] batch [5/18] time 0.252 (0.727) data 0.000 (0.432) loss 0.3240 (0.4694) acc 87.5000 (85.6250) lr 9.5492e-05 eta 0:01:54\n",
            "epoch [42/50] batch [10/18] time 0.153 (0.440) data 0.000 (0.216) loss 0.7114 (0.5316) acc 71.8750 (83.4375) lr 9.5492e-05 eta 0:01:06\n",
            "epoch [42/50] batch [15/18] time 0.153 (0.345) data 0.000 (0.144) loss 0.5610 (0.4963) acc 87.5000 (85.4167) lr 9.5492e-05 eta 0:00:50\n",
            "epoch [43/50] batch [5/18] time 0.263 (0.611) data 0.000 (0.313) loss 0.2866 (0.4845) acc 90.6250 (85.0000) lr 7.7836e-05 eta 0:01:24\n",
            "epoch [43/50] batch [10/18] time 0.153 (0.383) data 0.000 (0.157) loss 0.3066 (0.4523) acc 87.5000 (83.7500) lr 7.7836e-05 eta 0:00:51\n",
            "epoch [43/50] batch [15/18] time 0.153 (0.307) data 0.000 (0.105) loss 0.3650 (0.4335) acc 90.6250 (84.7917) lr 7.7836e-05 eta 0:00:39\n",
            "epoch [44/50] batch [5/18] time 0.358 (0.846) data 0.000 (0.358) loss 0.7632 (0.5304) acc 71.8750 (81.8750) lr 6.1847e-05 eta 0:01:42\n",
            "epoch [44/50] batch [10/18] time 0.154 (0.513) data 0.000 (0.179) loss 0.4937 (0.5081) acc 81.2500 (82.1875) lr 6.1847e-05 eta 0:00:59\n",
            "epoch [44/50] batch [15/18] time 0.155 (0.394) data 0.000 (0.120) loss 0.4436 (0.4909) acc 87.5000 (83.5417) lr 6.1847e-05 eta 0:00:43\n",
            "epoch [45/50] batch [5/18] time 0.291 (0.553) data 0.000 (0.292) loss 0.3674 (0.5085) acc 90.6250 (86.8750) lr 4.7586e-05 eta 0:00:56\n",
            "epoch [45/50] batch [10/18] time 0.152 (0.365) data 0.000 (0.146) loss 0.4470 (0.4471) acc 81.2500 (87.8125) lr 4.7586e-05 eta 0:00:35\n",
            "epoch [45/50] batch [15/18] time 0.151 (0.295) data 0.000 (0.098) loss 0.3721 (0.4374) acc 87.5000 (88.1250) lr 4.7586e-05 eta 0:00:27\n",
            "epoch [46/50] batch [5/18] time 0.272 (0.644) data 0.004 (0.334) loss 0.5288 (0.5009) acc 78.1250 (83.7500) lr 3.5112e-05 eta 0:00:54\n",
            "epoch [46/50] batch [10/18] time 0.153 (0.400) data 0.000 (0.168) loss 0.3306 (0.4379) acc 90.6250 (87.5000) lr 3.5112e-05 eta 0:00:31\n",
            "epoch [46/50] batch [15/18] time 0.154 (0.318) data 0.000 (0.112) loss 0.2825 (0.4314) acc 90.6250 (87.5000) lr 3.5112e-05 eta 0:00:23\n",
            "epoch [47/50] batch [5/18] time 0.156 (0.766) data 0.000 (0.519) loss 0.4712 (0.5687) acc 78.1250 (82.5000) lr 2.4472e-05 eta 0:00:51\n",
            "epoch [47/50] batch [10/18] time 0.152 (0.460) data 0.000 (0.259) loss 0.4697 (0.5036) acc 84.3750 (84.3750) lr 2.4472e-05 eta 0:00:28\n",
            "epoch [47/50] batch [15/18] time 0.154 (0.358) data 0.000 (0.173) loss 0.3411 (0.4967) acc 87.5000 (84.1667) lr 2.4472e-05 eta 0:00:20\n",
            "epoch [48/50] batch [5/18] time 0.161 (0.604) data 0.004 (0.382) loss 0.3813 (0.5312) acc 87.5000 (84.3750) lr 1.5708e-05 eta 0:00:29\n",
            "epoch [48/50] batch [10/18] time 0.152 (0.380) data 0.000 (0.191) loss 0.4878 (0.4882) acc 84.3750 (84.6875) lr 1.5708e-05 eta 0:00:16\n",
            "epoch [48/50] batch [15/18] time 0.153 (0.304) data 0.000 (0.127) loss 0.5034 (0.4789) acc 87.5000 (85.8333) lr 1.5708e-05 eta 0:00:11\n",
            "epoch [49/50] batch [5/18] time 0.408 (0.814) data 0.000 (0.341) loss 0.2949 (0.4891) acc 93.7500 (88.7500) lr 8.8564e-06 eta 0:00:25\n",
            "epoch [49/50] batch [10/18] time 0.153 (0.499) data 0.000 (0.171) loss 1.0410 (0.5649) acc 71.8750 (84.0625) lr 8.8564e-06 eta 0:00:12\n",
            "epoch [49/50] batch [15/18] time 0.154 (0.385) data 0.000 (0.114) loss 0.3477 (0.5588) acc 90.6250 (83.1250) lr 8.8564e-06 eta 0:00:08\n",
            "epoch [50/50] batch [5/18] time 0.161 (0.619) data 0.004 (0.365) loss 0.4622 (0.4773) acc 87.5000 (86.8750) lr 3.9426e-06 eta 0:00:08\n",
            "epoch [50/50] batch [10/18] time 0.161 (0.387) data 0.000 (0.183) loss 0.3391 (0.4620) acc 90.6250 (87.5000) lr 3.9426e-06 eta 0:00:03\n",
            "epoch [50/50] batch [15/18] time 0.153 (0.309) data 0.000 (0.122) loss 0.6357 (0.4759) acc 81.2500 (86.2500) lr 3.9426e-06 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets_custom_lr/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:28<00:00,  1.31it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,415\n",
            "* accuracy: 93.1%\n",
            "* error: 6.9%\n",
            "* macro_f1: 93.0%\n",
            "Elapsed: 0:05:15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare results of custom experiments\n",
        "!echo \"═══════════════════════════════════════════════════\"\n",
        "!echo \"       CUSTOM EXPERIMENTS COMPARISON\"\n",
        "!echo \"═══════════════════════════════════════════════════\"\n",
        "\n",
        "!echo \"\\n🔧 Baseline (16-shot, N_CTX=16, CSC=False, LR=0.002):\"\n",
        "!grep \"=> result\" -A 3 output/oxford_pets_16shot/log.txt | tail -3\n",
        "\n",
        "!echo \"\\n🔧 Custom: N_CTX=8:\"\n",
        "!grep \"=> result\" -A 3 output/oxford_pets_custom_ctx8/log.txt | tail -3\n",
        "\n",
        "!echo \"\\n🔧 Custom: CSC=True:\"\n",
        "!grep \"=> result\" -A 3 output/oxford_pets_custom_csc/log.txt | tail -3\n",
        "\n",
        "!echo \"\\n🔧 Custom: LR=0.001:\"\n",
        "!grep \"=> result\" -A 3 output/oxford_pets_custom_lr/log.txt | tail -3"
      ],
      "metadata": {
        "id": "88Pw26wc8X2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d206207a-7579-4bcf-b4a9-5830e55ac5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "═══════════════════════════════════════════════════\n",
            "       CUSTOM EXPERIMENTS COMPARISON\n",
            "═══════════════════════════════════════════════════\n",
            "\\n🔧 Baseline (16-shot, N_CTX=16, CSC=False, LR=0.002):\n",
            "* total: 3,669\n",
            "* correct: 3,426\n",
            "* accuracy: 93.4%\n",
            "\\n🔧 Custom: N_CTX=8:\n",
            "* total: 3,669\n",
            "* correct: 3,411\n",
            "* accuracy: 93.0%\n",
            "\\n🔧 Custom: CSC=True:\n",
            "* total: 3,669\n",
            "* correct: 3,306\n",
            "* accuracy: 90.1%\n",
            "\\n🔧 Custom: LR=0.001:\n",
            "* total: 3,669\n",
            "* correct: 3,415\n",
            "* accuracy: 93.1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization & Analysis (Optional)\n",
        "\n",
        "These cells provide additional analysis and visualization of your results. They are optional but recommended for deeper understanding.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bNSByv4i8dHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze training time for each experiment\n",
        "%%bash\n",
        "echo \"═══════════════════════════════════════════\"\n",
        "echo \"        TRAINING TIME ANALYSIS\"\n",
        "echo \"═══════════════════════════════════════════\"\n",
        "\n",
        "for exp in 1shot 4shot 16shot; do\n",
        "    echo -e \"\\n📊 oxford_pets_${exp}:\"\n",
        "    grep \"Elapsed:\" output/oxford_pets_${exp}/log.txt | tail -1\n",
        "done\n"
      ],
      "metadata": {
        "id": "pIqeUXYt8agz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9e66e5-513c-4690-e767-890b7ef20a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "═══════════════════════════════════════════\n",
            "        TRAINING TIME ANALYSIS\n",
            "═══════════════════════════════════════════\n",
            "\n",
            "📊 oxford_pets_1shot:\n",
            "Elapsed: 0:01:06\n",
            "\n",
            "📊 oxford_pets_4shot:\n",
            "Elapsed: 0:01:54\n",
            "\n",
            "📊 oxford_pets_16shot:\n",
            "Elapsed: 0:05:12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and display learned prompt tokens (advanced)\n",
        "import torch\n",
        "\n",
        "# Load 16-shot model checkpoint\n",
        "checkpoint_path = \"output/oxford_pets_16shot/prompt_learner/model.pth.tar-50\"\n",
        "\n",
        "print(\"=== Learned Prompt Analysis ===\")\n",
        "print(f\"Loading checkpoint from: {checkpoint_path}\\n\")\n",
        "\n",
        "try:\n",
        "    # Load with weights_only=False for compatibility\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "    print(\"✓ Checkpoint loaded successfully!\")\n",
        "    print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
        "    print(f\"\\nModel state_dict keys: {len(checkpoint['state_dict'])} parameters\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error loading checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "8xFa7nNA8gV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283b4d33-2605-4fdb-f1ad-73beeba77ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Learned Prompt Analysis ===\n",
            "Loading checkpoint from: output/oxford_pets_16shot/prompt_learner/model.pth.tar-50\n",
            "\n",
            "✓ Checkpoint loaded successfully!\n",
            "Checkpoint keys: ['state_dict', 'epoch', 'optimizer', 'scheduler', 'val_result']\n",
            "\n",
            "Model state_dict keys: 3 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary of all experiments and their outputs\n",
        "!echo \"═══════════════════════════════════════════\"\n",
        "!echo \"      ALL EXPERIMENTS DIRECTORY\"\n",
        "!echo \"═══════════════════════════════════════════\"\n",
        "!ls -lh output/\n",
        "\n",
        "!echo \"\\n📊 Total disk space used by experiments:\"\n",
        "!du -sh output/"
      ],
      "metadata": {
        "id": "IWW798kp8jTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0906fbc-e690-4cfc-e8b3-826ed25064a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "═══════════════════════════════════════════\n",
            "      ALL EXPERIMENTS DIRECTORY\n",
            "═══════════════════════════════════════════\n",
            "total 24K\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:33 oxford_pets_16shot\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:24 oxford_pets_1shot\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:27 oxford_pets_4shot\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:48 oxford_pets_custom_csc\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:42 oxford_pets_custom_ctx8\n",
            "drwxr-xr-x 4 root root 4.0K Nov 15 19:54 oxford_pets_custom_lr\n",
            "\\n📊 Total disk space used by experiments:\n",
            "16M\toutput/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Summary & Next Steps\n",
        "\n",
        "### 🎯 What We Accomplished:\n",
        "- ✅ Set up CoOp training environment on Google Colab\n",
        "- ✅ Fixed PyTorch 2.x compatibility issues\n",
        "- ✅ Downloaded and prepared Oxford-IIIT Pets dataset\n",
        "- ✅ Trained models with 1-shot, 4-shot, and 16-shot configurations\n",
        "- ✅ Achieved ~93% accuracy on 16-shot learning\n",
        "- ✅ Experimented with custom hyperparameters\n",
        "\n",
        "### 📚 Next Steps:\n",
        "1. Try different datasets (e.g., Caltech101, Food101)\n",
        "2. Experiment with CoCoOp (improved version of CoOp)\n",
        "3. Test on domain shift scenarios\n",
        "4. Fine-tune for your specific use case\n",
        "\n",
        "### 📖 References:\n",
        "- **Paper**: [Learning to Prompt for Vision-Language Models (IJCV 2022)](https://arxiv.org/abs/2109.01134)\n",
        "- **CoOp Repo**: [https://github.com/KaiyangZhou/CoOp](https://github.com/KaiyangZhou/CoOp)\n",
        "- **Dassl Repo**: [https://github.com/KaiyangZhou/Dassl.pytorch](https://github.com/KaiyangZhou/Dassl.pytorch)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6RkssuFR8nmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a comprehensive final report\n",
        "!echo \"╔═══════════════════════════════════════════════════════════╗\"\n",
        "!echo \"║                   FINAL PROJECT REPORT                    ║\"\n",
        "!echo \"╚═══════════════════════════════════════════════════════════╝\"\n",
        "\n",
        "!echo \"\\n📋 PROJECT INFORMATION:\"\n",
        "!echo \"  • Dataset: Oxford-IIIT Pets (37 classes)\"\n",
        "!echo \"  • Model: CoOp (Context Optimization for CLIP)\"\n",
        "!echo \"  • Backbone: ViT-B/16\"\n",
        "!echo \"  • Experiments: 6 training runs\"\n",
        "\n",
        "!echo \"\\n📊 MAIN RESULTS:\"\n",
        "!echo \"  ┌─────────────┬──────────────┬─────────────┐\"\n",
        "!echo \"  │ Experiment  │ Accuracy (%) │ Macro F1 (%)│\"\n",
        "!echo \"  ├─────────────┼──────────────┼─────────────┤\"\n",
        "!echo \"  │ 1-Shot      │    ~90.7     │    ~90.6    │\"\n",
        "!echo \"  │ 4-Shot      │    ~92.6     │    ~92.5    │\"\n",
        "!echo \"  │ 16-Shot     │    ~93.0+    │    ~93.0+   │\"\n",
        "!echo \"  └─────────────┴──────────────┴─────────────┘\"\n",
        "\n",
        "!echo \"\\n💾 SAVED OUTPUTS:\"\n",
        "!echo \"  • Local: /content/CoOp/output/\"\n",
        "!echo \"  • Google Drive: MyDrive/CoOp_OxfordPets_Results/\"\n",
        "!echo \"  • Archive: oxford_pets_results.zip\"\n",
        "\n",
        "!echo \"\\n✅ PROJECT STATUS: COMPLETED SUCCESSFULLY\"\n",
        "!echo \"\\n═══════════════════════════════════════════════════════════\""
      ],
      "metadata": {
        "id": "DJuMJd6t8mHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad78e696-025a-4cdc-a449-4fc4286390a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╔═══════════════════════════════════════════════════════════╗\n",
            "║                   FINAL PROJECT REPORT                    ║\n",
            "╚═══════════════════════════════════════════════════════════╝\n",
            "\\n📋 PROJECT INFORMATION:\n",
            "  • Dataset: Oxford-IIIT Pets (37 classes)\n",
            "  • Model: CoOp (Context Optimization for CLIP)\n",
            "  • Backbone: ViT-B/16\n",
            "  • Experiments: 6 training runs\n",
            "\\n📊 MAIN RESULTS:\n",
            "  ┌─────────────┬──────────────┬─────────────┐\n",
            "  │ Experiment  │ Accuracy (%) │ Macro F1 (%)│\n",
            "  ├─────────────┼──────────────┼─────────────┤\n",
            "  │ 1-Shot      │    ~90.7     │    ~90.6    │\n",
            "  │ 4-Shot      │    ~92.6     │    ~92.5    │\n",
            "  │ 16-Shot     │    ~93.0+    │    ~93.0+   │\n",
            "  └─────────────┴──────────────┴─────────────┘\n",
            "\\n💾 SAVED OUTPUTS:\n",
            "  • Local: /content/CoOp/output/\n",
            "  • Google Drive: MyDrive/CoOp_OxfordPets_Results/\n",
            "  • Archive: oxford_pets_results.zip\n",
            "\\n✅ PROJECT STATUS: COMPLETED SUCCESSFULLY\n",
            "\\n═══════════════════════════════════════════════════════════\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎓 Acknowledgments & License\n",
        "\n",
        "This notebook is based on the following repositories:\n",
        "\n",
        "### Original Works:\n",
        "- **CoOp**: [KaiyangZhou/CoOp](https://github.com/KaiyangZhou/CoOp) (MIT License)\n",
        "- **Dassl.pytorch**: [KaiyangZhou/Dassl.pytorch](https://github.com/KaiyangZhou/Dassl.pytorch) (MIT License)\n",
        "\n",
        "### Citation:\n",
        "If you use this code in your research, please cite:\n",
        "\n"
      ],
      "metadata": {
        "id": "bM3VSpvS81oV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceYiQpj48sNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}